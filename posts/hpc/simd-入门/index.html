<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SIMD 入门 | diefish's blog</title><meta name=keywords content="CS,HPC,CXX"><meta name=description content="1. What is SIMD?
SIMD，即 Single Instruction Multiple Data ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。
SIMD 的核心思想是用一条指令同时对多个数据进行操作，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的向量寄存器 (Vector Registers) 和能够操作这些寄存器的向量指令 (Vector Instructions)。一个向量操作可以同时完成多个标量操作，从而实现数据并行 (Data Parallelism)，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。
2. How SIMD Works
要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。
2.1. Vector Registers
向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 Instruction Set Architecture (ISA, 指令集架构) 提供了不同宽度和名称的向量寄存器。


SSE (Streaming SIMD Extensions)：提供了 128 位的 XMM 寄存器。


AVX (Advanced Vector Extensions)：提供了 256 位的 YMM 寄存器。"><meta name=author content="diefish"><link rel=canonical href=https://diefish1024.github.io/posts/hpc/simd-%E5%85%A5%E9%97%A8/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://diefish1024.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=mask-icon href=https://diefish1024.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://diefish1024.github.io/posts/hpc/simd-%E5%85%A5%E9%97%A8/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://diefish1024.github.io/posts/hpc/simd-%E5%85%A5%E9%97%A8/"><meta property="og:site_name" content="diefish's blog"><meta property="og:title" content="SIMD 入门"><meta property="og:description" content="1. What is SIMD? SIMD，即 Single Instruction Multiple Data ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。
SIMD 的核心思想是用一条指令同时对多个数据进行操作，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的向量寄存器 (Vector Registers) 和能够操作这些寄存器的向量指令 (Vector Instructions)。一个向量操作可以同时完成多个标量操作，从而实现数据并行 (Data Parallelism)，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。
2. How SIMD Works 要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。
2.1. Vector Registers 向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 Instruction Set Architecture (ISA, 指令集架构) 提供了不同宽度和名称的向量寄存器。
SSE (Streaming SIMD Extensions)：提供了 128 位的 XMM 寄存器。
AVX (Advanced Vector Extensions)：提供了 256 位的 YMM 寄存器。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-02T16:13:00+08:00"><meta property="article:modified_time" content="2025-09-02T16:13:00+08:00"><meta property="article:tag" content="CS"><meta property="article:tag" content="HPC"><meta property="article:tag" content="CXX"><meta property="og:image" content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:title content="SIMD 入门"><meta name=twitter:description content="1. What is SIMD?
SIMD，即 Single Instruction Multiple Data ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。
SIMD 的核心思想是用一条指令同时对多个数据进行操作，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的向量寄存器 (Vector Registers) 和能够操作这些寄存器的向量指令 (Vector Instructions)。一个向量操作可以同时完成多个标量操作，从而实现数据并行 (Data Parallelism)，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。
2. How SIMD Works
要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。
2.1. Vector Registers
向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 Instruction Set Architecture (ISA, 指令集架构) 提供了不同宽度和名称的向量寄存器。


SSE (Streaming SIMD Extensions)：提供了 128 位的 XMM 寄存器。


AVX (Advanced Vector Extensions)：提供了 256 位的 YMM 寄存器。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://diefish1024.github.io/posts/"},{"@type":"ListItem","position":2,"name":"HPC","item":"https://diefish1024.github.io/posts/hpc/"},{"@type":"ListItem","position":3,"name":"SIMD 入门","item":"https://diefish1024.github.io/posts/hpc/simd-%E5%85%A5%E9%97%A8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SIMD 入门","name":"SIMD 入门","description":"1. What is SIMD? SIMD，即 Single Instruction Multiple Data ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。\nSIMD 的核心思想是用一条指令同时对多个数据进行操作，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的向量寄存器 (Vector Registers) 和能够操作这些寄存器的向量指令 (Vector Instructions)。一个向量操作可以同时完成多个标量操作，从而实现数据并行 (Data Parallelism)，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。\n2. How SIMD Works 要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。\n2.1. Vector Registers 向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 Instruction Set Architecture (ISA, 指令集架构) 提供了不同宽度和名称的向量寄存器。\nSSE (Streaming SIMD Extensions)：提供了 128 位的 XMM 寄存器。\nAVX (Advanced Vector Extensions)：提供了 256 位的 YMM 寄存器。\n","keywords":["CS","HPC","CXX"],"articleBody":"1. What is SIMD? SIMD，即 Single Instruction Multiple Data ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。\nSIMD 的核心思想是用一条指令同时对多个数据进行操作，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的向量寄存器 (Vector Registers) 和能够操作这些寄存器的向量指令 (Vector Instructions)。一个向量操作可以同时完成多个标量操作，从而实现数据并行 (Data Parallelism)，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。\n2. How SIMD Works 要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。\n2.1. Vector Registers 向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 Instruction Set Architecture (ISA, 指令集架构) 提供了不同宽度和名称的向量寄存器。\nSSE (Streaming SIMD Extensions)：提供了 128 位的 XMM 寄存器。\nAVX (Advanced Vector Extensions)：提供了 256 位的 YMM 寄存器。\nAVX-512：提供了 512 位的 ZMM 寄存器。\nARM NEON：主要用于移动设备，提供 128 位的向量寄存器。\n比如一个 YMM 寄存器可以同时存放 8 个单精度浮点数（8 * 32 位 = 256 位）或 4 个双精度浮点数（4 * 64 位 = 256 位）。\n2.2. Vector Instructions 向量指令是专门用来操作向量寄存器中数据的指令。这些指令通常与标量指令功能对应，但作用于整个向量。\n算术运算：向量加、减、乘、除。\n逻辑运算：向量与、或、异或。\n数据加载/存储：将内存中的连续数据块加载到向量寄存器，或将寄存器中的数据存回内存。\n数据重排 (Shuffle/Permute)：在向量寄存器内部重新排列数据元素，这是许多高级算法优化的关键。\n3. SIMD Programming Models 实际编程中，主要通过两种凡是来利用 SIMD：自动向量化和手动向量化。\n3.1 Automatic Vectorization Automatic Vectorization (自动向量化) 是指编译器自动分析代码（通常是循环），并将其转换为 SIMD 指令的过程。这是最简单、最直接的优化方式。\n要让编译器成功进行自动向量化，代码需要满足一些条件：\n循环结构简单: 循环体内部没有复杂的分支判断。\n无数据依赖: 循环的每次迭代之间没有依赖关系。例如，a[i] = a[i-1] + 1 这样的代码就存在数据依赖，无法被直接向量化。\n内存访问连续: 对数组的访问是连续的，例如 row-major order (行主序) 访问。\n一个能被自动向量化的简单例子：\n1 2 3 4 5 6 7 void vector_add(float* a, float* b, float* c, int n) { for (int i = 0; i \u003c n; ++i) { // 每次迭代之间没有数据依赖 // 内存访问也是连续的 c[i] = a[i] + b[i]; } } 现代的编译器（比如 Clang, GCC）在开启优化选项时会默认尝试自动向量化。\n3.2 Manual Vectorization with Intrinsics 当自动向量化不能满足性能要求，或者循环逻辑太复杂导致编译器无法分析时，就需要进行手动向量化。最常用的方法是使用 Intrinsics (内建函数)。\nIntrinsics 是编译器提供的、与特定汇编指令一一对应的函数。我们可以和调用普通函数一样使用，而编译器会直接将其翻译成对应的 SIMD 指令。\n这种方式的优点是：\n可以精准控制使用哪条 SIMD 指令，实现最大程度的优化。 可以实现自动向量化无法完成的复杂逻辑。 缺点是：\n代码可移植性差，和特定的架构强相关，基于特定 ISA 编写的代码不能在不支持该指令集的 CPU 上运行（除非使用 qemu ）。 需要学习特定指令集对应的函数，非常繁琐。 3.3 An Example 可以通过一个实例来对比普通实现和使用 AVX Intrinsics 的手动向量化实现。\n普通实现：\n1 2 3 4 5 6 // 传统的标量实现 void scalar_add(float* a, float* b, float* c, int n) { for (int i = 0; i \u003c n; ++i) { c[i] = a[i] + b[i]; } } 手动向量化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 引入AVX头文件 #include void avx_add(float* a, float* b, float* c, int n) { // 假设 n 是8的倍数，便于演示 for (int i = 0; i \u003c n; i += 8) { // 1. 从内存加载8个浮点数到 YMM 寄存器 __m256 vec_a = _mm256_load_ps(\u0026a[i]); __m256 vec_b = _mm256_load_ps(\u0026b[i]); // 2. 执行向量加法，一条指令完成8个浮点数相加 __m256 vec_c = _mm256_add_ps(vec_a, vec_b); // 3. 将计算结果从 YMM 寄存器存回内存 _mm256_store_ps(\u0026c[i], vec_c); } } __m256 是 AVX 中的数据类型，代表一个 256 位的向量。 _mm256_load_ps 从内存加载数据到向量寄存器。 _mm256_add_ps 执行单精度浮点数的向量加法。 _mm256_store_ps 将结果存回内存。 通过这种方式，循环迭代次数减少为原来的 1/8，并且每次迭代处理的数据量是原来的 8 倍，理论上性能提升巨大，但是由于这个入水平有限，写的 benchmark 没打过编译器自动优化✋😭🤚可能需要复杂一点的任务才能明显体现性能的优越性。\nSummary SIMD 是一种利用 Data Parallelism (数据并行) 提升性能的关键技术。\n其核心是通过 Vector Registers (向量寄存器) 和 Vector Instructions (向量指令)，实现单指令处理多数据的目标。\nAutomatic Vectorization (自动向量化) 是最便捷的 SIMD 优化方法，依赖于编译器的能力。\n当需要极致性能和精确控制时，可以使用 Intrinsics (内建函数) 进行手动向量化。\nReferences 向量化 - HPC入门指南 lect19 - NJU OS 2025 ","wordCount":"384","inLanguage":"en","image":"https://diefish1024.github.io/images/avatar.jpg","datePublished":"2025-09-02T16:13:00+08:00","dateModified":"2025-09-02T16:13:00+08:00","author":{"@type":"Person","name":"diefish"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://diefish1024.github.io/posts/hpc/simd-%E5%85%A5%E9%97%A8/"},"publisher":{"@type":"Organization","name":"diefish's blog","logo":{"@type":"ImageObject","url":"https://diefish1024.github.io/images/avatar.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://diefish1024.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://diefish1024.github.io/images/avatar.jpg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://diefish1024.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://diefish1024.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://diefish1024.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://diefish1024.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://diefish1024.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://diefish1024.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/hpc/>HPC</a></div><h1 class="post-title entry-hint-parent">SIMD 入门</h1><div class=post-meta><span title='2025-09-02 16:13:00 +0800 +0800'>September 2, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;diefish&nbsp;|&nbsp;<a href=https://github.com/diefish1024/diefish1024.github.io/blob/main/content/posts/HPC/simd-%e5%85%a5%e9%97%a8.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-what-is-simd aria-label="1. What is SIMD?">1. What is SIMD?</a></li><li><a href=#2-how-simd-works aria-label="2. How SIMD Works">2. How SIMD Works</a><ul><li><a href=#21-vector-registers aria-label="2.1. Vector Registers">2.1. Vector Registers</a></li><li><a href=#22-vector-instructions aria-label="2.2. Vector Instructions">2.2. Vector Instructions</a></li></ul></li><li><a href=#3-simd-programming-models aria-label="3. SIMD Programming Models">3. SIMD Programming Models</a><ul><li><a href=#31-automatic-vectorization aria-label="3.1 Automatic Vectorization">3.1 Automatic Vectorization</a></li><li><a href=#32-manual-vectorization-with-intrinsics aria-label="3.2 Manual Vectorization with Intrinsics">3.2 Manual Vectorization with Intrinsics</a></li><li><a href=#33-an-example aria-label="3.3 An Example">3.3 An Example</a></li></ul></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=1-what-is-simd>1. What is SIMD?<a hidden class=anchor aria-hidden=true href=#1-what-is-simd>#</a></h2><p>SIMD，即 <strong>Single Instruction Multiple Data</strong> ，是一种并行计算的模式。传统的单指令单数据模型，也就是一条指令 CPU 只能处理一份数据，这在科学计算和图像渲染等大量数据密集的任务中是非常低效的。</p><p>SIMD 的核心思想是<strong>用一条指令同时对多个数据进行操作</strong>，现代的 CPU 为此设计了特殊的硬件单元，包括宽位（比如 128、256 或 512 位）的<strong>向量寄存器 (Vector Registers)</strong> 和能够操作这些寄存器的<strong>向量指令 (Vector Instructions)</strong>。一个向量操作可以同时完成多个标量操作，从而实现<strong>数据并行 (Data Parallelism)</strong>，提高效率。假设一个 256 位的向量寄存器可以容纳 8 个 32 位浮点数，一条向量加法指令就可以一次性完成 8 个浮点数的加法，理论上将这部分计算的吞吐量提升至原来的 8 倍；并且相比于执行 8 条独立的标量加法指令，CPU 只需要获取并解码一条向量加法指令，这降低了指令流水线的压力。</p><h2 id=2-how-simd-works>2. How SIMD Works<a hidden class=anchor aria-hidden=true href=#2-how-simd-works>#</a></h2><p>要理解 SIMD 的工作原理，需要了解两个核心概念：向量寄存器和向量指令。</p><h3 id=21-vector-registers>2.1. Vector Registers<a hidden class=anchor aria-hidden=true href=#21-vector-registers>#</a></h3><p>向量寄存器是 CPU 内部的特殊存储单元，其宽度远大于通用寄存器。不同的 <strong>Instruction Set Architecture (ISA, 指令集架构)</strong> 提供了不同宽度和名称的向量寄存器。</p><ul><li><p><strong>SSE (Streaming SIMD Extensions)</strong>：提供了 128 位的 <code>XMM</code> 寄存器。</p></li><li><p><strong>AVX (Advanced Vector Extensions)</strong>：提供了 256 位的 <code>YMM</code> 寄存器。</p></li><li><p><strong>AVX-512</strong>：提供了 512 位的 <code>ZMM</code> 寄存器。</p></li><li><p><strong>ARM NEON</strong>：主要用于移动设备，提供 128 位的向量寄存器。</p></li></ul><p>比如一个 <code>YMM</code> 寄存器可以同时存放 8 个单精度浮点数（8 * 32 位 = 256 位）或 4 个双精度浮点数（4 * 64 位 = 256 位）。</p><h3 id=22-vector-instructions>2.2. Vector Instructions<a hidden class=anchor aria-hidden=true href=#22-vector-instructions>#</a></h3><p>向量指令是专门用来操作向量寄存器中数据的指令。这些指令通常与标量指令功能对应，但作用于整个向量。</p><ul><li><p><strong>算术运算</strong>：向量加、减、乘、除。</p></li><li><p><strong>逻辑运算</strong>：向量与、或、异或。</p></li><li><p><strong>数据加载/存储</strong>：将内存中的连续数据块加载到向量寄存器，或将寄存器中的数据存回内存。</p></li><li><p><strong>数据重排 (Shuffle/Permute)</strong>：在向量寄存器内部重新排列数据元素，这是许多高级算法优化的关键。</p></li></ul><h2 id=3-simd-programming-models>3. SIMD Programming Models<a hidden class=anchor aria-hidden=true href=#3-simd-programming-models>#</a></h2><p>实际编程中，主要通过两种凡是来利用 SIMD：自动向量化和手动向量化。</p><h3 id=31-automatic-vectorization>3.1 Automatic Vectorization<a hidden class=anchor aria-hidden=true href=#31-automatic-vectorization>#</a></h3><p><strong>Automatic Vectorization (自动向量化)</strong> 是指编译器自动分析代码（通常是循环），并将其转换为 SIMD 指令的过程。这是最简单、最直接的优化方式。</p><p>要让编译器成功进行自动向量化，代码需要满足一些条件：</p><ul><li><p><strong>循环结构简单</strong>: 循环体内部没有复杂的分支判断。</p></li><li><p><strong>无数据依赖</strong>: 循环的每次迭代之间没有依赖关系。例如，<code>a[i] = a[i-1] + 1</code> 这样的代码就存在数据依赖，无法被直接向量化。</p></li><li><p><strong>内存访问连续</strong>: 对数组的访问是连续的，例如 <strong>row-major order (行主序)</strong> 访问。</p></li></ul><p>一个能被自动向量化的简单例子：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vector_add</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>a</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>b</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>c</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// 每次迭代之间没有数据依赖
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// 内存访问也是连续的
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>现代的编译器（比如 Clang, GCC）在开启优化选项时会默认尝试自动向量化。</p><h3 id=32-manual-vectorization-with-intrinsics>3.2 Manual Vectorization with Intrinsics<a hidden class=anchor aria-hidden=true href=#32-manual-vectorization-with-intrinsics>#</a></h3><p>当自动向量化不能满足性能要求，或者循环逻辑太复杂导致编译器无法分析时，就需要进行<strong>手动向量化</strong>。最常用的方法是使用 <strong>Intrinsics (内建函数)</strong>。</p><p>Intrinsics 是编译器提供的、与特定汇编指令一一对应的函数。我们可以和调用普通函数一样使用，而编译器会直接将其翻译成对应的 SIMD 指令。</p><p>这种方式的优点是：</p><ul><li>可以精准控制使用哪条 SIMD 指令，实现最大程度的优化。</li><li>可以实现自动向量化无法完成的复杂逻辑。</li></ul><p>缺点是：</p><ul><li>代码可移植性差，和特定的架构强相关，基于特定 ISA 编写的代码不能在不支持该指令集的 CPU 上运行（除非使用 <code>qemu</code> ）。</li><li>需要学习特定指令集对应的函数，非常繁琐。</li></ul><h3 id=33-an-example>3.3 An Example<a hidden class=anchor aria-hidden=true href=#33-an-example>#</a></h3><p>可以通过一个实例来对比普通实现和使用 AVX Intrinsics 的手动向量化实现。</p><p><strong>普通实现：</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>// 传统的标量实现
</span></span><span class=line><span class=cl>void scalar_add(float* a, float* b, float* c, int n) {
</span></span><span class=line><span class=cl>    for (int i = 0; i &lt; n; ++i) {
</span></span><span class=line><span class=cl>        c[i] = a[i] + b[i];
</span></span><span class=line><span class=cl>    }
</span></span><span class=line><span class=cl>}
</span></span></code></pre></td></tr></table></div></div><p><strong>手动向量化</strong>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>// 引入AVX头文件
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=cp>#include</span> <span class=cpf>&lt;immintrin.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>avx_add</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>a</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>b</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>c</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 假设 n 是8的倍数，便于演示
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>8</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// 1. 从内存加载8个浮点数到 YMM 寄存器
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>__m256</span> <span class=n>vec_a</span> <span class=o>=</span> <span class=n>_mm256_load_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>__m256</span> <span class=n>vec_b</span> <span class=o>=</span> <span class=n>_mm256_load_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>b</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// 2. 执行向量加法，一条指令完成8个浮点数相加
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>__m256</span> <span class=n>vec_c</span> <span class=o>=</span> <span class=n>_mm256_add_ps</span><span class=p>(</span><span class=n>vec_a</span><span class=p>,</span> <span class=n>vec_b</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// 3. 将计算结果从 YMM 寄存器存回内存
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>_mm256_store_ps</span><span class=p>(</span><span class=o>&amp;</span><span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>vec_c</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>__m256</code> 是 AVX 中的数据类型，代表一个 256 位的向量。</li><li><code>_mm256_load_ps</code> 从内存加载数据到向量寄存器。</li><li><code>_mm256_add_ps</code> 执行单精度浮点数的向量加法。</li><li><code>_mm256_store_ps</code> 将结果存回内存。</li></ul><p>通过这种方式，循环迭代次数减少为原来的 1/8，并且每次迭代处理的数据量是原来的 8 倍，理论上性能提升巨大，但是由于这个入水平有限，写的 benchmark 没打过编译器自动优化✋😭🤚可能需要复杂一点的任务才能明显体现性能的优越性。</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><ul><li><p>SIMD 是一种利用 <strong>Data Parallelism (数据并行)</strong> 提升性能的关键技术。</p></li><li><p>其核心是通过 <strong>Vector Registers (向量寄存器)</strong> 和 <strong>Vector Instructions (向量指令)</strong>，实现单指令处理多数据的目标。</p></li><li><p><strong>Automatic Vectorization (自动向量化)</strong> 是最便捷的 SIMD 优化方法，依赖于编译器的能力。</p></li><li><p>当需要极致性能和精确控制时，可以使用 <strong>Intrinsics (内建函数)</strong> 进行手动向量化。</p></li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://xflops.sjtu.edu.cn/hpc-start-guide/parallel-computing/SIMD/>向量化 - HPC入门指南</a></li><li><a href=https://jyywiki.cn/OS/2025/lect19.md>lect19 - NJU OS 2025</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://diefish1024.github.io/tags/cs/>CS</a></li><li><a href=https://diefish1024.github.io/tags/hpc/>HPC</a></li><li><a href=https://diefish1024.github.io/tags/cxx/>CXX</a></li></ul><nav class=paginav><a class=prev href=https://diefish1024.github.io/posts/solutions/xflops2024-bithack/><span class=title>« Prev</span><br><span>Xflops2024-Bithack</span>
</a><a class=next href=https://diefish1024.github.io/posts/misc/copt-%E6%B1%82%E8%A7%A3%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/><span class=title>Next »</span><br><span>COPT 求解器学习笔记</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on x" href="https://x.com/intent/tweet/?text=SIMD%20%e5%85%a5%e9%97%a8&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f&amp;hashtags=CS%2cHPC%2cCXX"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f&amp;title=SIMD%20%e5%85%a5%e9%97%a8&amp;summary=SIMD%20%e5%85%a5%e9%97%a8&amp;source=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f&title=SIMD%20%e5%85%a5%e9%97%a8"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on whatsapp" href="https://api.whatsapp.com/send?text=SIMD%20%e5%85%a5%e9%97%a8%20-%20https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on telegram" href="https://telegram.me/share/url?text=SIMD%20%e5%85%a5%e9%97%a8&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SIMD 入门 on ycombinator" href="https://news.ycombinator.com/submitlink?t=SIMD%20%e5%85%a5%e9%97%a8&u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2fsimd-%25E5%2585%25A5%25E9%2597%25A8%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://diefish1024.github.io/>diefish's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>