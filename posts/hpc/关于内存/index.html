<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>关于内存 | diefish's blog</title><meta name=keywords content="CS,HPC,CPU"><meta name=description content="如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。
Understanding Memory Hierarchy
为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。
Registers, Caches, and Main Memory


寄存器 (Registers)： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。


缓存 (Cache)： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。

L1 缓存 (Level 1 Cache)：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。
L2 缓存 (Level 2 Cache)：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。
L3 缓存 (Level 3 Cache)：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。



主内存 (Main Memory/RAM)： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。


TLB (Translation Lookaside Buffer)： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。"><meta name=author content="diefish"><link rel=canonical href=https://diefish1024.github.io/posts/hpc/%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98/><link crossorigin=anonymous href=/assets/css/stylesheet.7e33168b13c822c8560dd6cce14b81ffdf7b6c118596a21d43f319d693f61534.css integrity="sha256-fjMWixPIIshWDdbM4UuB/997bBGFlqIdQ/MZ1pP2FTQ=" rel="preload stylesheet" as=style><link rel=icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://diefish1024.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=mask-icon href=https://diefish1024.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://diefish1024.github.io/posts/hpc/%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://diefish1024.github.io/posts/hpc/%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98/"><meta property="og:site_name" content="diefish's blog"><meta property="og:title" content="关于内存"><meta property="og:description" content="如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。
Understanding Memory Hierarchy 为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。
Registers, Caches, and Main Memory 寄存器 (Registers)： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。
缓存 (Cache)： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。
L1 缓存 (Level 1 Cache)：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。 L2 缓存 (Level 2 Cache)：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。 L3 缓存 (Level 3 Cache)：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。 主内存 (Main Memory/RAM)： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。
TLB (Translation Lookaside Buffer)： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-12T10:36:00+08:00"><meta property="article:modified_time" content="2025-09-12T10:36:00+08:00"><meta property="article:tag" content="CS"><meta property="article:tag" content="HPC"><meta property="article:tag" content="CPU"><meta property="og:image" content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:title content="关于内存"><meta name=twitter:description content="如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。
Understanding Memory Hierarchy
为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。
Registers, Caches, and Main Memory


寄存器 (Registers)： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。


缓存 (Cache)： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。

L1 缓存 (Level 1 Cache)：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。
L2 缓存 (Level 2 Cache)：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。
L3 缓存 (Level 3 Cache)：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。



主内存 (Main Memory/RAM)： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。


TLB (Translation Lookaside Buffer)： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://diefish1024.github.io/posts/"},{"@type":"ListItem","position":2,"name":"HPC","item":"https://diefish1024.github.io/posts/hpc/"},{"@type":"ListItem","position":3,"name":"关于内存","item":"https://diefish1024.github.io/posts/hpc/%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"关于内存","name":"关于内存","description":"如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。\nUnderstanding Memory Hierarchy 为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。\nRegisters, Caches, and Main Memory 寄存器 (Registers)： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。\n缓存 (Cache)： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。\nL1 缓存 (Level 1 Cache)：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。 L2 缓存 (Level 2 Cache)：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。 L3 缓存 (Level 3 Cache)：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。 主内存 (Main Memory/RAM)： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。\nTLB (Translation Lookaside Buffer)： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。\n","keywords":["CS","HPC","CPU"],"articleBody":"如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。\nUnderstanding Memory Hierarchy 为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。\nRegisters, Caches, and Main Memory 寄存器 (Registers)： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。\n缓存 (Cache)： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。\nL1 缓存 (Level 1 Cache)：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。 L2 缓存 (Level 2 Cache)：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。 L3 缓存 (Level 3 Cache)：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。 主内存 (Main Memory/RAM)： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。\nTLB (Translation Lookaside Buffer)： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。\n通过这种多级内存层次结构访问内存，我们需要尽可能满足局部性原理来提高效率：\n时间局部性 (Temporal Locality)：如果一个数据项最近被访问过，那么它很可能在不久的将来再次被访问。 空间局部性 (Spatial Locality)：如果一个数据项被访问了，那么它附近的内存地址中的数据项也很可能在不久的将来被访问。 Cache-Friendly Programming 编写“缓存友好”的代码意味着组织数据和访问模式，最大化缓存命中率。\nCache Line and Performance Impact 缓存行 (Cache Line)： 缓存和主内存之间数据传输的最小单元，通常为 64 字节。当 CPU 从主内存中请求一个字节时，整个缓存行都会被加载到缓存中。 这强调了空间局部性：如果你的程序按顺序访问内存，那么一次缓存加载可以为未来的访问提供多个数据项，从而提高效率。 伪共享 (False Sharing)：如果两个或多个独立的变量不幸地位于同一个缓存行中，并且被不同的 CPU 核心修改，那么即使它们逻辑上不相关，也会因为缓存一致性协议导致大量的缓存行失效和重新加载，从而严重影响性能。 Cache Hit/Miss and Coherence 缓存命中 (Cache Hit)：当 CPU 需要的数据已经在某个缓存级别中时，访问速度非常快。 缓存未命中 (Cache Miss)：当 CPU 需要的数据不在任何缓存中时，必须从更慢的内存级别（最终是主内存）获取数据，这会引入延迟。未命中可分为： 强制性未命中 (Compulsory Miss/Cold Miss)：首次访问数据。 容量性未命中 (Capacity Miss)：缓存太小，无法容纳所有活跃数据。 冲突性未命中 (Conflict Miss)：多个数据项映射到缓存中的同一个位置。 缓存一致性 (Cache Coherence)： 在多核处理器系统中，不同的核心可能有同一份数据在各自的缓存副本中。为了确保所有核心看到的数据是一致的最新版本，需要缓存一致性协议，如 MESI (Modified, Exclusive, Shared, Invalid) 协议。理解这些协议有助于避免伪共享等问题。 SoA vs. AoS 选择正确的数据布局对缓存性能至关重要。这部分在 HPC 中的 C 和 C++ 中也有提及。\n结构体数组 (AoS: Array of Structs)： struct Point { float x, y, z; } points[N];\n这种布局下，一个 Point 结构体的所有成员在内存中是连续的。如果你的代码经常需要访问一个点的所有坐标，这种布局是高效的。 数组结构体 (SoA: Struct of Arrays)： struct { float x[N], y[N], z[N]; } points_soa;\n如果你需要对所有点的 $ x $ 坐标执行操作，那么可以高效地利用缓存行，因为内存访问是高度连续的。对于 SIMD 向量化操作来说，SoA 通常更优化。 选择 SoA 还是 AoS 取决于数据访问模式：如果经常需要访问一个对象的所有属性，AoS 可能更好（但要注意缓存行对齐和填充）。如果经常需要对多个对象的某个特定属性进行批处理操作，SoA 通常是更好的选择。\nThe Memory Wall 内存墙是指 CPU 的计算速度与主内存的访问速度之间日益扩大的差距。CPU 处理能力的增长远远快于内存延迟的改进速度，这意味着即使 CPU 理论上可以执行大量的指令，但如果它必须经常等待数据从主内存中加载，那么大部分时间都会处于空闲状态，从而限制了实际的应用程序性能。\n解决方案：\n优化算法，减少对内存的访问次数。 最大化缓存命中率，利用数据局部性。 采用预取技术来隐藏内存访问延迟。 NUMA Architectures Non-Uniform Memory Access Challenges NUMA (Non-Uniform Memory Access) ，即非一致性内存访问架构，在多处理器系统中变得越来越普遍。在 NUMA 系统中，每个 CPU (或 CPU 插槽) 都有一组直接连接的本地内存，访问本地内存比访问连接到另一个 CPU 的远端内存要快得多。不同的内存器件和 CPU 核心从属不同的 Node，每个 Node 都有自己的集成内存控制器（IMC，Integrated Memory Controller）。\n如果一个线程在 CPU0 上运行，却频繁访问挂载在 CPU1 上的内存，性能会显著下降，因为数据必须通过处理器间互连（如 Intel 的 UPI 或 AMD 的 Infinity Fabric）传输，这会引入额外的延迟。\n不当的内存放置策略可能导致严重的性能瓶颈，甚至超过内存墙的限制。\nNUMA Optimization with numactl 为了在 NUMA 架构下获得最佳性能，我们必须确保计算尽可能地在靠近其所访问数据的 CPU 核心上进行。numactl 是一个强大的 Linux 命令行工具，它允许我们精确控制进程的 CPU 亲和性和内存分配策略。\n查看 NUMA 节点布局：numactl --hardware 命令可以显示系统中所有的 NUMA 节点、每个节点的 CPU 核心及其本地内存大小。 1 numactl --hardware 输出示例：\n1 2 3 4 5 6 7 8 9 10 11 available: 2 nodes (0-1) node 0 cpus: 0 1 2 3 node 0 size: 16000 MB node 0 free: 15000 MB node 1 cpus: 4 5 6 7 node 1 size: 16000 MB node 1 free: 15000 MB node distances: node 0 1 0: 10 21 1: 21 10 这表示系统有 2 个 NUMA 节点（0 和 1）。节点 0 拥有 CPU 核心 0-3，节点 1 拥有 CPU 核心 4-7。node distances 表示访问本地内存的成本为 10，访问远端内存的成本为 21，远端访问的开销约为本地的两倍。\n重要 numactl 选项： --cpunodebind ：将进程或线程绑定到指定 NUMA 节点上的 CPU 核心。例如，--cpunodebind=0 将进程限制在节点 0 上的 CPU。 --membind ：强制所有内存分配都来自指定 NUMA 节点。例如，--membind=1 将所有内存都从节点 1 分配。 --localalloc：在当前线程运行的 NUMA 节点上分配内存。这是最佳实践，因为它确保了数据存储在距离计算最近的位置。 --physcpubind ：将进程或线程绑定到特定的物理 CPU 核心。 NUMA Memory Access Test 可以通过一个简单的多线程数组求和程序来演示 numactl 对 NUMA 性能的影响。程序会分配一个非常大（足够超出 cache）的数组，然后使用 OpenMP 让多个线程并行计算数组元素的总和。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #include #include #include #include #include #include // 确保足够大以超出缓存并触及 NUMA 效应 const size_t ARRAY_SIZE = 1000000000ULL; int main() { std::cout \u003c\u003c \"Allocating array of \" \u003c\u003c ARRAY_SIZE * sizeof(long long) / (1024 * 1024 * 1024.0) \u003c\u003c \" GB...\" \u003c\u003c std::endl; std::vector\u003clong long\u003e data(ARRAY_SIZE); #pragma omp parallel for for (size_t i = 0; i \u003c ARRAY_SIZE; ++i) { data[i] = i % 100; } std::cout \u003c\u003c \"Array initialized.\" \u003c\u003c std::endl; int num_threads = 2; omp_set_num_threads(num_threads); std::cout \u003c\u003c \"Using \" \u003c\u003c num_threads \u003c\u003c \" OpenMP threads.\" \u003c\u003c std::endl; long long total_sum = 0; auto start = std::chrono::high_resolution_clock::now(); #pragma omp parallel reduction(+:total_sum) { int thread_id = omp_get_thread_num(); size_t chunk_size = ARRAY_SIZE / num_threads; size_t start_idx = thread_id * chunk_size; size_t end_idx = std::min(start_idx + chunk_size, ARRAY_SIZE); std::cout \u003c\u003c \"Thread \" \u003c\u003c thread_id \u003c\u003c \" processing from \" \u003c\u003c start_idx \u003c\u003c \" to \" \u003c\u003c end_idx \u003c\u003c std::endl; for (size_t i = start_idx; i \u003c end_idx; ++i) { total_sum += data[i]; } } auto end = std::chrono::high_resolution_clock::now(); std::chrono::duration\u003cdouble\u003e diff = end - start; std::cout \u003c\u003c \"Calculated total sum: \" \u003c\u003c total_sum \u003c\u003c std::endl; std::cout \u003c\u003c \"Time taken: \" \u003c\u003c diff.count() \u003c\u003c \" seconds\" \u003c\u003c std::endl; return 0; } 可以使用 GCC 编译这个程序：\n1 g++ -std=c++11 -O3 -fopenmp numa_test.cpp -o numa_test （由于我的电脑只有一个 NUMA 核心，所以下面测试无法进行。。。）\nBaseline： 1 ./numa_test 远端内存访问：CPU 在节点 1，内存绑定到节点 0。这时所有数据都是远端访问，理论上性能应该最差。 1 numactl --cpunodebind=1 --membind=0 ./numa_test 本地内存访问：CPU 在节点 0，内存绑定到节点 0。这是理性的 NUMA 配置，所有数据访问都是本地的。 1 numactl --cpunodebind=0 --membind=0 ./numa_test 真实多线程场景：CPU 绑定到节点 0 和 1，但内存仅分配到节点 0。跑在节点 1 上的线程将进行远端内存访问。 1 2 export OMP_NUM_THREADS=2 numactl --cpunodebind=0,1 --membind=0 ./numa_test Prefetching 预取 (Prefetching) 是一种技术，它尝试在 CPU 实际需要数据之前，就将其从较慢的内存层级加载到较快的缓存中。这有助于隐藏内存访问延迟，使 CPU 能够专注于计算。\n硬件预取器 (Hardware Prefetcher)： 现代 CPU 内置的智能逻辑单元，它们会监控内存访问模式，并根据检测到的模式（如顺序访问）自动预测接下来可能需要哪些数据，将其提前加载到缓存中。\n优点：全自动，无需程序员干预。 缺点：有时预测不准确，可能将无用数据加载到缓存中，挤出有用数据，甚至增加内存总线流量。 编译器预取 (Compiler Prefetching)： 一些编译器能够根据代码中的循环和访问模式，在编译时插入预取指令。通过 -O3 等优化选项或特定的编译器提示，可以启用此功能。\n软件预取 (Software Prefetching)： 程序员可以通过使用特殊的 CPU 指令（通常通过内联函数 Intrinsics 暴露）显式地告诉 CPU 预取哪些数据。 例如，在 x86 架构上：\n1 2 3 4 5 6 7 8 9 10 11 12 #include // For _mm_prefetch void process_data(int* arr, int n) { for (int i = 0; i \u003c n; ++i) { // 在实际访问 arr[i + PREFETCH_DISTANCE] 之前提前预取 if (i + PREFETCH_DISTANCE \u003c n) { _mm_prefetch((char*)\u0026arr[i + PREFETCH_DISTANCE], _MM_HINT_T0); } // 处理 arr[i] // ... } } 当硬件预取器无法有效应对复杂的访问模式时，软件预取可以提供更精确的控制。 需要程序员手动插入，可能会增加代码复杂性，不当使用可能导致性能下降。 Summary 在 HPC 领域，仅依靠 CPU 的原始计算能力和并行编程模型是不够的。深入理解计算机内存，是编写高性能代码的基础。通过采用缓存友好的编程，如优化数据布局和分块算法，我们可以显著提高应用程序的性能，真正发挥现代 CPU 的潜力。\nReference 每个程序员都应该知道的 CPU 知识：NUMA（知乎） 浅解 NUMA 机制（知乎） ","wordCount":"809","inLanguage":"en","image":"https://diefish1024.github.io/images/avatar.jpg","datePublished":"2025-09-12T10:36:00+08:00","dateModified":"2025-09-12T10:36:00+08:00","author":{"@type":"Person","name":"diefish"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://diefish1024.github.io/posts/hpc/%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98/"},"publisher":{"@type":"Organization","name":"diefish's blog","logo":{"@type":"ImageObject","url":"https://diefish1024.github.io/images/avatar.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://diefish1024.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://diefish1024.github.io/images/avatar.jpg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://diefish1024.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://diefish1024.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://diefish1024.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://diefish1024.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://diefish1024.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://diefish1024.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/hpc/>HPC</a></div><h1 class="post-title entry-hint-parent">关于内存</h1><div class=post-meta><span title='2025-09-12 10:36:00 +0800 +0800'>September 12, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;diefish&nbsp;|&nbsp;<a href=https://github.com/diefish1024/diefish1024.github.io/blob/main/content/posts/HPC/%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#understanding-memory-hierarchy aria-label="Understanding Memory Hierarchy">Understanding Memory Hierarchy</a><ul><li><a href=#registers-caches-and-main-memory aria-label="Registers, Caches, and Main Memory">Registers, Caches, and Main Memory</a></li></ul></li><li><a href=#cache-friendly-programming aria-label="Cache-Friendly Programming">Cache-Friendly Programming</a><ul><li><a href=#cache-line-and-performance-impact aria-label="Cache Line and Performance Impact">Cache Line and Performance Impact</a></li><li><a href=#cache-hitmiss-and-coherence aria-label="Cache Hit/Miss and Coherence">Cache Hit/Miss and Coherence</a></li><li><a href=#soa-vs-aos aria-label="SoA vs. AoS">SoA vs. AoS</a></li></ul></li><li><a href=#the-memory-wall aria-label="The Memory Wall">The Memory Wall</a></li><li><a href=#numa-architectures aria-label="NUMA Architectures">NUMA Architectures</a><ul><li><a href=#non-uniform-memory-access-challenges aria-label="Non-Uniform Memory Access Challenges">Non-Uniform Memory Access Challenges</a></li><li><a href=#numa-optimization-withnumactl aria-label="NUMA Optimization with numactl">NUMA Optimization with numactl</a></li><li><a href=#numa-memory-access-test aria-label="NUMA Memory Access Test">NUMA Memory Access Test</a></li></ul></li><li><a href=#prefetching aria-label=Prefetching>Prefetching</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><p>如何更好更快地访问内存是 HPC 中最大的瓶颈之一，仅仅了解 SIMD 或并行编程接口是不足够的，本文将梳理计算机的内存层次结构、缓存友好编程、内存墙现象、NUMA 架构以及预取技术。</p><h2 id=understanding-memory-hierarchy>Understanding Memory Hierarchy<a hidden class=anchor aria-hidden=true href=#understanding-memory-hierarchy>#</a></h2><p>为了充分利用现代 CPU 的性能，我们必须理解数据是如何在不同层级的内存组件之间流动的。</p><h3 id=registers-caches-and-main-memory>Registers, Caches, and Main Memory<a hidden class=anchor aria-hidden=true href=#registers-caches-and-main-memory>#</a></h3><ul><li><p><strong>寄存器 (Registers)</strong>： CPU 内置的、容量最小但速度最快的数据存储单元，用于存储正在被 CPU 活跃操作的数据。CPU 直接在寄存器上执行大部分计算。</p></li><li><p><strong>缓存 (Cache)</strong>： 位于 CPU 和主内存之间的小容量、高速存储区域。它们的目的是通过存储最可能被 CPU 再次访问的数据来减少对主内存的访问延迟。</p><ul><li><strong>L1 缓存 (Level 1 Cache)</strong>：最小、最快，通常分为数据缓存 (L1d) 和指令缓存 (L1i)，每个 CPU 核心独有。其访问速度与 CPU 核心时钟周期相近。</li><li><strong>L2 缓存 (Level 2 Cache)</strong>：比 L1 大且慢，每个 CPU 核心独有或由几个核心共享。</li><li><strong>L3 缓存 (Level 3 Cache)</strong>：最大、最慢的缓存，通常由同一 CPU 插槽上的所有核心共享。</li></ul></li><li><p><strong>主内存 (Main Memory/RAM)</strong>： 容量远大于缓存，但访问速度慢得多。当数据不在任何缓存中时，CPU 必须从主内存中获取。</p></li><li><p><strong>TLB (Translation Lookaside Buffer)</strong>： TLB 是一个专用的高性能缓存，用于存储虚拟地址到物理地址的转换映射。当 CPU 访问一个虚拟地址时，它首先检查 TLB。如果找到对应的物理地址（TLB 命中），则可以快速进行内存访问；如果未找到（TLB 未命中），则需要查询页表，这将导致显著的延迟。理解 TLB 对于优化内存页访问模式，尤其是在处理大型数据集时至关重要。</p></li></ul><p>通过这种多级内存层次结构访问内存，我们需要尽可能满足<strong>局部性原理</strong>来提高效率：</p><ul><li><strong>时间局部性 (Temporal Locality)</strong>：如果一个数据项最近被访问过，那么它很可能在不久的将来再次被访问。</li><li><strong>空间局部性 (Spatial Locality)</strong>：如果一个数据项被访问了，那么它附近的内存地址中的数据项也很可能在不久的将来被访问。</li></ul><h2 id=cache-friendly-programming>Cache-Friendly Programming<a hidden class=anchor aria-hidden=true href=#cache-friendly-programming>#</a></h2><p>编写“缓存友好”的代码意味着组织数据和访问模式，最大化缓存命中率。</p><h3 id=cache-line-and-performance-impact>Cache Line and Performance Impact<a hidden class=anchor aria-hidden=true href=#cache-line-and-performance-impact>#</a></h3><ul><li><strong>缓存行 (Cache Line)</strong>： 缓存和主内存之间数据传输的最小单元，通常为 64 字节。当 CPU 从主内存中请求一个字节时，整个缓存行都会被加载到缓存中。<ul><li>这强调了<strong>空间局部性</strong>：如果你的程序按顺序访问内存，那么一次缓存加载可以为未来的访问提供多个数据项，从而提高效率。</li><li><strong>伪共享 (False Sharing)</strong>：如果两个或多个独立的变量不幸地位于同一个缓存行中，并且被不同的 CPU 核心修改，那么即使它们逻辑上不相关，也会因为缓存一致性协议导致大量的缓存行失效和重新加载，从而严重影响性能。</li></ul></li></ul><h3 id=cache-hitmiss-and-coherence>Cache Hit/Miss and Coherence<a hidden class=anchor aria-hidden=true href=#cache-hitmiss-and-coherence>#</a></h3><ul><li><strong>缓存命中 (Cache Hit)</strong>：当 CPU 需要的数据已经在某个缓存级别中时，访问速度非常快。</li><li><strong>缓存未命中 (Cache Miss)</strong>：当 CPU 需要的数据不在任何缓存中时，必须从更慢的内存级别（最终是主内存）获取数据，这会引入延迟。未命中可分为：<ul><li><strong>强制性未命中 (Compulsory Miss/Cold Miss)</strong>：首次访问数据。</li><li><strong>容量性未命中 (Capacity Miss)</strong>：缓存太小，无法容纳所有活跃数据。</li><li><strong>冲突性未命中 (Conflict Miss)</strong>：多个数据项映射到缓存中的同一个位置。</li></ul></li><li><strong>缓存一致性 (Cache Coherence)</strong>： 在多核处理器系统中，不同的核心可能有同一份数据在各自的缓存副本中。为了确保所有核心看到的数据是一致的最新版本，需要缓存一致性协议，如 MESI (Modified, Exclusive, Shared, Invalid) 协议。理解这些协议有助于避免伪共享等问题。</li></ul><h3 id=soa-vs-aos>SoA vs. AoS<a hidden class=anchor aria-hidden=true href=#soa-vs-aos>#</a></h3><p>选择正确的数据布局对缓存性能至关重要。这部分在 <a href=HPC%20%E4%B8%AD%E7%9A%84%20C%20%E5%92%8C%20C++.md>HPC 中的 C 和 C++</a> 中也有提及。</p><ul><li><p><strong>结构体数组 (AoS: Array of Structs)</strong>： <code>struct Point { float x, y, z; } points[N];</code></p><ul><li>这种布局下，一个 <code>Point</code> 结构体的所有成员在内存中是连续的。如果你的代码经常需要访问一个点的所有坐标，这种布局是高效的。</li></ul></li><li><p><strong>数组结构体 (SoA: Struct of Arrays)</strong>： <code>struct { float x[N], y[N], z[N]; } points_soa;</code></p><ul><li>如果你需要对所有点的 $ x $ 坐标执行操作，那么可以高效地利用缓存行，因为内存访问是高度连续的。对于 SIMD 向量化操作来说，SoA 通常更优化。</li></ul></li></ul><p>选择 SoA 还是 AoS 取决于数据访问模式：如果经常需要访问一个对象的所有属性，AoS 可能更好（但要注意缓存行对齐和填充）。如果经常需要对多个对象的某个特定属性进行批处理操作，SoA 通常是更好的选择。</p><h2 id=the-memory-wall>The Memory Wall<a hidden class=anchor aria-hidden=true href=#the-memory-wall>#</a></h2><p><strong>内存墙</strong>是指 CPU 的计算速度与主内存的访问速度之间日益扩大的差距。CPU 处理能力的增长远远快于内存延迟的改进速度，这意味着即使 CPU 理论上可以执行大量的指令，但如果它必须经常等待数据从主内存中加载，那么大部分时间都会处于空闲状态，从而限制了实际的应用程序性能。</p><p><strong>解决方案</strong>：</p><ul><li>优化算法，减少对内存的访问次数。</li><li>最大化缓存命中率，利用数据局部性。</li><li>采用预取技术来隐藏内存访问延迟。</li></ul><h2 id=numa-architectures>NUMA Architectures<a hidden class=anchor aria-hidden=true href=#numa-architectures>#</a></h2><h3 id=non-uniform-memory-access-challenges>Non-Uniform Memory Access Challenges<a hidden class=anchor aria-hidden=true href=#non-uniform-memory-access-challenges>#</a></h3><p><strong>NUMA (Non-Uniform Memory Access)</strong> ，即非一致性内存访问架构，在多处理器系统中变得越来越普遍。在 NUMA 系统中，每个 CPU (或 CPU 插槽) 都有一组直接连接的本地内存，访问本地内存比访问连接到另一个 CPU 的远端内存要快得多。不同的内存器件和 CPU 核心从属不同的 Node，每个 Node 都有自己的集成内存控制器（IMC，Integrated Memory Controller）。</p><p>如果一个线程在 CPU0 上运行，却频繁访问挂载在 CPU1 上的内存，性能会显著下降，因为数据必须通过处理器间互连（如 Intel 的 UPI 或 AMD 的 Infinity Fabric）传输，这会引入额外的延迟。</p><p>不当的内存放置策略可能导致严重的性能瓶颈，甚至超过内存墙的限制。</p><h3 id=numa-optimization-withnumactl>NUMA Optimization with <code>numactl</code><a hidden class=anchor aria-hidden=true href=#numa-optimization-withnumactl>#</a></h3><p>为了在 NUMA 架构下获得最佳性能，我们必须确保计算尽可能地在靠近其所访问数据的 CPU 核心上进行。<code>numactl</code> 是一个强大的 Linux 命令行工具，它允许我们精确控制进程的 CPU 亲和性和内存分配策略。</p><ul><li><strong>查看 NUMA 节点布局</strong>：<code>numactl --hardware</code> 命令可以显示系统中所有的 NUMA 节点、每个节点的 CPU 核心及其本地内存大小。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>numactl --hardware
</span></span></code></pre></td></tr></table></div></div><p>输出示例：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>available: 2 nodes (0-1)
</span></span><span class=line><span class=cl>node 0 cpus: 0 1 2 3
</span></span><span class=line><span class=cl>node 0 size: 16000 MB
</span></span><span class=line><span class=cl>node 0 free: 15000 MB
</span></span><span class=line><span class=cl>node 1 cpus: 4 5 6 7
</span></span><span class=line><span class=cl>node 1 size: 16000 MB
</span></span><span class=line><span class=cl>node 1 free: 15000 MB
</span></span><span class=line><span class=cl>node distances:
</span></span><span class=line><span class=cl>node   0   1
</span></span><span class=line><span class=cl>  0:  10  21
</span></span><span class=line><span class=cl>  1:  21  10
</span></span></code></pre></td></tr></table></div></div><p>这表示系统有 2 个 NUMA 节点（0 和 1）。节点 0 拥有 CPU 核心 0-3，节点 1 拥有 CPU 核心 4-7。<code>node distances</code> 表示访问本地内存的成本为 10，访问远端内存的成本为 21，远端访问的开销约为本地的两倍。</p><ul><li><strong>重要 <code>numactl</code> 选项</strong>：<ul><li><code>--cpunodebind &lt;nodes></code>：将进程或线程绑定到指定 NUMA 节点上的 CPU 核心。例如，<code>--cpunodebind=0</code> 将进程限制在节点 0 上的 CPU。</li><li><code>--membind &lt;nodes></code>：强制所有内存分配都来自指定 NUMA 节点。例如，<code>--membind=1</code> 将所有内存都从节点 1 分配。</li><li><code>--localalloc</code>：在当前线程运行的 NUMA 节点上分配内存。这是最佳实践，因为它确保了数据存储在距离计算最近的位置。</li><li><code>--physcpubind &lt;cpus></code>：将进程或线程绑定到特定的物理 CPU 核心。</li></ul></li></ul><h3 id=numa-memory-access-test>NUMA Memory Access Test<a hidden class=anchor aria-hidden=true href=#numa-memory-access-test>#</a></h3><p>可以通过一个简单的多线程数组求和程序来演示 <code>numactl</code> 对 NUMA 性能的影响。程序会分配一个非常大（足够超出 cache）的数组，然后使用 OpenMP 让多个线程并行计算数组元素的总和。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;iostream&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;vector&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;numeric&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;chrono&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;omp.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;algorithm&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=c1>// 确保足够大以超出缓存并触及 NUMA 效应
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>size_t</span> <span class=n>ARRAY_SIZE</span> <span class=o>=</span> <span class=mi>1000000000ULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Allocating array of &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>ARRAY_SIZE</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>long</span> <span class=kt>long</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mf>1024.0</span><span class=p>)</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; GB...&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>long</span> <span class=kt>long</span><span class=o>&gt;</span> <span class=n>data</span><span class=p>(</span><span class=n>ARRAY_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=cp>#pragma omp parallel for
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>ARRAY_SIZE</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>100</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Array initialized.&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>num_threads</span> <span class=o>=</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>omp_set_num_threads</span><span class=p>(</span><span class=n>num_threads</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Using &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>num_threads</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; OpenMP threads.&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>long</span> <span class=kt>long</span> <span class=n>total_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>start</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>chrono</span><span class=o>::</span><span class=n>high_resolution_clock</span><span class=o>::</span><span class=n>now</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>    <span class=cp>#pragma omp parallel reduction(+:total_sum)
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>thread_id</span> <span class=o>=</span> <span class=n>omp_get_thread_num</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=n>size_t</span> <span class=n>chunk_size</span> <span class=o>=</span> <span class=n>ARRAY_SIZE</span> <span class=o>/</span> <span class=n>num_threads</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>size_t</span> <span class=n>start_idx</span> <span class=o>=</span> <span class=n>thread_id</span> <span class=o>*</span> <span class=n>chunk_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>size_t</span> <span class=n>end_idx</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>min</span><span class=p>(</span><span class=n>start_idx</span> <span class=o>+</span> <span class=n>chunk_size</span><span class=p>,</span> <span class=n>ARRAY_SIZE</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Thread &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>thread_id</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; processing from &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>start_idx</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; to &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>end_idx</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=n>size_t</span> <span class=n>i</span> <span class=o>=</span> <span class=n>start_idx</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>end_idx</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>total_sum</span> <span class=o>+=</span> <span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>end</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>chrono</span><span class=o>::</span><span class=n>high_resolution_clock</span><span class=o>::</span><span class=n>now</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>chrono</span><span class=o>::</span><span class=n>duration</span><span class=o>&lt;</span><span class=kt>double</span><span class=o>&gt;</span> <span class=n>diff</span> <span class=o>=</span> <span class=n>end</span> <span class=o>-</span> <span class=n>start</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Calculated total sum: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>total_sum</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>cout</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Time taken: &#34;</span> <span class=o>&lt;&lt;</span> <span class=n>diff</span><span class=p>.</span><span class=n>count</span><span class=p>()</span> <span class=o>&lt;&lt;</span> <span class=s>&#34; seconds&#34;</span> <span class=o>&lt;&lt;</span> <span class=n>std</span><span class=o>::</span><span class=n>endl</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>可以使用 GCC 编译这个程序：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>g++ -std<span class=o>=</span>c++11 -O3 -fopenmp numa_test.cpp -o numa_test
</span></span></code></pre></td></tr></table></div></div><p>（由于我的电脑只有一个 NUMA 核心，所以下面测试无法进行。。。）</p><ul><li><strong>Baseline</strong>：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./numa_test
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>远端内存访问</strong>：CPU 在节点 1，内存绑定到节点 0。这时所有数据都是远端访问，理论上性能应该最差。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>numactl --cpunodebind<span class=o>=</span><span class=m>1</span> --membind<span class=o>=</span><span class=m>0</span> ./numa_test
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>本地内存访问</strong>：CPU 在节点 0，内存绑定到节点 0。这是理性的 NUMA 配置，所有数据访问都是本地的。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>numactl --cpunodebind<span class=o>=</span><span class=m>0</span> --membind<span class=o>=</span><span class=m>0</span> ./numa_test
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>真实多线程场景</strong>：CPU 绑定到节点 0 和 1，但内存仅分配到节点 0。跑在节点 1 上的线程将进行远端内存访问。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OMP_NUM_THREADS</span><span class=o>=</span><span class=m>2</span>
</span></span><span class=line><span class=cl>numactl --cpunodebind<span class=o>=</span>0,1 --membind<span class=o>=</span><span class=m>0</span> ./numa_test
</span></span></code></pre></td></tr></table></div></div><h2 id=prefetching>Prefetching<a hidden class=anchor aria-hidden=true href=#prefetching>#</a></h2><p><strong>预取 (Prefetching)</strong> 是一种技术，它尝试在 CPU 实际需要数据之前，就将其从较慢的内存层级加载到较快的缓存中。这有助于隐藏内存访问延迟，使 CPU 能够专注于计算。</p><ul><li><p><strong>硬件预取器 (Hardware Prefetcher)</strong>： 现代 CPU 内置的智能逻辑单元，它们会监控内存访问模式，并根据检测到的模式（如顺序访问）自动预测接下来可能需要哪些数据，将其提前加载到缓存中。</p><ul><li>优点：全自动，无需程序员干预。</li><li>缺点：有时预测不准确，可能将无用数据加载到缓存中，挤出有用数据，甚至增加内存总线流量。</li></ul></li><li><p><strong>编译器预取 (Compiler Prefetching)</strong>： 一些编译器能够根据代码中的循环和访问模式，在编译时插入预取指令。通过 <code>-O3</code> 等优化选项或特定的编译器提示，可以启用此功能。</p></li><li><p><strong>软件预取 (Software Prefetching)</strong>： 程序员可以通过使用特殊的 CPU 指令（通常通过内联函数 Intrinsics 暴露）显式地告诉 CPU 预取哪些数据。 例如，在 x86 架构上：</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;xmmintrin.h&gt;</span><span class=cp> </span><span class=c1>// For _mm_prefetch
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>process_data</span><span class=p>(</span><span class=kt>int</span><span class=o>*</span> <span class=n>arr</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// 在实际访问 arr[i + PREFETCH_DISTANCE] 之前提前预取
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>if</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=n>PREFETCH_DISTANCE</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>_mm_prefetch</span><span class=p>((</span><span class=kt>char</span><span class=o>*</span><span class=p>)</span><span class=o>&amp;</span><span class=n>arr</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=n>PREFETCH_DISTANCE</span><span class=p>],</span> <span class=n>_MM_HINT_T0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=c1>// 处理 arr[i]
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>当硬件预取器无法有效应对复杂的访问模式时，软件预取可以提供更精确的控制。</li><li>需要程序员手动插入，可能会增加代码复杂性，不当使用可能导致性能下降。</li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>在 HPC 领域，仅依靠 CPU 的原始计算能力和并行编程模型是不够的。深入理解计算机内存，是编写高性能代码的基础。通过采用缓存友好的编程，如优化数据布局和分块算法，我们可以显著提高应用程序的性能，真正发挥现代 CPU 的潜力。</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://zhuanlan.zhihu.com/p/336365600>每个程序员都应该知道的 CPU 知识：NUMA</a>（知乎）</li><li><a href=https://zhuanlan.zhihu.com/p/67558970>浅解 NUMA 机制</a>（知乎）</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://diefish1024.github.io/tags/cs/>CS</a></li><li><a href=https://diefish1024.github.io/tags/hpc/>HPC</a></li><li><a href=https://diefish1024.github.io/tags/cpu/>CPU</a></li></ul><nav class=paginav><a class=prev href=https://diefish1024.github.io/posts/hpc/gemm-%E7%AE%97%E6%B3%95%E4%BC%98%E5%8C%96/><span class=title>« Prev</span><br><span>GEMM 算法优化</span>
</a><a class=next href=https://diefish1024.github.io/posts/solutions/xflops2024-bithack/><span class=title>Next »</span><br><span>Xflops2024-Bithack</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on x" href="https://x.com/intent/tweet/?text=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f&amp;hashtags=CS%2cHPC%2cCPU"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f&amp;title=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98&amp;summary=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98&amp;source=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f&title=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on whatsapp" href="https://api.whatsapp.com/send?text=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98%20-%20https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on telegram" href="https://telegram.me/share/url?text=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 关于内存 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%85%b3%e4%ba%8e%e5%86%85%e5%ad%98&u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fhpc%2f%25E5%2585%25B3%25E4%25BA%258E%25E5%2586%2585%25E5%25AD%2598%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://diefish1024.github.io/>diefish's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>