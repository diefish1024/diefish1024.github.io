<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>EmT | diefish's blog</title><meta name=keywords content="literature-note,EEG,emotion-recognition,Transformer,GNN"><meta name=description content="Abstract

问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱
方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构
核心模块：

TGC：将 EEG 信号转换为时序图序列
RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心）
TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心）
TSO：输出分类/回归结果


成果：在多个公开数据集的广义跨被试任务上面超过了 baseline

Introduction & Related Work
为什么 EEG 难以使用跨被试 (cross-subject) 的场景？

个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同
低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……）

目标是学习一种跨被试共享、具有泛化能力的情绪表征
Gpaph Neural Networks

核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理
代表工作：

ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层
GCN：通过局部一阶聚合近似光谱滤波
DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补



Temporal Context Learning

核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息
代表工作：

LSTM / TCN / TESANet / Conformer / AMDET
局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息



EEG Emotion Recognition

核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征
代表工作：

GCB-Net / TSception
局限性：没有关注长时序上下文信息



Method
EmT 是一个端到端的框架，包含四大模块："><meta name=author content="diefish"><link rel=canonical href=https://diefish1024.github.io/posts/literature-notes/emt/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://diefish1024.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=mask-icon href=https://diefish1024.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://diefish1024.github.io/posts/literature-notes/emt/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://diefish1024.github.io/posts/literature-notes/emt/"><meta property="og:site_name" content="diefish's blog"><meta property="og:title" content="EmT"><meta property="og:description" content="Abstract 问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱 方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构 核心模块： TGC：将 EEG 信号转换为时序图序列 RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心） TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心） TSO：输出分类/回归结果 成果：在多个公开数据集的广义跨被试任务上面超过了 baseline Introduction & Related Work 为什么 EEG 难以使用跨被试 (cross-subject) 的场景？
个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同 低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……） 目标是学习一种跨被试共享、具有泛化能力的情绪表征
Gpaph Neural Networks 核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理 代表工作： ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层 GCN：通过局部一阶聚合近似光谱滤波 DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补 Temporal Context Learning 核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息 代表工作： LSTM / TCN / TESANet / Conformer / AMDET 局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息 EEG Emotion Recognition 核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征 代表工作： GCB-Net / TSception 局限性：没有关注长时序上下文信息 Method EmT 是一个端到端的框架，包含四大模块："><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-30T12:55:00+08:00"><meta property="article:modified_time" content="2025-07-30T12:55:00+08:00"><meta property="article:tag" content="Literature-Note"><meta property="article:tag" content="EEG"><meta property="article:tag" content="Emotion-Recognition"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="GNN"><meta property="og:image" content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:title content="EmT"><meta name=twitter:description content="Abstract

问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱
方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构
核心模块：

TGC：将 EEG 信号转换为时序图序列
RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心）
TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心）
TSO：输出分类/回归结果


成果：在多个公开数据集的广义跨被试任务上面超过了 baseline

Introduction & Related Work
为什么 EEG 难以使用跨被试 (cross-subject) 的场景？

个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同
低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……）

目标是学习一种跨被试共享、具有泛化能力的情绪表征
Gpaph Neural Networks

核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理
代表工作：

ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层
GCN：通过局部一阶聚合近似光谱滤波
DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补



Temporal Context Learning

核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息
代表工作：

LSTM / TCN / TESANet / Conformer / AMDET
局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息



EEG Emotion Recognition

核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征
代表工作：

GCB-Net / TSception
局限性：没有关注长时序上下文信息



Method
EmT 是一个端到端的框架，包含四大模块："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://diefish1024.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Notes","item":"https://diefish1024.github.io/posts/literature-notes/"},{"@type":"ListItem","position":3,"name":"EmT","item":"https://diefish1024.github.io/posts/literature-notes/emt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"EmT","name":"EmT","description":"Abstract 问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱 方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构 核心模块： TGC：将 EEG 信号转换为时序图序列 RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心） TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心） TSO：输出分类/回归结果 成果：在多个公开数据集的广义跨被试任务上面超过了 baseline Introduction \u0026amp; Related Work 为什么 EEG 难以使用跨被试 (cross-subject) 的场景？\n个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同 低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……） 目标是学习一种跨被试共享、具有泛化能力的情绪表征\nGpaph Neural Networks 核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理 代表工作： ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层 GCN：通过局部一阶聚合近似光谱滤波 DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补 Temporal Context Learning 核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息 代表工作： LSTM / TCN / TESANet / Conformer / AMDET 局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息 EEG Emotion Recognition 核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征 代表工作： GCB-Net / TSception 局限性：没有关注长时序上下文信息 Method EmT 是一个端到端的框架，包含四大模块：\n","keywords":["literature-note","EEG","emotion-recognition","Transformer","GNN"],"articleBody":"Abstract 问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱 方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构 核心模块： TGC：将 EEG 信号转换为时序图序列 RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心） TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心） TSO：输出分类/回归结果 成果：在多个公开数据集的广义跨被试任务上面超过了 baseline Introduction \u0026 Related Work 为什么 EEG 难以使用跨被试 (cross-subject) 的场景？\n个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同 低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……） 目标是学习一种跨被试共享、具有泛化能力的情绪表征\nGpaph Neural Networks 核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理 代表工作： ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层 GCN：通过局部一阶聚合近似光谱滤波 DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补 Temporal Context Learning 核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息 代表工作： LSTM / TCN / TESANet / Conformer / AMDET 局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息 EEG Emotion Recognition 核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征 代表工作： GCB-Net / TSception 局限性：没有关注长时序上下文信息 Method EmT 是一个端到端的框架，包含四大模块：\nraw EEG -\u003e TGC -\u003e 时序图 -\u003e RMPG -\u003e Tokens -\u003e TCT -\u003e 深层特征 -\u003e TSO -\u003e Result\nEEG-Temporal-Graph Representations (TGC) 目标：将连续的 EEG 信号转化为结构化的时序图序列\n图：每个“图”是指在一个短的窗口内大脑状态的数学表示，图中的每个节点对应一个 EEG 电极通道，节点的特征是该通道在 7 个不同的频段上的 rPSD 时序序列：序列是通过滑动窗口技术截取的一段较长的 EEG 数据里面切分成许多重叠的短的子片段，每个子片段生成的图所形成的序列 双层滑动窗口分段：为了捕捉不同时间尺度上的信息，TGC 采用了一种双层分段策略\n首先将一次完整实验的 (trail) 的 EEG 数据，表示为 $ X \\in \\mathbb{R}^{c \\times L} $ （其中 $ c $ 为通道数，$ L $ 为总采样点数），通过一个较长的滑动窗口（长度为 $ l $ ，步长为 $ s $ ）分割成多个重叠的长片段 $ \\overline{X} \\in \\mathbb{R}^{c \\times l} $ 接着对于每一个长片段 $ \\overline{X} $ 使用一个更短的滑动窗口（长度为 $ l' $ ，步长为 $ s' $ ）将其分割为一系列子片段 $ \\tilde{X} \\in \\mathbb{R}^{c \\times l'} $ 这使得模型能够在一个长片段的标签下，观察到内部更精细的信号动态变化，为后续的 Transformer 模块捕捉时间上下文提供了基础 节点特征提取：对于每一个子片段 $ \\tilde{X} $，需要为其对应的图节点（即 EEG 通道）提取有意义的特征，论文选择了相对功率谱密度 (Relative PSD, rPSD) 作为节点属性\n具体地，使用 welch’s method 计算每个 EEG 通道在七个经典频带上的 rPSD 这样，每个子片段 $ \\tilde{X} $ 都对应一个特征矩阵 $ F \\in \\mathbb{R}^{c \\times f} $，其中 $ f=7 $ 最终，一个长片段 $ \\overline{X} $ 被转换成一个按时间顺序排列的图序列 $ G_{T} = \\{\\mathcal{G}^{i}\\} \\in \\mathbb{R}^{seq \\times c \\times f} $，其中 $ seq $ 是子片段的数量。这个时间图序列就是 RMPG 模块的输入\nResidual Multiview Pyramid GCN (RMPG) 核心：解决传统 GNN“单一视角”问题，为时间图序列 $ G_{T} $​ 中的每一个图 $ \\mathcal{G}^i $ 学习一个丰富的、多层次的空间表征，并将其压缩成一个单一的 token ，以供后续的 TCT 模块处理\nRMPG 模块由一个基础的图编码器 $ \\Phi_{g}(\\cdot) $ 构成，文中采用了 ChebyNet 作为基础图编码器，对于给定特征输入 $ F^{m-1} $ 和邻接矩阵 $ A $ （通过拉普拉斯算子 $ \\hat{L} $ 转换） $$ \\Phi_{g}(F^{m}, A) = \\sigma\\left( \\sum_{k=0}^{K-1}\\theta_{k}^{m}T_{k}(\\hat{L})F^{m-1} - b^{m}\\right) $$ 其中 $ m $ 为 GCN 层的索引，$ \\sigma $ 为 ReLU 激活函数，$ \\theta $ 为参数，$ T_{k} $ 为 $ k $ 阶 Chebyshev 多项式\n多视图学习 (Multiview Learning) ：为了模拟情绪背后多种认知子过程驱动的不同大脑连接模式，RMPG 并非使用单一的图卷积网络，而是并行地使用了多个 GCN 分支，$ \\{ \\Phi_{g}^{0}(\\cdot), \\Phi_{g}^{1}(\\cdot), \\dots, \\Phi_{g}^{i}(\\cdot) \\} $\n每个分支都拥有一个独立的可学习邻接矩阵 $ A^{i} \\in \\mathbb{R}^{c \\times c} $ ，能在模型训练过程中通过梯度反向传播进行端到端的优化 这意味着每个 GCN 分支都能从数据中学习到一种独特的大脑功能连接“视图” 金字塔学习 (Pyramid Learning) ：为了捕捉不同尺度的空间信息，并行的 GCN 分支被设计成具有不同的深度（即 GCN 层数）\n较浅的 GCN 能够有效地聚合全局的、跨脑区的功能连接信息，聚合远距离节点的信息而不过度平滑 较深的 GCN 能够更好地聚合局部邻域内的信息，在脑区内部形成一致的表征 GCN 的深度越深，其输出所代表的特征金字塔层级越高。 残差连接 (Residual) ：除了并行的 GCN 分支外，RMPG 还包含一个并行的线性残差分支。该分支直接对原始的时序图 $ G $ （或者对应的特征矩阵 $ F $ ）进行线性投影，不经过任何图卷积，从而保留最原始的节点信息，作为特征金字塔的“基座”\n线性投影层 $ LP(\\cdot) $ 将扁平化的图表示投影到隐藏嵌入 $ H_{g}^{i} \\in \\mathbb{R}^{d_{g}} $ 堆叠来自不同层的 GCN 的并行输出，得到多金字塔视图嵌入 $$ \\{ H_{g}^{i} \\} = \\{ LP^{i}(\\Gamma(\\Phi_{g}^{i}(F, A^{i}))) \\} $$ 其中 $ \\Gamma(\\cdot) $ 是扁平操作，$ \\{ \\cdot \\} $ 是堆叠操作 特征融合与 Token 生成：最终，对于图序列中的每一个图 $ G_{i} $，其所有 GCN 视图的输出 $ \\{ H_{i}^{g} \\} $ 和残差基座 $ H_{g-\\text{base}} $​ 通过一个 mean fusion 操作合并成当前时间步 $ i $ 的最终 token $ s_{i} $ $$ s_{i} = \\text{mean}(\\{ H_{g-\\text{base}}, H_{g}^{0}, H_{g}^{1}, \\dots, H_{g}^{i} \\}) $$\n通过 RMPG 模块，输入的图序列 $ G_{T} $ 被高效地转化为一个 token 序列 $ S_{T}=\\{ s^{i} \\} \\in \\mathbb{R}^{seq \\times d_{g}} $ ，这个序列既蕴含了每个时刻丰富的多视图、多层次空间信息，又具备了适合 Transformer 处理的格式\nTemporal Contextual Transformer (TCT) 核心：接受由 RMPG 生成的 token 序列 $ S_{T} $ ，并利用 Transformer 的结构来高效捕捉这些 token 之间的时间依赖关系；与标准的 Transformer 不同，TCT 引入了两种为 EEG 情绪识别任务定制的 Token Mixer，分别用于分类和回归任务\nTCT 模块由多个堆叠的 Transformer Block 组成，对于输入 token 序列 $ Z^{m} $ （ $ Z^{0} = S_{T} $ ），每个 Block 的计算过程为 $$ Z^{m'} = \\text{TokenMixer}(\\text{Norm}(Z^{m})) + Z^{m} $$ $$ Z^{m+1} = \\text{MLP}(\\text{Norm}(Z^{m'})) + Z^{m'} $$ 其中 $ m $ 是层的索引，MLP 是带 ReLU 激活函数的两层感知机\n$ \\text{TokenMixer}_{\\text{clas}} $ for Classification Tasks 旨在捕捉随时间变化的长短时序上下文信息\n多头自注意力 (Multi-head Self-Attention, MSA) ：用于全局地关注序列中与整体情绪状态高度相关的部分 $$ \\text{Attn}(Q,K,V) = \\text{softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} \\right)V $$\n并行应用多个注意力头（每个头有独立的 $ LP_h(\\cdot) $ 来生成 $ Q, K, V $），然后将所有头的输出拼接：$$ \\text{MSA}(S_{T}) = \\text{Concat}(\\text{Attn}(LP_0(S_{T})), ..., \\text{Attn}(LP_{n_{\\text{head}}-1}(S_{T}))) $$ 其中 $ S_{T} $ 是 token 序列，$ n_{\\text{head}} $ 是注意力头的数量 短期聚合层 (Short-Time Aggregation, STA) ：\n基于“情绪短期连续而长期变化”的先验知识，STA 在 MSA 之后应用来学习短期的上下文信息 首先对 MSA 的输出 $ H_{\\text{attn}} \\in \\mathbb{R}^{n_{\\text{head}} \\times \\text{seq} \\times d_{\\text{head}}} $ 应用一个带有比例因子 $ \\alpha $ 的 Dropout 层 $ \\text{dp}(\\alpha) $ 接着，通过 Conv2D 聚合 $ n_{\\text{anchor}} $ 个时序近邻 Conv2D 的卷积核 $ K_{\\text{cnn}} $ 的尺寸为 $ (n_{\\text{anchor}}, 1) $，步长为 $ (1,1) $ 最后，卷积的输出会被 Reshape 并进行线性投影（$ W_{\\text{sta}} $） 可以描述为 $$ \\text{STA}(H_{\\text{attn}}) = \\text{Reshape}(\\text{Conv2D}(\\text{dp}(H_{\\text{attn}}), K_{\\text{cnn}})) W_{\\text{sta}} $$ $ \\text{Reshape}(\\cdot) $ 将维度从 $ (n_{\\text{head}},\\text{seq},d_{\\text{head}}) $ 转换为 $ (\\text{seq}, n_{\\text{head}} \\cdot d_{\\text{head}}) $ ， $ W_{\\text{sta}} \\in \\mathbb{R}^{n_{\\text{head}} \\cdot d_{\\text{head}} \\times d_g} $ 是投影权重矩阵 $ \\text{TokenMixer}_{\\text{Clas}} $ 最终由上述两个模块串联构成 $$ \\text{TokenMixer}_{\\text{clas}}(S_{T}) = \\text{STA}(\\text{MSA}(S_{T})) $$\n$ \\text{TokenMixer}_{\\text{regr}} $ for Regression Tasks 旨在预测序列中情绪状态的连续变化\n不同于分类任务：分类任务的目标通常是从整个序列中提取几个核心特征来判断整体情绪状态，这通过 MSA 聚焦于重要部分是有效的；然而回归任务需要模型对序列中每个时步的连续情绪变化进行预测，因此不使用全局的 MSA，而是采用了一种基于 RNN 的混合器（ RNN family ）\nRNN Mixer ：\nRNN 结构天然适合处理连续序列的演变过程，能够更好地建模情绪值的平滑变化 经验性选择的两层双向 GRU (bi-directional GRU) 作为 $ \\text{TokenMixer}_{\\text{regr}} $ ，输出长度为 $ 2 \\times d_{\\text{head}} $ 计算过程为 $$ \\text{TokenMixer}_{\\text{regr}}(S_T) = \\text{RNNs}(\\text{LP}(S_T)) $$ 其中 $ LP(S_{T}) = S_{T}W_{v} $ ，把 token 序列投影为 $ V $ 值\nTSO Module 头部接收来自不同 Token Mixer 的输出：用于分类的 $ S_{\\text{clas}} $ 和用于回归的 $ S_{\\text{regr}} $\n分类任务：对所有 token 进行 mean fusion ，再通过线性层得到最终 logits $$ \\hat{Y}_{\\text{clas}} = \\text{mean}(S_{\\text{clas}})W_{\\text{clas}} + b_{\\text{clas}} $$ 其中 $ S_{\\text{clas}} \\in \\mathbb{R}^{\\text{seq} \\times d_{\\text{head}}} $ ，$ W_{\\text{clas}} \\in \\mathbb{R}^{d_{\\text{head}} \\times d_{\\text{class}}} $ ，$ b_{\\text{clas}} \\in \\mathbb{R}^{n_{\\text{class}}} $\n回归任务：直接将每个时间步的 token 特征通过线性层，得到对应每个时刻的回归值 $$ \\hat{Y}_{\\text{regr}} = S_{\\text{regr}}W_{\\text{regr}} + b_{\\text{regr}} $$ 其中 $ S_{\\text{regr}} \\in \\mathbb{R}^{\\text{seq} \\times 2\\cdot d_{\\text{head}}} $ （双向），$ W_{\\text{regr}} \\in \\mathbb{R}^{2\\cdot d_{\\text{head}} \\times 1} $ ，$ b_{\\text{regr}} \\in \\mathbb{R} $\nExperiment Datasets 使用 SEED, THU-EP, FACED, MAHNOB-HCI 四个公开数据集\n其中 SEED 数据集使用了 0.3-50 Hz 的带通滤波\nEEG-Temporal-Graph 长片段：窗口长度 $ l=20\\,\\mathrm{s} $ （即 $ 20 \\times f_{s} $ ），步长 $ s=4\\,\\mathrm{s} $ 子片段： 对于 SEED 和 THU-EP： $ l'=2\\,\\mathrm{s},s'=0.5\\,\\mathrm{s} $ 对于 FACED： $ l'=4\\,\\mathrm{s},s'=1\\,\\mathrm{s} $ Settings 数据划分： SEED：采用留一被试交叉验证，每次迭代一个被试的数据作为测试集，剩余数据中 80% 作为训练集，20% 作为验证集 THU-EP 和 FACED：采用留 $ n $ 被试交叉验证，其中 $ n_{\\text{THU-EP}}=8,n_{\\text{FACED}}=12 $ ，训练数据中 10% 作为验证集 分类任务：对 SEED、THU-EP 和 FACED 进行积极/消极情绪的二分类，THU-EP 和 FACED 的效价分数通过阈值 3.0 划分为高/低效价 回归任务：MAHNOB-HCI 并进行 LOSO 验证 Evaluation Metrics Classfication Accuracy：$$ \\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} $$ F1 Score：$$ \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}} = \\frac{\\text{TP}}{\\text{TP} + \\frac{1}{2}(\\text{FP} + \\text{FN})} $$ 其中 $ \\text{TP} $ 表示真阳性，$ \\text{TN} $ 表示真阴性，$ \\text{FP} $ 表示假阳性，$ \\text{FN} $ 表示假阴性\nRegression 给定预测值 $ \\hat{y} $ 和连续标签 $ y $ ：\n均方根误差 (RMSE) ：$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} (\\hat{y}_i - y_i)^2} $$ 皮尔逊相关系数 (PCC) ：$$ \\text{PCC} = \\frac{\\sigma_{\\hat{y}y}}{\\sigma_{\\hat{y}} \\sigma_y} = \\frac{\\sum_{i=0}^{N-1} (\\hat{y}_i - \\mu_{\\hat{y}})(\\hat{y}_i - \\mu_y)}{\\sqrt{\\sum_{i=0}^{N-1} (\\hat{y}_i - \\mu_{\\hat{y}})^2} \\sqrt{\\sum_{i=0}^{N-1} (y_i - \\mu_y)^2}} $$ 一致性相关系数 (CCC) ：$$ \\text{CCC} = \\frac{2\\sigma_{\\hat{y}y}}{\\sigma^2_{\\hat{y}} + \\sigma^2_y + (\\mu_{\\hat{y}} - \\mu_y)^2} $$ 其中 $ N $ 是向量中的元素数量，$ \\sigma_{\\hat{y}y} $ 是协方差，$ \\sigma_{\\hat{y}} $ 和 $ \\sigma_y $ 是标准差，$ \\mu_{\\hat{y}} $ 和 $ \\mu_y $ 是均值\nImplementation Details 模型的三种变体： Analyses Classification SEED： EmT-D 表现最佳，EmT-B 和 EmT-S 表现也良好，RGNN 表现第二佳 使用特征作为输入的模型通常优于直接使用 EEG 信号作为输入的模型，从时序特征而非直接特征学习通常能取得更好的性能（RGNN 除外），这表明了学习时序上下文信息的有效性，EmT 借助基于 GCN 的模块，能更好地学习空间信息 THU-EP / FACED： THU-EP：EmT-B 取得了最佳 F1 分数，Conformer 取得了最佳 ACC FACED：EmT-B 取得了第二佳 ACC 和最佳 F1 分数 由于类不平衡，F1 分数比 ACC 更重要，EmT-B 在这两个数据集上均取得了最高 F1 分数 与 SEED 不同，直接使用 EEG 作为输入的 baseline 模型表现更好，这可能因为这两个数据集的被试人数更多 Features： SEED (62 channels，15 subjects) ：EmT-D (8 层) 表现最佳，说明 Transformer 层数越多，学到的空间信息越丰富 THU-EP 和 FACED (32 channels，more subjects) ：EmT-B (4 层) 表现更好，通道数少且被试间变异性大时，更深的模型（EmT-D）容易过拟合 Regression 在 MAHNOB-HCI 数据集上，EmT-Regr (LP+LSTM) 取得了最低的 RMSE，而 EmT-Regr (LP+GRU) 取得了最佳的 PCC 和 CCC 使用 MSA 作为 Token Mixer 时，模型性能急剧下降，甚至低于所有基线模型 这表明对于回归任务，融合所有片段的信息至关重要，而 RNN 的顺序信息融合能力比 MSA 更适合建模连续的情绪变化 Ablation Study Effect of EEG Features Effect of The Depth and Width of GCNs in RMPG Depth：增加 GCN 层的数量会导致性能显著下降，这与更深 GCN 中存在的过平滑问题一致 Width：宽度从 8 增加到 32 时，性能呈正相关；当宽度进一步增加时，性能下降，这可能是由更大的模型尺寸导致的过拟合 Effect of The Number of TCT Blocks Classfication (SEED) ：TCT 块的数量从 2 增加到 8 时，ACC 和 F1 分数均显著提高，这表明增加 TCT 块数能增强模型捕捉时序上下文信息的能力，从而提升分类性能 Regression (MAHNOB-HCI) ：TCT 块的数量对性能指标几乎没有影响 Visualization Learned Connections 在 SEED 数据集上学习到的两种不同连接模式的可视化证据 在 SEED 数据集上，两个可学习的邻接矩阵揭示了情绪认知过程中不同的连接模式：\n(a) 中主要关注额叶、顶叶和颞叶区域之间的连接，这些区域与心理注意力密切相关 (b) 中则包括额叶、颞叶和顶叶区域之间的互动（与情绪相关），以及枕叶和顶叶区域的互动（与视觉过程相关，因为刺激为视频） Learned Temporal Contextual Information Classfication (FACED) Regression 分类任务在 TCT 块之前，特征随时间变化，TCT 块之后，激活变得更加一致；这可能是因为自注意力机制关注与整体情绪状态高度相关的部分，而 STA 层通过聚合邻近的时序信息来平滑波动 回归任务：特征空间也显示出时序变化；与分类不同，回归特征并非简单平滑，形成了更复杂的表示 总结：TCT 块处理分类和回归任务的方式不同：分类中，特征被平滑以增强可分离性；回归中，RNN Token Mixer 保留了时序变化，从而实现连续情绪预测 ","wordCount":"1186","inLanguage":"en","image":"https://diefish1024.github.io/images/avatar.jpg","datePublished":"2025-07-30T12:55:00+08:00","dateModified":"2025-07-30T12:55:00+08:00","author":{"@type":"Person","name":"diefish"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://diefish1024.github.io/posts/literature-notes/emt/"},"publisher":{"@type":"Organization","name":"diefish's blog","logo":{"@type":"ImageObject","url":"https://diefish1024.github.io/images/avatar.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://diefish1024.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://diefish1024.github.io/images/avatar.jpg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://diefish1024.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://diefish1024.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://diefish1024.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://diefish1024.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://diefish1024.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://diefish1024.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/literature-notes/>Literature Notes</a></div><h1 class="post-title entry-hint-parent">EmT</h1><div class=post-meta><span title='2025-07-30 12:55:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;diefish&nbsp;|&nbsp;<a href=https://github.com/diefish1024/diefish1024.github.io/blob/main/content/posts/Literature%20Notes/emt.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#abstract aria-label=Abstract>Abstract</a></li><li><a href=#introduction--related-work aria-label="Introduction & Related Work">Introduction & Related Work</a><ul><li><a href=#gpaph-neural-networks aria-label="Gpaph Neural Networks">Gpaph Neural Networks</a></li><li><a href=#temporal-context-learning aria-label="Temporal Context Learning">Temporal Context Learning</a></li><li><a href=#eeg-emotion-recognition aria-label="EEG Emotion Recognition">EEG Emotion Recognition</a></li></ul></li><li><a href=#method aria-label=Method>Method</a><ul><li><a href=#eeg-temporal-graph-representations-tgc aria-label="EEG-Temporal-Graph Representations (TGC)">EEG-Temporal-Graph Representations (TGC)</a></li><li><a href=#residual-multiview-pyramid-gcn-rmpg aria-label="Residual Multiview Pyramid GCN (RMPG)">Residual Multiview Pyramid GCN (RMPG)</a></li><li><a href=#temporal-contextual-transformer-tct aria-label="Temporal Contextual Transformer (TCT)">Temporal Contextual Transformer (TCT)</a><ul><li><a href=#hahahugoshortcode15s52hbhb-for-classification-tasks aria-label="$ \text{TokenMixer}_{\text{clas}} $ for Classification Tasks">$ \text{TokenMixer}_{\text{clas}} $ for Classification Tasks</a></li><li><a href=#hahahugoshortcode15s74hbhb-for-regression-tasks aria-label="$ \text{TokenMixer}_{\text{regr}} $ for Regression Tasks">$ \text{TokenMixer}_{\text{regr}} $ for Regression Tasks</a></li></ul></li><li><a href=#tso-module aria-label="TSO Module">TSO Module</a></li></ul></li><li><a href=#experiment aria-label=Experiment>Experiment</a><ul><li><a href=#datasets aria-label=Datasets>Datasets</a><ul><li><a href=#eeg-temporal-graph aria-label=EEG-Temporal-Graph>EEG-Temporal-Graph</a></li></ul></li><li><a href=#settings aria-label=Settings>Settings</a></li><li><a href=#evaluation-metrics aria-label="Evaluation Metrics">Evaluation Metrics</a><ul><li><a href=#classfication aria-label=Classfication>Classfication</a></li><li><a href=#regression aria-label=Regression>Regression</a></li></ul></li><li><a href=#implementation-details aria-label="Implementation Details">Implementation Details</a></li></ul></li><li><a href=#analyses aria-label=Analyses>Analyses</a><ul><li><a href=#classification aria-label=Classification>Classification</a></li><li><a href=#regression-1 aria-label=Regression>Regression</a></li><li><a href=#ablation-study aria-label="Ablation Study">Ablation Study</a></li><li><a href=#effect-of-eeg-features aria-label="Effect of EEG Features">Effect of EEG Features</a></li><li><a href=#effect-of-the-depth-and-width-of-gcns-in-rmpg aria-label="Effect of The Depth and Width of GCNs in RMPG">Effect of The Depth and Width of GCNs in RMPG</a></li><li><a href=#effect-of-the-number-of-tct-blocks aria-label="Effect of The Number of TCT Blocks">Effect of The Number of TCT Blocks</a></li><li><a href=#visualization aria-label=Visualization>Visualization</a><ul><li><a href=#learned-connections aria-label="Learned Connections">Learned Connections</a></li><li><a href=#learned-temporal-contextual-information aria-label="Learned Temporal Contextual Information">Learned Temporal Contextual Information</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><ul><li><strong>问题</strong>：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱</li><li><strong>方案</strong>：提出 <strong>Emotion Transformer (EmT)</strong> ，为 Graph-Transformer 混和架构</li><li><strong>核心模块</strong>：<ol><li><strong>TGC</strong>：将 EEG 信号转换为时序图序列</li><li><strong>RMPG</strong>：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心）</li><li><strong>TCT</strong>：使用任务自适应的 Transformer，学习 token 序列上下文（核心）</li><li><strong>TSO</strong>：输出分类/回归结果</li></ol></li><li><strong>成果</strong>：在多个公开数据集的广义跨被试任务上面超过了 baseline</li></ul><h2 id=introduction--related-work>Introduction & Related Work<a hidden class=anchor aria-hidden=true href=#introduction--related-work>#</a></h2><p>为什么 EEG 难以使用跨被试 (cross-subject) 的场景？</p><ul><li><strong>个体差异</strong>：不同被试生理结构和认知策略差异，导致 EEG 模式不同</li><li><strong>低信噪比</strong>：EEG 信号容易受到外源噪声干扰（肌电、眼电……）</li></ul><p>目标是学习一种<strong>跨被试共享</strong>、具有<strong>泛化能力</strong>的情绪表征</p><h3 id=gpaph-neural-networks>Gpaph Neural Networks<a hidden class=anchor aria-hidden=true href=#gpaph-neural-networks>#</a></h3><ul><li><strong>核心思想</strong>：EEG 数据具有非欧图结构，适合使用 GNN 来处理</li><li><strong>代表工作</strong>：<ul><li><strong>ChebyNet</strong>：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层</li><li><strong>GCN</strong>：通过局部一阶聚合近似光谱滤波</li><li><strong>DGCNN / RGNN</strong>：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有<strong>局限性</strong>；而 EmT 通过<strong>多视图可学习邻接矩阵</strong>和<strong>时序图</strong>来弥补</li></ul></li></ul><h3 id=temporal-context-learning>Temporal Context Learning<a hidden class=anchor aria-hidden=true href=#temporal-context-learning>#</a></h3><ul><li><strong>核心理念</strong>：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息</li><li><strong>代表工作</strong>：<ul><li>LSTM / TCN / TESANet / Conformer / AMDET</li><li><strong>局限性</strong>：这些方法通常从扁平化的 EEG 特征向量学习，可能<strong>未能有效学习空间关系</strong>；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息</li></ul></li></ul><h3 id=eeg-emotion-recognition>EEG Emotion Recognition<a hidden class=anchor aria-hidden=true href=#eeg-emotion-recognition>#</a></h3><ul><li><strong>核心理念</strong>：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征</li><li><strong>代表工作</strong>：<ul><li>GCB-Net / TSception</li><li>局限性：没有关注长时序上下文信息</li></ul></li></ul><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>EmT 是一个端到端的框架，包含四大模块：</p><p><code>raw EEG</code> -> <strong>TGC</strong> -> <code>时序图</code> -> <strong>RMPG</strong> -> <code>Tokens</code> -> <strong>TCT</strong> -> <code>深层特征</code> -> <strong>TSO</strong> -> <code>Result</code></p><p><img loading=lazy src=/images/emt/pasted-image-20250730164135-png></p><h3 id=eeg-temporal-graph-representations-tgc>EEG-Temporal-Graph Representations (TGC)<a hidden class=anchor aria-hidden=true href=#eeg-temporal-graph-representations-tgc>#</a></h3><p><strong>目标</strong>：将连续的 EEG 信号转化为结构化的时序图序列</p><ul><li><strong>图</strong>：每个“图”是指在一个短的窗口内大脑状态的数学表示，图中的每个节点对应一个 EEG 电极通道，节点的特征是该通道在 7 个不同的频段上的 <strong>rPSD</strong></li><li><strong>时序序列</strong>：序列是通过滑动窗口技术截取的一段较长的 EEG 数据里面切分成许多重叠的短的子片段，每个子片段生成的图所形成的序列</li></ul><p><img loading=lazy src=/images/emt/pasted-image-20250731233612-png></p><ul><li><p><strong>双层滑动窗口分段</strong>：为了捕捉不同时间尺度上的信息，TGC 采用了一种双层分段策略</p><ul><li>首先将一次完整实验的 (trail) 的 EEG 数据，表示为 $ X \in \mathbb{R}^{c \times L} $ （其中 $ c $ 为通道数，$ L $ 为总采样点数），通过一个较长的滑动窗口（长度为 $ l $ ，步长为 $ s $ ）分割成多个重叠的<strong>长片段</strong> $ \overline{X} \in \mathbb{R}^{c \times l} $</li><li>接着对于每一个长片段 $ \overline{X} $ 使用一个更短的滑动窗口（长度为 $ l' $ ，步长为 $ s' $ ）将其分割为一系列<strong>子片段</strong> $ \tilde{X} \in \mathbb{R}^{c \times l'} $</li><li>这使得模型能够在一个长片段的标签下，观察到内部更精细的信号动态变化，为后续的 Transformer 模块捕捉时间上下文提供了基础</li></ul></li><li><p><strong>节点特征提取</strong>：对于每一个子片段 $ \tilde{X} $，需要为其对应的图节点（即 EEG 通道）提取有意义的特征，论文选择了<strong>相对功率谱密度 (Relative PSD, rPSD)</strong> 作为节点属性</p><ul><li>具体地，使用 <strong>welch&rsquo;s method</strong> 计算每个 EEG 通道在七个经典频带上的 rPSD</li><li>这样，每个子片段 $ \tilde{X} $ 都对应一个特征矩阵 $ F \in \mathbb{R}^{c \times f} $，其中 $ f=7 $</li></ul></li></ul><p>最终，一个长片段 $ \overline{X} $ 被转换成一个按时间顺序排列的图序列 $ G_{T} = \{\mathcal{G}^{i}\} \in \mathbb{R}^{seq \times c \times f} $，其中 $ seq $ 是子片段的数量。这个时间图序列就是 RMPG 模块的输入</p><h3 id=residual-multiview-pyramid-gcn-rmpg>Residual Multiview Pyramid GCN (RMPG)<a hidden class=anchor aria-hidden=true href=#residual-multiview-pyramid-gcn-rmpg>#</a></h3><p><strong>核心</strong>：解决传统 GNN“单一视角”问题，为时间图序列 $ G_{T} $​ 中的每一个图 $ \mathcal{G}^i $ 学习一个丰富的、多层次的空间表征，并将其压缩成一个单一的 token ，以供后续的 TCT 模块处理</p><p>RMPG 模块由一个基础的图编码器 $ \Phi_{g}(\cdot) $ 构成，文中采用了 ChebyNet 作为基础图编码器，对于给定特征输入 $ F^{m-1} $ 和邻接矩阵 $ A $ （通过拉普拉斯算子 $ \hat{L} $ 转换）
$$
\Phi_{g}(F^{m}, A) = \sigma\left( \sum_{k=0}^{K-1}\theta_{k}^{m}T_{k}(\hat{L})F^{m-1} - b^{m}\right)
$$
其中 $ m $ 为 GCN 层的索引，$ \sigma $ 为 ReLU 激活函数，$ \theta $ 为参数，$ T_{k} $ 为 $ k $ 阶 Chebyshev 多项式</p><ul><li><p><strong>多视图学习 (Multiview Learning)</strong> ：为了模拟情绪背后多种认知子过程驱动的不同大脑连接模式，RMPG 并非使用单一的图卷积网络，而是并行地使用了多个 GCN 分支，$ \{ \Phi_{g}^{0}(\cdot), \Phi_{g}^{1}(\cdot), \dots, \Phi_{g}^{i}(\cdot) \} $</p><ul><li>每个分支都拥有一个<strong>独立的可学习邻接矩阵</strong> $ A^{i} \in \mathbb{R}^{c \times c} $ ，能在模型训练过程中通过梯度反向传播进行端到端的优化</li><li>这意味着每个 GCN 分支都能从数据中学习到一种独特的大脑功能连接“视图”</li></ul></li><li><p><strong>金字塔学习 (Pyramid Learning)</strong> ：为了捕捉不同尺度的空间信息，并行的 GCN 分支被设计成具有不同的深度（即 GCN 层数）</p><ul><li><strong>较浅的 GCN</strong> 能够有效地聚合<strong>全局的、跨脑区</strong>的功能连接信息，聚合远距离节点的信息而不过度平滑</li><li><strong>较深的 GCN</strong> 能够更好地聚合<strong>局部邻域内</strong>的信息，在脑区内部形成一致的表征</li><li>GCN 的深度越深，其输出所代表的特征金字塔层级越高。</li></ul></li><li><p><strong>残差连接 (Residual)</strong> ：除了并行的 GCN 分支外，RMPG 还包含一个并行的线性残差分支。该分支直接对原始的时序图 $ G $ （或者对应的特征矩阵 $ F $ ）进行线性投影，不经过任何图卷积，从而保留最原始的节点信息，作为特征金字塔的“基座”</p><ul><li>线性投影层 $ LP(\cdot) $ 将扁平化的图表示投影到隐藏嵌入 $ H_{g}^{i} \in \mathbb{R}^{d_{g}} $</li><li>堆叠来自不同层的 GCN 的并行输出，得到多金字塔视图嵌入 $$
\{ H_{g}^{i} \} = \{ LP^{i}(\Gamma(\Phi_{g}^{i}(F, A^{i}))) \}
$$ 其中 $ \Gamma(\cdot) $ 是扁平操作，$ \{ \cdot \} $ 是堆叠操作</li></ul></li></ul><p><strong>特征融合与 Token 生成</strong>：最终，对于图序列中的每一个图 $ G_{i} $，其所有 GCN 视图的输出 $ \{ H_{i}^{g} \} $ 和残差基座 $ H_{g-\text{base}} $​ 通过一个 <strong>mean fusion</strong> 操作合并成当前时间步 $ i $ 的最终 token $ s_{i} $
$$
s_{i} = \text{mean}(\{ H_{g-\text{base}}, H_{g}^{0}, H_{g}^{1}, \dots, H_{g}^{i} \})
$$</p><p>通过 RMPG 模块，输入的图序列 $ G_{T} $ 被高效地转化为一个 token 序列 $ S_{T}=\{ s^{i} \} \in \mathbb{R}^{seq \times d_{g}} $ ，这个序列既蕴含了每个时刻丰富的多视图、多层次空间信息，又具备了适合 Transformer 处理的格式</p><h3 id=temporal-contextual-transformer-tct>Temporal Contextual Transformer (TCT)<a hidden class=anchor aria-hidden=true href=#temporal-contextual-transformer-tct>#</a></h3><p><strong>核心</strong>：接受由 RMPG 生成的 token 序列 $ S_{T} $ ，并利用 Transformer 的结构来高效捕捉这些 token 之间的时间依赖关系；与标准的 Transformer 不同，TCT 引入了两种为 EEG 情绪识别任务定制的 <strong>Token Mixer</strong>，分别用于分类和回归任务</p><p>TCT 模块由多个堆叠的 Transformer Block 组成，对于输入 token 序列 $ Z^{m} $ （ $ Z^{0} = S_{T} $ ），每个 Block 的计算过程为
$$
Z^{m'} = \text{TokenMixer}(\text{Norm}(Z^{m})) + Z^{m}
$$
$$
Z^{m+1} = \text{MLP}(\text{Norm}(Z^{m'})) + Z^{m'}
$$
其中 $ m $ 是层的索引，MLP 是带 ReLU 激活函数的两层感知机</p><h4 id=hahahugoshortcode15s52hbhb-for-classification-tasks>$ \text{TokenMixer}_{\text{clas}} $ for Classification Tasks<a hidden class=anchor aria-hidden=true href=#hahahugoshortcode15s52hbhb-for-classification-tasks>#</a></h4><p>旨在捕捉随时间变化的<strong>长短时序上下文信息</strong></p><ul><li><p><strong>多头自注意力 (Multi-head Self-Attention, MSA)</strong> ：用于<strong>全局地关注</strong>序列中与整体情绪状态高度相关的部分 $$
\text{Attn}(Q,K,V) = \text{softmax}\left( \frac{QK^{T}}{\sqrt{ d }} \right)V
$$</p><ul><li>并行应用多个注意力头（每个头有独立的 $ LP_h(\cdot) $ 来生成 $ Q, K, V $），然后将所有头的输出拼接：$$
\text{MSA}(S_{T}) = \text{Concat}(\text{Attn}(LP_0(S_{T})), ..., \text{Attn}(LP_{n_{\text{head}}-1}(S_{T})))
$$ 其中 $ S_{T} $ 是 token 序列，$ n_{\text{head}} $ 是注意力头的数量</li></ul></li><li><p><strong>短期聚合层 (Short-Time Aggregation, STA)</strong> ：</p><ul><li>基于“情绪短期连续而长期变化”的先验知识，STA 在 MSA 之后应用来学习短期的上下文信息<ul><li>首先对 MSA 的输出 $ H_{\text{attn}} \in \mathbb{R}^{n_{\text{head}} \times \text{seq} \times d_{\text{head}}} $ 应用一个带有比例因子 $ \alpha $ 的 Dropout 层 $ \text{dp}(\alpha) $</li><li>接着，通过 Conv2D 聚合 $ n_{\text{anchor}} $ 个时序近邻</li><li><code>Conv2D</code> 的卷积核 $ K_{\text{cnn}} $ 的尺寸为 $ (n_{\text{anchor}}, 1) $，步长为 $ (1,1) $</li><li>最后，卷积的输出会被 Reshape 并进行线性投影（$ W_{\text{sta}} $）</li><li>可以描述为 $$
\text{STA}(H_{\text{attn}}) = \text{Reshape}(\text{Conv2D}(\text{dp}(H_{\text{attn}}), K_{\text{cnn}})) W_{\text{sta}}
$$ $ \text{Reshape}(\cdot) $ 将维度从 $ (n_{\text{head}},\text{seq},d_{\text{head}}) $ 转换为 $ (\text{seq}, n_{\text{head}} \cdot d_{\text{head}}) $ ， $ W_{\text{sta}} \in \mathbb{R}^{n_{\text{head}} \cdot d_{\text{head}} \times d_g} $ 是投影权重矩阵</li></ul></li></ul></li><li><p>$ \text{TokenMixer}_{\text{Clas}} $ 最终由上述两个模块串联构成 $$
\text{TokenMixer}_{\text{clas}}(S_{T}) = \text{STA}(\text{MSA}(S_{T}))
$$</p></li></ul><h4 id=hahahugoshortcode15s74hbhb-for-regression-tasks>$ \text{TokenMixer}_{\text{regr}} $ for Regression Tasks<a hidden class=anchor aria-hidden=true href=#hahahugoshortcode15s74hbhb-for-regression-tasks>#</a></h4><p>旨在预测序列中情绪状态的<strong>连续变化</strong></p><ul><li><p><strong>不同于分类任务</strong>：分类任务的目标通常是从整个序列中提取几个核心特征来判断整体情绪状态，这通过 MSA 聚焦于重要部分是有效的；然而回归任务需要模型对序列中<strong>每个时步的连续情绪变化</strong>进行预测，因此不使用全局的 MSA，而是采用了一种<strong>基于 RNN 的混合器</strong>（ RNN family ）</p></li><li><p><strong>RNN Mixer</strong> ：</p><ul><li>RNN 结构天然适合处理连续序列的演变过程，能够更好地建模情绪值的平滑变化</li><li>经验性选择的<strong>两层双向 GRU (bi-directional GRU)</strong> 作为 $ \text{TokenMixer}_{\text{regr}} $ ，输出长度为 $ 2 \times d_{\text{head}} $</li></ul></li><li><p>计算过程为 $$
\text{TokenMixer}_{\text{regr}}(S_T) = \text{RNNs}(\text{LP}(S_T))
$$ 其中 $ LP(S_{T}) = S_{T}W_{v} $ ，把 token 序列投影为 $ V $ 值</p></li></ul><h3 id=tso-module>TSO Module<a hidden class=anchor aria-hidden=true href=#tso-module>#</a></h3><p>头部接收来自不同 Token Mixer 的输出：用于分类的 $ S_{\text{clas}} $ 和用于回归的 $ S_{\text{regr}} $</p><ul><li><p><strong>分类任务</strong>：对所有 token 进行 <strong>mean fusion</strong> ，再通过线性层得到最终 logits $$
\hat{Y}_{\text{clas}} = \text{mean}(S_{\text{clas}})W_{\text{clas}} + b_{\text{clas}}
$$ 其中 $ S_{\text{clas}} \in \mathbb{R}^{\text{seq} \times d_{\text{head}}} $ ，$ W_{\text{clas}} \in \mathbb{R}^{d_{\text{head}} \times d_{\text{class}}} $ ，$ b_{\text{clas}} \in \mathbb{R}^{n_{\text{class}}} $</p></li><li><p><strong>回归任务</strong>：直接将每个时间步的 token 特征通过线性层，得到对应每个时刻的回归值 $$
\hat{Y}_{\text{regr}} = S_{\text{regr}}W_{\text{regr}} + b_{\text{regr}}
$$ 其中 $ S_{\text{regr}} \in \mathbb{R}^{\text{seq} \times 2\cdot d_{\text{head}}} $ （双向），$ W_{\text{regr}} \in \mathbb{R}^{2\cdot d_{\text{head}} \times 1} $ ，$ b_{\text{regr}} \in \mathbb{R} $</p></li></ul><h2 id=experiment>Experiment<a hidden class=anchor aria-hidden=true href=#experiment>#</a></h2><h3 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h3><p>使用 <strong>SEED</strong>, <strong>THU-EP</strong>, <strong>FACED</strong>, <strong>MAHNOB-HCI</strong> 四个公开数据集</p><p>其中 SEED 数据集使用了 0.3-50 Hz 的带通滤波</p><h4 id=eeg-temporal-graph>EEG-Temporal-Graph<a hidden class=anchor aria-hidden=true href=#eeg-temporal-graph>#</a></h4><ul><li><strong>长片段</strong>：窗口长度 $ l=20\,\mathrm{s} $ （即 $ 20 \times f_{s} $ ），步长 $ s=4\,\mathrm{s} $</li><li><strong>子片段</strong>：<ul><li>对于 SEED 和 THU-EP： $ l'=2\,\mathrm{s},s'=0.5\,\mathrm{s} $</li><li>对于 FACED： $ l'=4\,\mathrm{s},s'=1\,\mathrm{s} $</li></ul></li></ul><h3 id=settings>Settings<a hidden class=anchor aria-hidden=true href=#settings>#</a></h3><ul><li><strong>数据划分</strong>：<ul><li><strong>SEED</strong>：采用留一被试交叉验证，每次迭代一个被试的数据作为测试集，剩余数据中 80% 作为训练集，20% 作为验证集</li><li><strong>THU-EP</strong> 和 <strong>FACED</strong>：采用留 $ n $ 被试交叉验证，其中 $ n_{\text{THU-EP}}=8,n_{\text{FACED}}=12 $ ，训练数据中 10% 作为验证集</li></ul></li><li><strong>分类任务</strong>：对 SEED、THU-EP 和 FACED 进行积极/消极情绪的二分类，THU-EP 和 FACED 的效价分数通过阈值 3.0 划分为高/低效价</li><li><strong>回归任务</strong>：MAHNOB-HCI 并进行 LOSO 验证</li></ul><h3 id=evaluation-metrics>Evaluation Metrics<a hidden class=anchor aria-hidden=true href=#evaluation-metrics>#</a></h3><h4 id=classfication>Classfication<a hidden class=anchor aria-hidden=true href=#classfication>#</a></h4><ul><li><strong>Accuracy</strong>：$$
\text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + FP + TN + FN}}
$$</li><li><strong>F1 Score</strong>：$$
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision + Recall}} = \frac{\text{TP}}{\text{TP} + \frac{1}{2}(\text{FP} + \text{FN})}
$$</li></ul><p>其中 $ \text{TP} $ 表示真阳性，$ \text{TN} $ 表示真阴性，$ \text{FP} $ 表示假阳性，$ \text{FN} $ 表示假阴性</p><h4 id=regression>Regression<a hidden class=anchor aria-hidden=true href=#regression>#</a></h4><p>给定预测值 $ \hat{y} $ 和连续标签 $ y $ ：</p><ul><li><strong>均方根误差 (RMSE)</strong> ：$$
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=0}^{N-1} (\hat{y}_i - y_i)^2}
$$</li><li><strong>皮尔逊相关系数 (PCC)</strong> ：$$
\text{PCC} = \frac{\sigma_{\hat{y}y}}{\sigma_{\hat{y}} \sigma_y} = \frac{\sum_{i=0}^{N-1} (\hat{y}_i - \mu_{\hat{y}})(\hat{y}_i - \mu_y)}{\sqrt{\sum_{i=0}^{N-1} (\hat{y}_i - \mu_{\hat{y}})^2} \sqrt{\sum_{i=0}^{N-1} (y_i - \mu_y)^2}}
$$</li><li><strong>一致性相关系数 (CCC)</strong> ：$$
\text{CCC} = \frac{2\sigma_{\hat{y}y}}{\sigma^2_{\hat{y}} + \sigma^2_y + (\mu_{\hat{y}} - \mu_y)^2}
$$</li></ul><p>其中 $ N $ 是向量中的元素数量，$ \sigma_{\hat{y}y} $ 是协方差，$ \sigma_{\hat{y}} $ 和 $ \sigma_y $ 是标准差，$ \mu_{\hat{y}} $ 和 $ \mu_y $ 是均值</p><h3 id=implementation-details>Implementation Details<a hidden class=anchor aria-hidden=true href=#implementation-details>#</a></h3><p>模型的三种变体：
<img loading=lazy src=/images/emt/pasted-image-20250802162952-png></p><h2 id=analyses>Analyses<a hidden class=anchor aria-hidden=true href=#analyses>#</a></h2><h3 id=classification>Classification<a hidden class=anchor aria-hidden=true href=#classification>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802163455-png></p><ul><li><strong>SEED</strong>：<ul><li>EmT-D 表现最佳，EmT-B 和 EmT-S 表现也良好，RGNN 表现第二佳</li><li>使用特征作为输入的模型通常优于直接使用 EEG 信号作为输入的模型，从时序特征而非直接特征学习通常能取得更好的性能（RGNN 除外），这表明了学习时序上下文信息的有效性，EmT 借助基于 GCN 的模块，能更好地学习空间信息</li></ul></li><li><strong>THU-EP / FACED</strong>：<ul><li><strong>THU-EP</strong>：EmT-B 取得了最佳 F1 分数，Conformer 取得了最佳 ACC</li><li><strong>FACED</strong>：EmT-B 取得了第二佳 ACC 和最佳 F1 分数</li><li>由于类不平衡，F1 分数比 ACC 更重要，EmT-B 在这两个数据集上均取得了最高 F1 分数</li><li>与 SEED 不同，直接使用 EEG 作为输入的 baseline 模型表现更好，这可能因为这两个数据集的被试人数更多</li></ul></li><li><strong>Features</strong>：<ul><li><strong>SEED (62 channels，15 subjects)</strong> ：EmT-D (8 层) 表现最佳，说明 Transformer 层数越多，学到的空间信息越丰富</li><li><strong>THU-EP 和 FACED (32 channels，more subjects)</strong> ：EmT-B (4 层) 表现更好，通道数少且被试间变异性大时，更深的模型（EmT-D）容易过拟合</li></ul></li></ul><h3 id=regression-1>Regression<a hidden class=anchor aria-hidden=true href=#regression-1>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802164137-png></p><ul><li>在 MAHNOB-HCI 数据集上，EmT-Regr (LP+LSTM) 取得了最低的 RMSE，而 EmT-Regr (LP+GRU) 取得了最佳的 PCC 和 CCC</li><li>使用 MSA 作为 Token Mixer 时，模型性能急剧下降，甚至低于所有基线模型</li><li>这表明对于回归任务，融合所有片段的信息至关重要，而 RNN 的顺序信息融合能力比 MSA 更适合建模连续的情绪变化</li></ul><h3 id=ablation-study>Ablation Study<a hidden class=anchor aria-hidden=true href=#ablation-study>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802164718-png></p><h3 id=effect-of-eeg-features>Effect of EEG Features<a hidden class=anchor aria-hidden=true href=#effect-of-eeg-features>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802165136-png></p><h3 id=effect-of-the-depth-and-width-of-gcns-in-rmpg>Effect of The Depth and Width of GCNs in RMPG<a hidden class=anchor aria-hidden=true href=#effect-of-the-depth-and-width-of-gcns-in-rmpg>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802165509-png></p><ul><li><strong>Depth</strong>：增加 GCN 层的数量会导致性能显著下降，这与更深 GCN 中存在的<strong>过平滑</strong>问题一致</li><li><strong>Width</strong>：宽度从 8 增加到 32 时，性能呈正相关；当宽度进一步增加时，性能下降，这可能是由更大的模型尺寸导致的过拟合</li></ul><h3 id=effect-of-the-number-of-tct-blocks>Effect of The Number of TCT Blocks<a hidden class=anchor aria-hidden=true href=#effect-of-the-number-of-tct-blocks>#</a></h3><p><img loading=lazy src=/images/emt/pasted-image-20250802170025-png></p><ul><li><strong>Classfication (SEED)</strong> ：TCT 块的数量从 2 增加到 8 时，ACC 和 F1 分数均显著提高，这表明增加 TCT 块数能增强模型捕捉时序上下文信息的能力，从而提升分类性能</li><li><strong>Regression (MAHNOB-HCI)</strong> ：TCT 块的数量对性能指标几乎没有影响</li></ul><h3 id=visualization>Visualization<a hidden class=anchor aria-hidden=true href=#visualization>#</a></h3><h4 id=learned-connections>Learned Connections<a hidden class=anchor aria-hidden=true href=#learned-connections>#</a></h4><p>在 SEED 数据集上学习到的两种不同连接模式的可视化证据
<img loading=lazy src=/images/emt/pasted-image-20250801002808-png></p><p>在 SEED 数据集上，两个可学习的邻接矩阵揭示了情绪认知过程中不同的连接模式：</p><ul><li>(a) 中主要关注额叶、顶叶和颞叶区域之间的连接，这些区域与心理注意力密切相关</li><li>(b) 中则包括额叶、颞叶和顶叶区域之间的互动（与情绪相关），以及枕叶和顶叶区域的互动（与视觉过程相关，因为刺激为视频）</li></ul><h4 id=learned-temporal-contextual-information>Learned Temporal Contextual Information<a hidden class=anchor aria-hidden=true href=#learned-temporal-contextual-information>#</a></h4><p><strong>Classfication (FACED)</strong>
<img loading=lazy src=/images/emt/pasted-image-20250802171153-png></p><p><strong>Regression</strong>
<img loading=lazy src=/images/emt/pasted-image-20250802171249-png></p><ul><li>分类任务在 TCT 块之前，特征随时间变化，TCT 块之后，激活变得更加一致；这可能是因为自注意力机制关注与整体情绪状态高度相关的部分，而 STA 层通过聚合邻近的时序信息来平滑波动</li><li>回归任务：特征空间也显示出时序变化；与分类不同，回归特征并非简单平滑，形成了更复杂的表示</li><li><strong>总结</strong>：TCT 块处理分类和回归任务的方式不同：分类中，特征被平滑以增强可分离性；回归中，RNN Token Mixer 保留了时序变化，从而实现连续情绪预测</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://diefish1024.github.io/tags/literature-note/>Literature-Note</a></li><li><a href=https://diefish1024.github.io/tags/eeg/>EEG</a></li><li><a href=https://diefish1024.github.io/tags/emotion-recognition/>Emotion-Recognition</a></li><li><a href=https://diefish1024.github.io/tags/transformer/>Transformer</a></li><li><a href=https://diefish1024.github.io/tags/gnn/>GNN</a></li></ul><nav class=paginav><a class=prev href=https://diefish1024.github.io/posts/ai-infra/kv-cache-%E5%85%A5%E9%97%A8/><span class=title>« Prev</span><br><span>KV Cache 入门</span>
</a><a class=next href=https://diefish1024.github.io/posts/literature-notes/ssa/><span class=title>Next »</span><br><span>SSA</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on x" href="https://x.com/intent/tweet/?text=EmT&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f&amp;hashtags=literature-note%2cEEG%2cemotion-recognition%2cTransformer%2cGNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f&amp;title=EmT&amp;summary=EmT&amp;source=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f&title=EmT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on whatsapp" href="https://api.whatsapp.com/send?text=EmT%20-%20https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on telegram" href="https://telegram.me/share/url?text=EmT&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share EmT on ycombinator" href="https://news.ycombinator.com/submitlink?t=EmT&u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fliterature-notes%2femt%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://diefish1024.github.io/>diefish's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>