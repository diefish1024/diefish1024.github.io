<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MATH2701 HW5 | diefish's blog</title><meta name=keywords content="learning,math,homework,probability-theory"><meta name=description content="Problem 1
(1)
根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为
$$ 

\mathbb{P}(X=x) = \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } },\quad x = 0,1,\dots,m

 $$
(2)
矩生成函数定义为 $ M_{X}(\theta) = \mathbb{E}[e^{ \theta X }] $。我们带入得到
$$ 

\begin{align*}
M_{X}(\theta) & = \sum_{x}e^{ \theta x }\mathbb{P}(X=x) \\
 & = \sum_{x=0}^{m} e^{ \theta x } \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } } \\
 & = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} e^{ \theta x }\binom{ m }{ x } \binom{ n-m }{ k-x } 
\end{align*}

 $$
(3)"><meta name=author content="diefish"><link rel=canonical href=https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/math2701-hw5/><link crossorigin=anonymous href=/assets/css/stylesheet.7e33168b13c822c8560dd6cce14b81ffdf7b6c118596a21d43f319d693f61534.css integrity="sha256-fjMWixPIIshWDdbM4UuB/997bBGFlqIdQ/MZ1pP2FTQ=" rel="preload stylesheet" as=style><link rel=icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://diefish1024.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=mask-icon href=https://diefish1024.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/math2701-hw5/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/math2701-hw5/"><meta property="og:site_name" content="diefish's blog"><meta property="og:title" content="MATH2701 HW5"><meta property="og:description" content="Problem 1 (1)
根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为 $$ \mathbb{P}(X=x) = \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } },\quad x = 0,1,\dots,m $$ (2)
矩生成函数定义为 $ M_{X}(\theta) = \mathbb{E}[e^{ \theta X }] $。我们带入得到 $$ \begin{align*} M_{X}(\theta) & = \sum_{x}e^{ \theta x }\mathbb{P}(X=x) \\ & = \sum_{x=0}^{m} e^{ \theta x } \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } } \\ & = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} e^{ \theta x }\binom{ m }{ x } \binom{ n-m }{ k-x } \end{align*} $$ (3)"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-19T09:20:00+08:00"><meta property="article:modified_time" content="2025-11-19T09:20:00+08:00"><meta property="article:tag" content="Learning"><meta property="article:tag" content="Math"><meta property="article:tag" content="Homework"><meta property="article:tag" content="Probability-Theory"><meta property="og:image" content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:title content="MATH2701 HW5"><meta name=twitter:description content="Problem 1
(1)
根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为
$$ 

\mathbb{P}(X=x) = \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } },\quad x = 0,1,\dots,m

 $$
(2)
矩生成函数定义为 $ M_{X}(\theta) = \mathbb{E}[e^{ \theta X }] $。我们带入得到
$$ 

\begin{align*}
M_{X}(\theta) & = \sum_{x}e^{ \theta x }\mathbb{P}(X=x) \\
 & = \sum_{x=0}^{m} e^{ \theta x } \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } } \\
 & = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} e^{ \theta x }\binom{ m }{ x } \binom{ n-m }{ k-x } 
\end{align*}

 $$
(3)"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://diefish1024.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Class Notes","item":"https://diefish1024.github.io/posts/class-notes/"},{"@type":"ListItem","position":3,"name":"MATH2701 Probability Theory","item":"https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/"},{"@type":"ListItem","position":4,"name":"MATH2701 HW5","item":"https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/math2701-hw5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MATH2701 HW5","name":"MATH2701 HW5","description":"Problem 1 (1)\n根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为 $$ \\mathbb{P}(X=x) = \\dfrac{\\binom{ m }{ x } \\binom{ n-m }{ k-x } }{\\binom{ n }{ k } },\\quad x = 0,1,\\dots,m $$ (2)\n矩生成函数定义为 $ M_{X}(\\theta) = \\mathbb{E}[e^{ \\theta X }] $。我们带入得到 $$ \\begin{align*} M_{X}(\\theta) \u0026 = \\sum_{x}e^{ \\theta x }\\mathbb{P}(X=x) \\\\ \u0026 = \\sum_{x=0}^{m} e^{ \\theta x } \\dfrac{\\binom{ m }{ x } \\binom{ n-m }{ k-x } }{\\binom{ n }{ k } } \\\\ \u0026 = \\dfrac{1}{\\binom{ n }{ k } }\\sum_{x=0}^{m} e^{ \\theta x }\\binom{ m }{ x } \\binom{ n-m }{ k-x } \\end{align*} $$ (3)\n","keywords":["learning","math","homework","probability-theory"],"articleBody":"Problem 1 (1)\n根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为 $$ \\mathbb{P}(X=x) = \\dfrac{\\binom{ m }{ x } \\binom{ n-m }{ k-x } }{\\binom{ n }{ k } },\\quad x = 0,1,\\dots,m $$ (2)\n矩生成函数定义为 $ M_{X}(\\theta) = \\mathbb{E}[e^{ \\theta X }] $。我们带入得到 $$ \\begin{align*} M_{X}(\\theta) \u0026 = \\sum_{x}e^{ \\theta x }\\mathbb{P}(X=x) \\\\ \u0026 = \\sum_{x=0}^{m} e^{ \\theta x } \\dfrac{\\binom{ m }{ x } \\binom{ n-m }{ k-x } }{\\binom{ n }{ k } } \\\\ \u0026 = \\dfrac{1}{\\binom{ n }{ k } }\\sum_{x=0}^{m} e^{ \\theta x }\\binom{ m }{ x } \\binom{ n-m }{ k-x } \\end{align*} $$ (3)\n首先计算 $ \\mathbb{E}[X] $。 $$ \\begin{align*} \\mathbb{E}[X] \u0026 = M'_{X}(0) = \\dfrac{\\mathrm{d} }{\\mathrm{d}\\theta} \\left( \\sum_{x}e^{ \\theta x }\\mathbb{P}(X=x) \\right)\\bigg|_{\\theta=0} \\\\ \u0026 = \\sum_{x=0}^{m} x\\mathbb{P}(X=x) \\\\ \u0026 = \\dfrac{1}{\\binom{ n }{ k }} \\sum_{x=0}^{m} x\\binom{ m }{ x } \\binom{ n-m }{ k-x } \\\\ \u0026 = \\dfrac{1}{\\binom{ n }{ k } }\\sum_{x=0}^{m} m\\binom{ m-1 }{ x-1 } \\binom{ n-m }{ k-x } \\\\ \u0026 = \\dfrac{m}{\\binom{ n }{ k } } \\sum_{y=0}^{m-1} \\binom{ m-1 }{ y } \\binom{ n-m }{ k-y-1 } \\\\ \u0026 = \\dfrac{m}{\\binom{ n }{ k } }\\binom{ n-m+m-1 }{ k-1 } = \\dfrac{m}{\\binom{ n }{ k } }\\binom{ n-1 }{ k-1 } \\\\ \u0026 = \\dfrac{mk}{n} \\end{align*} $$ 接着计算 $ \\mathbb{E}[X^{2}] $。由于 $ \\mathbb{E}[X^{2}] $ 的表达式化简较为困难，我们考虑求解 $ \\mathbb{E}[X(X-1)] $。根据 LOTUS，我们有 $$ \\begin{align*} \\mathbb{E}[X(X-1)] \u0026 = \\sum_{x=0}^{m} x(x-1)\\mathbb{P}(X=x) \\\\ \u0026 = \\dfrac{1}{\\binom{ n }{ k } }\\sum_{x=0}^{m} x(x-1)\\binom{ m }{ x } \\binom{ n-m }{ k-x } \\\\ \u0026 = \\dfrac{m}{\\binom{ n }{ k } }\\sum_{x=0}^{m} (x-1)\\binom{ m-1 }{ x-1 } \\binom{ n-m }{ k-x } \\\\ \u0026 = \\dfrac{m(m-1)}{\\binom{ m }{ k } }\\sum_{x=0}^{m} \\binom{ m-2 }{ x-2 } \\binom{ n-m }{ k-x } \\\\ \u0026 = \\dfrac{m(m-1)}{\\binom{ m }{ k } }\\binom{ n-2 }{ k-2 } \\\\ \u0026 = \\dfrac{m(m-1)k(k-1)}{n(n-1)} \\end{align*} $$ 从而带入方差公式得到 $$ \\begin{align*} \\text{Var}[X] \u0026 = \\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2} \\\\ \u0026 = \\mathbb{E}[X(X-1)] + \\mathbb{E}[X] - (\\mathbb{E}[X])^{2} \\\\ \u0026 = \\dfrac{m(m-1)k(k-1)}{n(n-1)} + \\dfrac{mk}{n} - \\left( \\dfrac{mk}{n} \\right)^{2} \\\\ \u0026 = \\dfrac{mk}{n}\\left[ \\dfrac{(m-1)(k-1)}{n-1}+1 - \\dfrac{mk}{n}\\right] \\\\ \u0026 = \\dfrac{mk}{n}\\cdot \\dfrac{n-m}{n}\\cdot \\dfrac{n-k}{n-1} \\end{align*} $$ 综上得到 $$ \\mathbb{E}[X] = \\dfrac{mk}{n},\\quad \\text{Var}[X] = \\dfrac{mk}{n}\\cdot \\dfrac{n-m}{n}\\cdot \\dfrac{n-k}{n-1} $$\nProblem 2 (1)\n首先我们计算 $ X $ 的 pdf，记为 $ f_{X}(x) $，那么 $$ f_{X}(x) = \\prod_{i=1}^{n} f_{X_{i}}(x_{i}) = \\prod_{i=1}^{n} \\dfrac{1}{\\sqrt{ 2\\pi }}e^{ -x_{i}^{2} / 2 } = \\dfrac{1}{(2\\pi)^{n / 2}}e^{ - \\frac{1}{2}\\sum_{i=1}^{n} x_{i}^{2} } $$ 由于 $ \\| x \\|^{2}=x^{T}x = \\sum_{i=1}^{n}x_{i}^{2} $，我们可以化简为 $$ f_{X}(x) = \\dfrac{1}{(2\\pi)^{n / 2}}e^{ - \\frac{1}{2} x^{T}x } $$\n根据题设，我们知道 $$ y = Qx \\implies x = Q^{T}y $$ 从而计算该变换雅可比行列式的绝对值，有 $$ J = \\left| \\det\\left( \\dfrac{ \\partial x }{ \\partial y } \\right) \\right| = \\left| \\det(Q^{T}) \\right| $$ 根据正定矩阵的性质，我们知道 $ \\det Q^{T}=\\det Q=\\pm 1 $，因此 $ J=1 $。\n于是根据变量代换公式，我们得到 $$ f_{Y}(y) = f_{X}(x(y))\\cdot J = f_{X}(Q^{T}y)\\cdot 1 $$ 带入 $ Q^{T}y $，其中 $ x^{T}x $ 变为 $ y^{T}Q Q^{T}y=y^{T}y $ 即可得到 $$ f_{Y}(y) = \\dfrac{1}{(2\\pi)^{n/2}}\\exp\\left( -\\dfrac{1}{2}y^{T} y\\right) $$ 从而 $ x $ 与 $ y $ 的分布一致。\n(2)\n对于单位球面上的两个向量 $ z_{1} $ 和 $ z_{2} $，利用线性代数可以证明存在正交矩阵 $ Q $ 使得 $$ z_{2}=Qz_{1} $$ 我们考虑变换后的随机变量 $ QZ $，得到 $$ QZ = Q\\left( \\dfrac{X}{\\| X \\| } \\right)= \\dfrac{QX}{\\| X \\| } \\xlongequal{\\| QX \\| = \\| X \\| } \\dfrac{QX}{\\| QX \\| } $$ 我们令 $ Y=QX $，就有 $ QZ=\\dfrac{Y}{\\| Y \\|} $。利用 $ (1) $ 的结论，我们知道 $ X $ 和 $ Y $ 是同分布的，从而 $ Z $ 和 $ QZ $ 是同分布的。这就说明了 $ Z $ 具有旋转不变性，也就有 $ Z $ 在球面上取 $ z_{1} $ 附近的概率密度，必须等于取 $ z_{2}=Qz_{1} $ 附近的概率密度，也就是 $$ f_{Z}(z) = f_{Z}(Qz) $$ 我们可以找到 $ Q $ 将球面上的任意点 $ z_{1} $ 映射到球面上的任意点 $ z_{2} $，从而说明了 $$ \\forall z_{1},z_{2}\\in S^{n-1},\\quad f_{Z}(z_{1})=f_{Z}(z_{2}) $$\nProblem 3 (1)\n根据题设我们有 $$ \\begin{cases} u = xy \\\\ v = \\dfrac{x}{y} \\end{cases} $$ 从而得到 $ x=\\sqrt{ uv },y=\\sqrt{ u / v } $。我们计算变换的 Jacobi 行列式 $$ J = \\det\\left( \\dfrac{ \\partial (x,y) }{ \\partial (u,v) } \\right) = \\det \\begin{pmatrix} \\dfrac{1}{2}\\sqrt{ \\dfrac{v}{u} } \u0026 \\dfrac{1}{2}\\sqrt{ \\dfrac{u}{v} } \\\\ \\dfrac{1}{2\\sqrt{ uv }} \u0026 -\\dfrac{1}{2v}\\sqrt{ \\dfrac{u}{v} } \\end{pmatrix} = - \\dfrac{1}{2v} $$ 从而 $ \\left| J \\right|= \\dfrac{1}{2v} $。\n于是我们就得到了联合密度函数为 $$ f_{UV}(u,v) = f_{XY}(\\sqrt{ uv },\\sqrt{ u / v }) \\cdot \\left| J \\right| = 1 \\cdot\\dfrac{1}{2v} = \\dfrac{1}{2v} $$ (2)\n我们需要分区域讨论。将区域 $ [0,1]^{2} $ 沿对角线分成两个区域 $$ D_{1} = \\{ (x,y)\\mid 0\\leq x \u003c y\\leq 1 \\},\\quad D_{2} = [0,1]^{2}\\setminus D_{1} $$ 我们知道在 $ D_{1} $ 上有 $ U=X,V=Y $，在 $ D_{2} $ 上有 $ U=Y,V=X $。\n对于目标区域 $ 0\\leq u\\leq v\\leq 1 $，我们分别考虑来着 $ D_{1} $ 和 $ D_{2} $ 的贡献。来自 $ D_{1} $ 的映射满足 $ (x,y)=(u,v) $，从而 Jacobi 行列式为 $ \\left| J_{1} \\right|=1 $，产生的贡献为 $ f_{XY}(u,v)\\cdot 1=1 $。来自 $ D_{2} $ 的映射满足 $ (x,y)=(v,u) $，对应的 Jacobi 行列式为 $ \\left| J_{2} \\right|=1 $，产生的贡献为 $ f_{XY}(u,v)\\cdot 1=1 $。\n我们将两部分贡献相加，就得到了 $$ f_{UV}(u,v)=1+1=2 $$ (3)\n我们需要证明 $ f_{UV}(u,v) $ 是各分支逆映射密度函数乘以 Jacobi 行列式后的叠加。\n设 $ A $ 是 $ UV $ 平面上任意的 Borel 可测集。我们需要计算 $ \\mathbb{P}((U,V)\\in A) $。根据全概率公式，将事件分解到两个不相交的区域 $ D_{g} $ 和 $ D_{h} $ 上，有 $$ \\begin{align*} \\mathbb{P}((U,V)\\in A) \u0026 = \\mathbb{P}((X,Y)\\in D_{g},g(X,Y)\\in A) +\\mathbb{P}((X,Y)\\in D_{h},h(X,Y)\\in A) \\\\ \u0026 = \\underset{ \\{ (x,y)\\in D_{g}\\mid g(x,y)\\in A \\} }{ \\iint } f_{XY}(x,y)\\mathrm{d}x\\mathrm{d}y + \\underset{ \\{ (x,y)\\in D_{h}\\mid h(x,y)\\in A \\} }{ \\iint } f_{XY}(x,y)\\mathrm{d}x\\mathrm{d}y \\end{align*} $$ 对于第一个积分进行变量代换，令 $ (u,v)=g(x,y) $，则 $ (x,y)=g^{-1}(u,v) $。此时积分区域变为 $ A\\cap I_{g} $，面积元变为 $ \\left| J_{g^{-1}} \\right|\\mathrm{d}u\\mathrm{d}v $。同理对于第二个积分，令 $ (u,v)=h(x,y) $，那么积分区域变为 $ A\\cap I_{h} $，面积元变为 $ \\left| J_{^{-1}h} \\right|\\mathrm{d}u\\mathrm{d}v $。带入就得到了 $$ \\underset{ A\\cap I_{g} }{ \\iint } f_{XY}(g^{-1}(u,v))\\left| J_{g^{-1}} \\right| \\mathrm{d}u\\mathrm{d}v + \\underset{ A\\cap I_{h} }{ \\iint } f_{XY}(h^{-1}(u,v))\\left| J_{h^{-1}} \\right| \\mathrm{d}u\\mathrm{d}v $$ 利用指示函数将积分区域统一为 $ A $，就有 $$ \\mathbb{P}((U,V)\\in A) = \\underset{ A }{ \\iint } [f_{XY}(g^{-1}(u,v))\\left| J_{g^{-1}} \\right|\\cdot \\mathbb{I}_{(u,v)\\in I_{g}} + f_{XY}(h^{-1}(u,v))\\left| J_{h^{-1}} \\right|\\cdot \\mathbb{I}_{(u,v)\\in I_{h}}] \\mathrm{d}u\\mathrm{d}v $$ 根据 pdf 的定义，被积函数即为 $ f_{UV}(u,v) $，从而得证。\n(4)\n由题设，$ X\\sim U[0,1],Y\\sim U[0,2] $，并且两者独立。我们知道 $ f_{XY}(x,y)=f_{X}\\cdot f_{Y}=\\frac{1}{2} $。\n将矩形区域 $ R=[0,2]\\times[0,1] $ 按照直线 $ y=x $ 切割成两部分，分别为 $ D_{g} $ 满足 $ X\u003c Y $，$ D_{h} $ 满足 $ X\u003eY $。\n其中 $ D_{g} $ 区域 $ \\left| J_{1} \\right|=1 $，满足 $ u=x,v=y $，贡献 $ f_{XY}(u,v)=\\frac{1}{2} $，需要 $ 0\\leq u\u003c v\\leq 1 $。$ D_{h} $ 区域 $ \\left| J_{2} \\right|=1 $，满足 $ u=y,v=x $，贡献 $ f_{XY}(v,u)= \\frac{1}{2} $,需要 $ 0\\leq u\u003c v\\leq 1 $ 或者 $ 0\\leq u\\leq 1 $ 并且 $ 1\u003c v\\leq 2 $。我们考虑叠加两个区域的贡献，需要分两种情况，在 $ 0\\leq u\\leq v\\leq 1 $ 时同时被 $ D_{g} $ 和 $ D_{h} $ 覆盖，在 $ 0\\leq u\\leq 1\u003c v\\leq 2 $ 时只被 $ D_{h} $ 覆盖，因此 $$ f_{UV}(u,v) = \\begin{cases} 1, \u0026 0\\leq u\\leq v\\leq 1 \\\\ 0.5, \u0026 0\\leq u\\leq 1\u003c v\\leq 2 \\\\ 0, \u0026 \\text{o.w.} \\end{cases} $$\nProblem 4 (1)\n由于 $ q\\in(0,1) $，因此事件 $ \\{ Y=t \\} $ 等价于 $ (0,1)\\times \\{ t \\} $，所以 $$ \\mathbb{P}(Y=t)=\\binom{ T }{ t } \\int_{0}^{1} q^{t}(1-q)^{T-t} \\mathrm{d}q = \\binom{ T }{ t } \\cdot \\dfrac{t!(T-t)!}{(T+1)!} = \\dfrac{1}{T+1} $$ 说明 $$ \\mathbb{P}(Y=t)=\\dfrac{1}{T+1},\\quad t\\in[T]\\cup \\{ 0 \\} $$ 这说明在先验分布均匀的情况下，正面次数也均匀分布。\n(2)\n为了计算 $ Q $ 的概率密度，我们考虑 $ \\mathbb{P}(Q\\in[a,b]) $。根据全概率公式，我们知道 $$ \\begin{align*} \\mathbb{P}(Q\\in[a,b]) \u0026 = \\sum_{t=0}^{T} \\mathbb{P}(Q\\in[a,b]\\mid Y=t) \\\\ \u0026 = \\sum_{t=0}^{T} \\int_{a}^{b} \\binom{ T }{ t } q^{t}(1-q)^{T-t} \\mathrm{d}q \\\\ \u0026 = \\int_{a}^{b} \\sum_{t=0}^{T} q^{t}(1-q)^{T-t} \\mathrm{d}q \\\\ \u0026 = \\int_{a}^{b} 1 \\mathrm{d}q \\\\ \u0026 = b-a \\end{align*} $$ 从而说明这是一个均匀分布。因此 $$ p_{Q} = \\begin{cases} 1, \u0026 q\\in(0,1) \\\\ 0, \u0026 \\text{o.w.} \\end{cases} $$ 也就是 $ Q\\sim U(0,1) $。\n下面我们计算条件概率密度函数。根据定义，我们先计算 $$ \\mathbb{P}(Q\\in[q,q+h],Y=t) = \\int_{q}^{q+h} \\binom{ T }{ t } u^{t}(1-u)^{T-t} \\mathrm{d}u $$ 根据微积分基本定理，我们知道 $$ \\lim_{ h \\to 0 } \\dfrac{1}{h}\\cdot \\mathbb{P}(Q\\in[q,q+h],Y=t) = \\binom{ T }{ t } q^{t}(1-q)^{T-t} $$ 再根据第一问 $ \\mathbb{P}(Y=t)= \\frac{1}{T+1} $，我们知道 $$ \\begin{align*} p_{Q|Y}(q|t) \u0026 = \\binom{ T }{ t } q^{t}(1-q)^{T-t}\\cdot \\dfrac{1}{1 / (T+1)} \\\\ \u0026 = \\dfrac{(T+1)!}{t!(T-t)!}q^{t}(1-q)^{T-t} \\end{align*} $$ (3)\n根据定义，有 $$ M_{X}(\\theta) = \\mathbb{E}\\left[ 1 + \\sum_{n=1}^{\\infty} \\dfrac{\\theta^{n}}{n!}X^{n} \\right] = 1+\\sum_{n=1}^{\\infty} \\dfrac{\\theta^{n}}{n!}\\mathbb{E}[X^{n}] $$ 我们考虑计算 $ \\mathbb{E}[X^{n}] $。\n根据 $ (1) $ 式，我们得到 $$ \\begin{align*} \\mathbb{E}[X^{n}] \u0026 = \\int_{0}^{1} x^{n}p_{X}(x) \\mathrm{d}x \\\\ \u0026 = \\int_{0}^{1} x^{n} \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\mathrm{d}x \\\\ \u0026 = \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_{0}^{1} x^{(n+\\alpha)-1}(1-x)^{\\beta-1} \\mathrm{d}x \\\\ \u0026 = \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\dfrac{\\Gamma(n+\\alpha)\\Gamma(\\beta)}{\\Gamma(n+\\alpha+\\beta)} \\\\ \u0026 = \\dfrac{\\Gamma(n+\\alpha)}{\\Gamma(\\alpha)}\\cdot \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(n+\\alpha+\\beta)} \\\\ \u0026 = \\prod_{k=0}^{n-1} \\dfrac{\\alpha+k}{\\alpha+\\beta+k} \\end{align*} $$ 其中最后一行利用了 $ \\Gamma $ 函数递推的性质。\n带入就得到了 $$ M_{X}(\\theta) = 1 + \\sum_{n=1}^{\\infty} \\dfrac{\\theta^{n}}{n!}\\cdot \\left( \\prod_{k=0}^{n-1} \\dfrac{\\alpha+k}{\\alpha+\\beta+k} \\right) $$\nProblem 5 (1)\n设随机变量 $ X\\sim\\text{Exp}(\\lambda) $，它的 pdf 和 cdf 分别为 $$ f(x) = \\lambda e^{ -\\lambda x }(x\\geq 0),\\quad F(x) = \\int_{0}^{x} \\lambda e^{ -\\lambda x } \\mathrm{d}x =1 - e^{ -\\lambda x } $$ 若 $ Y $ 是一个随机变量， CDF 为 $ F_{Y} $。那么利用 CDF 的单调性与定义可知，变换后的变量小于 $ u $ 的概率 $ P(F_Y(Y) \\le u) $ 等价于 $ P(Y \\le F_Y^{-1}(u)) $，而这恰好等于 $ F_Y(F_Y^{-1}(u)) = u $，这完全符合均匀分布的定义，因此我们知道 $ F_{Y}(Y) $ 服从 $ [0,1] $ 上的均匀分布。\n因此我们有 $ F(X)\\sim U[0,1] $，设 $ F(X)=U\\sim U[0,1] $，我们就可以得到 $ X=F^{-1}(U) $，从而 $$ \\begin{align*} 1 - e^{ -\\lambda X } \u0026 = U \\\\ e^{ -\\lambda X } \u0026 = 1-U \\\\ \\implies X \u0026 = - \\dfrac{1}{\\lambda}\\ln(1-U) \\end{align*} $$ 由于 $ 1-U $ 也服从 $ U[0,1] $，因此我们就得到了采样公式为 $$ X = -\\dfrac{1}{\\lambda}\\ln U $$ (2)\n根据题设，有 $$ \\begin{align*} f_{X}(x) \u0026 = \\dfrac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{ -x },\\quad x\u003e0 \\\\ f_{Y}(y) \u0026 = \\dfrac{1}{\\Gamma(\\beta)}y^{\\beta-1}e^{ -y },\\quad y\u003e0 \\end{align*} $$ 我们用 $ S,T $ 表示 $ X,Y $，得到 $$ X = ST, \\quad Y = S(1-T) $$ 从而 Jacobi 行列式为 $$ J = \\begin{vmatrix} t \u0026 s \\\\ 1-t \u0026 -s \\end{vmatrix} = -s \\implies \\left| J \\right| = s\\quad (S\u003e0) $$ 于是得到 $$ f_{ST}(s,t) = f_{XY}(x(s,t),y(s,t))\\cdot \\left| J \\right| = f_{XY}(st,s-st)\\cdot s $$ 其中由于 $ X,Y $ 独立 $$ f_{XY}(x,y) = f_{X}(x)\\cdot f_{Y}(y) = \\dfrac{1}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}y^{\\beta-1}e^{ -(x+y) } $$\n带入就得到了 $$ f_{ST}(s,t) = \\dfrac{1}{\\Gamma(\\alpha)\\Gamma(\\beta)} s ^{\\alpha+\\beta - 1}e^{ -s }t ^{\\alpha-1}(1-t)^{\\beta-1} $$ 定义域为 $ s\u003e0,0\u003c t\u003c 1 $。\n(3)\n观察 $ f_{ST} $，发现可以拆分成关于 $ s $ 和关于 $ t $ 的函数的乘积： $$ \\begin{align*} f_{ST}(s,t) \u0026 = \\dfrac{1}{\\Gamma(\\alpha)\\Gamma(\\beta)}(s^{\\alpha+\\beta-1}e^{ -s })(t ^{\\alpha-1}(1-t)^{\\beta-1}) \\\\ \u0026 = \\underbrace{ \\left( \\dfrac{1}{\\Gamma(\\alpha+\\beta)}s ^{(\\alpha+\\beta)-1}e^{ -s } \\right) }_{ g(s) }\\underbrace{ \\left( \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}t ^{\\alpha-1}(1-t)^{\\beta-1} \\right) }_{ h(t) } \\end{align*} $$ 从而 $ f_{ST}(s,t)=g(s)h(t) $，并且 $ s $ 和 $ t $ 的范围相互不依赖，从而 $ S $ 和 $ T $ 相互独立。\n我们接着讨论 $ T $ 的分布。根据上面的拆分，我们知道 $$ f_{T}(t)=h(t) = \\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}t ^{\\alpha-1}(1-t)^{\\beta-1} $$ 这正是 $ T\\sim\\text{Beta}(\\alpha,\\beta) $ 的定义。\nProblem 6 (1)\n令 $ Z=X_{0} / \\sigma $，于是 $$ \\mathbb{E}[Z] = \\mathbb{E}\\left[ \\dfrac{X_{0}}{\\sigma} \\right] = \\dfrac{\\mathbb{E}[X_{0}]}{\\sigma} = \\dfrac{1}{\\sigma}\\cdot 0 = 0 $$ 以及 $$ \\text{Var}[Z] = \\text{Var}\\left[ \\dfrac{X_{0}}{\\sigma} \\right] = \\dfrac{\\text{Var}[X_{0}]}{\\sigma^{2}} = 1 $$ 并且由于 $ X_{0} $ 符合正态分布，因此经过线性变换得到 $ Z $ 以后依旧符合正态分布。\n因此 $ Z $ 服从期望为 $ 0 $，方差为 $ 1 $ 的正态分布，即 $ Z\\sim \\mathcal{N}(0,1) $。\n(2)\n我们展开 $ \\sum_{i=1}^{n}(X_{i}-\\hat{X}_{n})^{2} $，得到 $$ \\begin{align*} \\sum_{i=1}^{n} (X_{i}-\\hat{X}_{n})^{2} \u0026 = \\sum_{i=1}^{n} X_{i}^{2} - 2\\hat{X}_{n}\\sum_{i=1}^{n} X_{i} + n\\hat{X}_{n}^{2} \\\\ \u0026 = \\sum_{i=1}^{n} X_{i}^{2} - n\\hat{X}_{n}^{2} \\end{align*} $$ 因此我们需要计算 $ \\mathbb{E}[\\sum_{i=1}^{n} X_{i}^{2} - n\\hat{X}_{n}^{2}] $。\n对于 $ \\mathbb{E}[X_{i}^{2}] $ 项，我们利用方差公式，得到 $$ \\mathbb{E}[X_{i}^{2}] = \\text{Var}[X_{i}] + (\\mathbb{E}[X_{i}])^{2} = \\sigma^{2} + 0 = \\sigma^{2} $$ 所以 $ \\mathbb{E}\\left[ \\sum_{i=1}^{n}X_{i}^{2} \\right]=\\sum_{i=1}^{n}\\mathbb{E}[X_{i}^{2}]=n\\sigma^{2} $。\n对于 $ \\mathbb{E}[\\hat{X}_{n}^{2}] $，我们先计算 $ \\mathbb{E}[\\hat{X}_{n}] $ 和 $ \\text{Var}[\\hat{X}_{n}] $，得到 $$ \\mathbb{E}[\\hat{X}_{n}]=0,\\quad \\text{Var}[\\hat{X}_{n}] = \\text{Var}\\left[ \\dfrac{1}{n}\\sum X_{i} \\right] = \\dfrac{\\sigma^{2}}{n} $$ 从而 $$ \\mathbb{E}[\\hat{X}_{n}^{2}] = \\mathbb{E}[\\hat{X}_{n}] + \\text{Var}[\\hat{X}_{n}] = \\dfrac{\\sigma^{2}}{n} $$ 于是 $$ \\mathbb{E}\\left[ \\sum_{i=1}^{n} (X_{i}-\\hat{X}_{n})^{2} \\right] = n\\sigma^{2} - n\\left( \\dfrac{\\sigma^{2}}{n} \\right) = (n-1)\\sigma^{2} $$ 这样就算出了 $$ \\mathbb{E}[S_{n}^{2}] = \\mathbb{E}\\left[ \\dfrac{\\sum_{i=1}^{n} (X_{i}-\\hat{X}_{n})^{2}}{n-1} \\right] = \\sigma^{2} $$ (3)\n按照定义， $$ \\hat{X}_{2} = \\dfrac{X_{1}+X_{2}}{2}\\implies S_{2}^{2}=\\dfrac{1}{2-1}\\sum_{i=1}^{2} (X_{i}-\\hat{X}_{2})^{2} = \\dfrac{(X_{1}-X_{2})^{2}}{2} $$ 从而得到 $$ S_{2} = \\dfrac{\\left| X_{1}-X_{2} \\right| }{\\sqrt{ 2 }} $$ 进而得到 $$ Y_{2} = \\dfrac{X_{0}}{S_{2}} = \\dfrac{\\sqrt{ 2 }X_{0}}{\\left| X_{1}-X_{2} \\right| } $$ 我们直到 $ X_{0}\\sim \\mathcal{N}(0,\\sigma^{2}) $。令 $ D=X_{1}-X_{2} $，由于 $ X_{1},X_{2} $ 独立且均服从 $ \\mathcal{N}(0,\\sigma^{2}) $ 分布，因此 $ D\\sim \\mathcal{N}(0,\\sigma^{2}+\\sigma^{2})=\\mathcal{N}(0,2\\sigma^{2}) $。我们将它们写成标准正态分布，就有 $ \\dfrac{X_{0}}{\\sigma}=U\\sim \\mathcal{N}(0,1) $ 以及 $ \\dfrac{D}{\\sqrt{ 2 }\\sigma}=V\\sim \\mathcal{N}(0,1) $。带入就有 $$ Y_{2} = \\dfrac{\\sqrt{ 2 }(\\sigma U)}{\\left| \\sqrt{ 2\\sigma V } \\right| } = \\dfrac{U}{\\left| V \\right| } $$ 我们设 $ W=\\left| V \\right| $，那么 $ W $ 服从分布 $$ p_{W}(w) = \\dfrac{2}{\\sqrt{ 2\\pi }}e^{ -w^{2} / 2 },\\quad w\u003e0 $$ 带入 $ Y=U / W $，计算 Jacobi 行列式并带入即可得到 $$ \\begin{align*} p_{Y_{2}}(y) \u0026 = \\int_{-\\infty}^{\\infty} p_{UW}(yw,w)\\cdot \\left| w \\right| \\mathrm{d}w \\\\ \u0026 = \\int_{0}^{\\infty} w\\cdot p_{U}(yw)\\cdot p_{W}(w) \\mathrm{d}w \\\\ \u0026 = \\int_{0}^{\\infty} w\\cdot\\left( \\dfrac{1}{\\sqrt{ 2\\pi }}e^{ -(yw)^{2}/2 } \\right)\\cdot\\left( \\dfrac{2}{\\sqrt{ 2\\pi }}e^{ -\\omega^{2}/2 } \\right) \\mathrm{d}w \\\\ \u0026 = \\dfrac{1}{\\pi}\\int_{0}^{\\infty} we^{ -w^{2}(y^{2}+1)/2 } \\mathrm{d}w \\\\ \u0026 = \\dfrac{1}{\\pi(1+y^{2})}[e^{ -(y^{2}+1)w^{2}/2 }]_{0}^{\\infty} \\\\ \u0026 = \\dfrac{1}{\\pi(1+y^{2})} \\end{align*} $$ 说明了 pdf 为 $ p_{Y_{2}}(y)=1 / \\pi(y^{2}+1) $，就是标准柯西分布的 pdf。\n(4)\n我们需要证明在 $ \\theta\\neq 0 $ 时 $ \\mathbb{E}[e^{ \\theta Y_{2} }] $ 发散。根据定义，我们有 $$ \\begin{align*} \\mathbb{E}[e^{ \\theta Y_{2} }] \u0026 = \\int_{-\\infty}^{\\infty} e^{ \\theta y }\\cdot p_{Y_{2}}(y) \\mathrm{d}y \\\\ \u0026 = \\dfrac{1}{\\pi}\\int_{-\\infty}^{\\infty} \\dfrac{e^{ \\theta y }}{1+y^{2}} \\mathrm{d}y \\\\ \u0026 \u003e \\dfrac{1}{\\pi}\\int_{0}^{\\infty} \\dfrac{e^{ \\theta y }}{1+y^{2}} \\mathrm{d}y \\end{align*} $$ 对于任意 $ \\theta\u003e0 $，显然存在 $ M\u003e0 $ 使得当 $ y\u003eM $ 时有 $$ e^{ \\theta y }\u003e1+y^{2} $$ 从而 $$ \\begin{align*} \\int_{0}^{\\infty} \\dfrac{e^{ \\theta y }}{1+y^{2}} \\mathrm{d}y \u0026 \u003e \\int_{M}^{\\infty} \\dfrac{e^{ \\theta y }}{1+y^{2}} \\mathrm{d}y \\\\ \u0026 \u003e \\int_{M}^{\\infty} 1\\cdot \\mathrm{d}y \\\\ \u0026 = \\infty \\end{align*} $$ 从而得到了 $ \\mathbb{E}[e^{ \\theta Y_{2} }] $ 发散。这意味着 $ Y_{2} $ 的矩生成函数对于任意 $ \\theta\\neq 0 $ 均不存在。\n","wordCount":"2132","inLanguage":"en","image":"https://diefish1024.github.io/images/avatar.jpg","datePublished":"2025-11-19T09:20:00+08:00","dateModified":"2025-11-19T09:20:00+08:00","author":{"@type":"Person","name":"diefish"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/math2701-hw5/"},"publisher":{"@type":"Organization","name":"diefish's blog","logo":{"@type":"ImageObject","url":"https://diefish1024.github.io/images/avatar.jpg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://diefish1024.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://diefish1024.github.io/images/avatar.jpg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://diefish1024.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://diefish1024.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://diefish1024.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://diefish1024.github.io/search/ title=Search><span>Search</span></a></li><li><a href=https://diefish1024.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://diefish1024.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/class-notes/>Class Notes</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/posts/class-notes/math2701-probability-theory/>MATH2701 Probability Theory</a></div><h1 class="post-title entry-hint-parent">MATH2701 HW5</h1><div class=post-meta><span title='2025-11-19 09:20:00 +0800 +0800'>November 19, 2025</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;diefish&nbsp;|&nbsp;<a href=https://github.com/diefish1024/diefish1024.github.io/blob/main/content/posts/Class%20Notes/MATH2701%20Probability%20Theory/math2701-hw5.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem-1 aria-label="Problem 1">Problem 1</a></li><li><a href=#problem-2 aria-label="Problem 2">Problem 2</a></li><li><a href=#problem-3 aria-label="Problem 3">Problem 3</a></li><li><a href=#problem-4 aria-label="Problem 4">Problem 4</a></li><li><a href=#problem-5 aria-label="Problem 5">Problem 5</a></li><li><a href=#problem-6 aria-label="Problem 6">Problem 6</a></li></ul></div></details></div><div class=post-content><h2 id=problem-1>Problem 1<a hidden class=anchor aria-hidden=true href=#problem-1>#</a></h2><p><strong>(1)</strong></p><p>根据题设条件，我们知道最多抽取 $ m $ 个黑球，最少抽取 $ 0 $ 个黑球。设随机变量 $ X $ 表示抽到黑球的个数，根据组合意义，我们得到 $ X $ 的概率质量函数为
$$
\mathbb{P}(X=x) = \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } },\quad x = 0,1,\dots,m
$$
<strong>(2)</strong></p><p>矩生成函数定义为 $ M_{X}(\theta) = \mathbb{E}[e^{ \theta X }] $。我们带入得到
$$
\begin{align*}
M_{X}(\theta) & = \sum_{x}e^{ \theta x }\mathbb{P}(X=x) \\
& = \sum_{x=0}^{m} e^{ \theta x } \dfrac{\binom{ m }{ x } \binom{ n-m }{ k-x } }{\binom{ n }{ k } } \\
& = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} e^{ \theta x }\binom{ m }{ x } \binom{ n-m }{ k-x }
\end{align*}
$$
<strong>(3)</strong></p><p>首先计算 $ \mathbb{E}[X] $。
$$
\begin{align*}
\mathbb{E}[X] & = M'_{X}(0) = \dfrac{\mathrm{d} }{\mathrm{d}\theta} \left( \sum_{x}e^{ \theta x }\mathbb{P}(X=x) \right)\bigg|_{\theta=0} \\
& = \sum_{x=0}^{m} x\mathbb{P}(X=x) \\
& = \dfrac{1}{\binom{ n }{ k }} \sum_{x=0}^{m} x\binom{ m }{ x } \binom{ n-m }{ k-x } \\
& = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} m\binom{ m-1 }{ x-1 } \binom{ n-m }{ k-x } \\
& = \dfrac{m}{\binom{ n }{ k } } \sum_{y=0}^{m-1} \binom{ m-1 }{ y } \binom{ n-m }{ k-y-1 } \\
& = \dfrac{m}{\binom{ n }{ k } }\binom{ n-m+m-1 }{ k-1 } = \dfrac{m}{\binom{ n }{ k } }\binom{ n-1 }{ k-1 } \\
& = \dfrac{mk}{n}
\end{align*}
$$
接着计算 $ \mathbb{E}[X^{2}] $。由于 $ \mathbb{E}[X^{2}] $ 的表达式化简较为困难，我们考虑求解 $ \mathbb{E}[X(X-1)] $。根据 LOTUS，我们有
$$
\begin{align*}
\mathbb{E}[X(X-1)] & = \sum_{x=0}^{m} x(x-1)\mathbb{P}(X=x) \\
& = \dfrac{1}{\binom{ n }{ k } }\sum_{x=0}^{m} x(x-1)\binom{ m }{ x } \binom{ n-m }{ k-x } \\
& = \dfrac{m}{\binom{ n }{ k } }\sum_{x=0}^{m} (x-1)\binom{ m-1 }{ x-1 } \binom{ n-m }{ k-x } \\
& = \dfrac{m(m-1)}{\binom{ m }{ k } }\sum_{x=0}^{m} \binom{ m-2 }{ x-2 } \binom{ n-m }{ k-x } \\
& = \dfrac{m(m-1)}{\binom{ m }{ k } }\binom{ n-2 }{ k-2 } \\
& = \dfrac{m(m-1)k(k-1)}{n(n-1)}
\end{align*}
$$
从而带入方差公式得到
$$
\begin{align*}
\text{Var}[X] & = \mathbb{E}[X^{2}]-(\mathbb{E}[X])^{2} \\
& = \mathbb{E}[X(X-1)] + \mathbb{E}[X] - (\mathbb{E}[X])^{2} \\
& = \dfrac{m(m-1)k(k-1)}{n(n-1)} + \dfrac{mk}{n} - \left( \dfrac{mk}{n} \right)^{2} \\
& = \dfrac{mk}{n}\left[ \dfrac{(m-1)(k-1)}{n-1}+1 - \dfrac{mk}{n}\right] \\
& = \dfrac{mk}{n}\cdot \dfrac{n-m}{n}\cdot \dfrac{n-k}{n-1}
\end{align*}
$$
综上得到
$$
\mathbb{E}[X] = \dfrac{mk}{n},\quad \text{Var}[X] = \dfrac{mk}{n}\cdot \dfrac{n-m}{n}\cdot \dfrac{n-k}{n-1}
$$</p><h2 id=problem-2>Problem 2<a hidden class=anchor aria-hidden=true href=#problem-2>#</a></h2><p><strong>(1)</strong></p><p>首先我们计算 $ X $ 的 pdf，记为 $ f_{X}(x) $，那么
$$
f_{X}(x) = \prod_{i=1}^{n} f_{X_{i}}(x_{i}) = \prod_{i=1}^{n} \dfrac{1}{\sqrt{ 2\pi }}e^{ -x_{i}^{2} / 2 } = \dfrac{1}{(2\pi)^{n / 2}}e^{ - \frac{1}{2}\sum_{i=1}^{n} x_{i}^{2} }
$$
由于 $ \| x \|^{2}=x^{T}x = \sum_{i=1}^{n}x_{i}^{2} $，我们可以化简为
$$
f_{X}(x) = \dfrac{1}{(2\pi)^{n / 2}}e^{ - \frac{1}{2} x^{T}x }
$$</p><p>根据题设，我们知道
$$
y = Qx \implies x = Q^{T}y
$$
从而计算该变换雅可比行列式的绝对值，有
$$
J = \left| \det\left( \dfrac{ \partial x }{ \partial y } \right) \right| = \left| \det(Q^{T}) \right|
$$
根据正定矩阵的性质，我们知道 $ \det Q^{T}=\det Q=\pm 1 $，因此 $ J=1 $。</p><p>于是根据变量代换公式，我们得到
$$
f_{Y}(y) = f_{X}(x(y))\cdot J = f_{X}(Q^{T}y)\cdot 1
$$
带入 $ Q^{T}y $，其中 $ x^{T}x $ 变为 $ y^{T}Q Q^{T}y=y^{T}y $ 即可得到
$$
f_{Y}(y) = \dfrac{1}{(2\pi)^{n/2}}\exp\left( -\dfrac{1}{2}y^{T} y\right)
$$
从而 $ x $ 与 $ y $ 的分布一致。</p><p><strong>(2)</strong></p><p>对于单位球面上的两个向量 $ z_{1} $ 和 $ z_{2} $，利用线性代数可以证明存在正交矩阵 $ Q $ 使得
$$
z_{2}=Qz_{1}
$$
我们考虑变换后的随机变量 $ QZ $，得到
$$
QZ = Q\left( \dfrac{X}{\| X \| } \right)= \dfrac{QX}{\| X \| } \xlongequal{\| QX \| = \| X \| } \dfrac{QX}{\| QX \| }
$$
我们令 $ Y=QX $，就有 $ QZ=\dfrac{Y}{\| Y \|} $。利用 $ (1) $ 的结论，我们知道 $ X $ 和 $ Y $ 是同分布的，从而 $ Z $ 和 $ QZ $ 是同分布的。这就说明了 $ Z $ 具有旋转不变性，也就有 $ Z $ 在球面上取 $ z_{1} $ 附近的概率密度，必须等于取 $ z_{2}=Qz_{1} $ 附近的概率密度，也就是
$$
f_{Z}(z) = f_{Z}(Qz)
$$
我们可以找到 $ Q $ 将球面上的任意点 $ z_{1} $ 映射到球面上的任意点 $ z_{2} $，从而说明了
$$
\forall z_{1},z_{2}\in S^{n-1},\quad f_{Z}(z_{1})=f_{Z}(z_{2})
$$</p><h2 id=problem-3>Problem 3<a hidden class=anchor aria-hidden=true href=#problem-3>#</a></h2><p><strong>(1)</strong></p><p>根据题设我们有
$$
\begin{cases}
u = xy \\
v = \dfrac{x}{y}
\end{cases}
$$
从而得到 $ x=\sqrt{ uv },y=\sqrt{ u / v } $。我们计算变换的 Jacobi 行列式
$$
J = \det\left( \dfrac{ \partial (x,y) }{ \partial (u,v) } \right) = \det \begin{pmatrix}
\dfrac{1}{2}\sqrt{ \dfrac{v}{u} } & \dfrac{1}{2}\sqrt{ \dfrac{u}{v} } \\
\dfrac{1}{2\sqrt{ uv }} & -\dfrac{1}{2v}\sqrt{ \dfrac{u}{v} }
\end{pmatrix} = - \dfrac{1}{2v}
$$
从而 $ \left| J \right|= \dfrac{1}{2v} $。</p><p>于是我们就得到了联合密度函数为
$$
f_{UV}(u,v) = f_{XY}(\sqrt{ uv },\sqrt{ u / v }) \cdot \left| J \right| = 1 \cdot\dfrac{1}{2v} = \dfrac{1}{2v}
$$
<strong>(2)</strong></p><p>我们需要分区域讨论。将区域 $ [0,1]^{2} $ 沿对角线分成两个区域
$$
D_{1} = \{ (x,y)\mid 0\leq x < y\leq 1 \},\quad D_{2} = [0,1]^{2}\setminus D_{1}
$$
我们知道在 $ D_{1} $ 上有 $ U=X,V=Y $，在 $ D_{2} $ 上有 $ U=Y,V=X $。</p><p>对于目标区域 $ 0\leq u\leq v\leq 1 $，我们分别考虑来着 $ D_{1} $ 和 $ D_{2} $ 的贡献。来自 $ D_{1} $ 的映射满足 $ (x,y)=(u,v) $，从而 Jacobi 行列式为 $ \left| J_{1} \right|=1 $，产生的贡献为 $ f_{XY}(u,v)\cdot 1=1 $。来自 $ D_{2} $ 的映射满足 $ (x,y)=(v,u) $，对应的 Jacobi 行列式为 $ \left| J_{2} \right|=1 $，产生的贡献为 $ f_{XY}(u,v)\cdot 1=1 $。</p><p>我们将两部分贡献相加，就得到了
$$
f_{UV}(u,v)=1+1=2
$$
<strong>(3)</strong></p><p>我们需要证明 $ f_{UV}(u,v) $ 是各分支逆映射密度函数乘以 Jacobi 行列式后的叠加。</p><p>设 $ A $ 是 $ UV $ 平面上任意的 Borel 可测集。我们需要计算 $ \mathbb{P}((U,V)\in A) $。根据全概率公式，将事件分解到两个不相交的区域 $ D_{g} $ 和 $ D_{h} $ 上，有
$$
\begin{align*}
\mathbb{P}((U,V)\in A) & = \mathbb{P}((X,Y)\in D_{g},g(X,Y)\in A) +\mathbb{P}((X,Y)\in D_{h},h(X,Y)\in A) \\
& = \underset{ \{ (x,y)\in D_{g}\mid g(x,y)\in A \} }{ \iint } f_{XY}(x,y)\mathrm{d}x\mathrm{d}y + \underset{ \{ (x,y)\in D_{h}\mid h(x,y)\in A \} }{ \iint } f_{XY}(x,y)\mathrm{d}x\mathrm{d}y
\end{align*}
$$
对于第一个积分进行变量代换，令 $ (u,v)=g(x,y) $，则 $ (x,y)=g^{-1}(u,v) $。此时积分区域变为 $ A\cap I_{g} $，面积元变为 $ \left| J_{g^{-1}} \right|\mathrm{d}u\mathrm{d}v $。同理对于第二个积分，令 $ (u,v)=h(x,y) $，那么积分区域变为 $ A\cap I_{h} $，面积元变为 $ \left| J_{^{-1}h} \right|\mathrm{d}u\mathrm{d}v $。带入就得到了
$$
\underset{ A\cap I_{g} }{ \iint } f_{XY}(g^{-1}(u,v))\left| J_{g^{-1}} \right| \mathrm{d}u\mathrm{d}v + \underset{ A\cap I_{h} }{ \iint } f_{XY}(h^{-1}(u,v))\left| J_{h^{-1}} \right| \mathrm{d}u\mathrm{d}v
$$
利用指示函数将积分区域统一为 $ A $，就有
$$
\mathbb{P}((U,V)\in A) = \underset{ A }{ \iint } [f_{XY}(g^{-1}(u,v))\left| J_{g^{-1}} \right|\cdot \mathbb{I}_{(u,v)\in I_{g}} + f_{XY}(h^{-1}(u,v))\left| J_{h^{-1}} \right|\cdot \mathbb{I}_{(u,v)\in I_{h}}] \mathrm{d}u\mathrm{d}v
$$
根据 pdf 的定义，被积函数即为 $ f_{UV}(u,v) $，从而得证。</p><p><strong>(4)</strong></p><p>由题设，$ X\sim U[0,1],Y\sim U[0,2] $，并且两者独立。我们知道 $ f_{XY}(x,y)=f_{X}\cdot f_{Y}=\frac{1}{2} $。</p><p>将矩形区域 $ R=[0,2]\times[0,1] $ 按照直线 $ y=x $ 切割成两部分，分别为 $ D_{g} $ 满足 $ X< Y $，$ D_{h} $ 满足 $ X>Y $。</p><p>其中 $ D_{g} $ 区域 $ \left| J_{1} \right|=1 $，满足 $ u=x,v=y $，贡献 $ f_{XY}(u,v)=\frac{1}{2} $，需要 $ 0\leq u< v\leq 1 $。$ D_{h} $ 区域 $ \left| J_{2} \right|=1 $，满足 $ u=y,v=x $，贡献 $ f_{XY}(v,u)= \frac{1}{2} $,需要 $ 0\leq u< v\leq 1 $ 或者 $ 0\leq u\leq 1 $ 并且 $ 1< v\leq 2 $。我们考虑叠加两个区域的贡献，需要分两种情况，在 $ 0\leq u\leq v\leq 1 $ 时同时被 $ D_{g} $ 和 $ D_{h} $ 覆盖，在 $ 0\leq u\leq 1< v\leq 2 $ 时只被 $ D_{h} $ 覆盖，因此
$$
f_{UV}(u,v) = \begin{cases}
1, & 0\leq u\leq v\leq 1 \\
0.5, & 0\leq u\leq 1< v\leq 2 \\
0, & \text{o.w.}
\end{cases}
$$</p><h2 id=problem-4>Problem 4<a hidden class=anchor aria-hidden=true href=#problem-4>#</a></h2><p><strong>(1)</strong></p><p>由于 $ q\in(0,1) $，因此事件 $ \{ Y=t \} $ 等价于 $ (0,1)\times \{ t \} $，所以
$$
\mathbb{P}(Y=t)=\binom{ T }{ t } \int_{0}^{1} q^{t}(1-q)^{T-t} \mathrm{d}q = \binom{ T }{ t } \cdot \dfrac{t!(T-t)!}{(T+1)!} = \dfrac{1}{T+1}
$$
说明
$$
\mathbb{P}(Y=t)=\dfrac{1}{T+1},\quad t\in[T]\cup \{ 0 \}
$$
这说明在先验分布均匀的情况下，正面次数也均匀分布。</p><p><strong>(2)</strong></p><p>为了计算 $ Q $ 的概率密度，我们考虑 $ \mathbb{P}(Q\in[a,b]) $。根据全概率公式，我们知道
$$
\begin{align*}
\mathbb{P}(Q\in[a,b]) & = \sum_{t=0}^{T} \mathbb{P}(Q\in[a,b]\mid Y=t) \\
& = \sum_{t=0}^{T} \int_{a}^{b} \binom{ T }{ t } q^{t}(1-q)^{T-t} \mathrm{d}q \\
& = \int_{a}^{b} \sum_{t=0}^{T} q^{t}(1-q)^{T-t} \mathrm{d}q \\
& = \int_{a}^{b} 1 \mathrm{d}q \\
& = b-a
\end{align*}
$$
从而说明这是一个均匀分布。因此
$$
p_{Q} = \begin{cases}
1, & q\in(0,1) \\
0, & \text{o.w.}
\end{cases}
$$
也就是 $ Q\sim U(0,1) $。</p><p>下面我们计算条件概率密度函数。根据定义，我们先计算
$$
\mathbb{P}(Q\in[q,q+h],Y=t) = \int_{q}^{q+h} \binom{ T }{ t } u^{t}(1-u)^{T-t} \mathrm{d}u
$$
根据微积分基本定理，我们知道
$$
\lim_{ h \to 0 } \dfrac{1}{h}\cdot \mathbb{P}(Q\in[q,q+h],Y=t) = \binom{ T }{ t } q^{t}(1-q)^{T-t}
$$
再根据第一问 $ \mathbb{P}(Y=t)= \frac{1}{T+1} $，我们知道
$$
\begin{align*}
p_{Q|Y}(q|t) & = \binom{ T }{ t } q^{t}(1-q)^{T-t}\cdot \dfrac{1}{1 / (T+1)} \\
& = \dfrac{(T+1)!}{t!(T-t)!}q^{t}(1-q)^{T-t}
\end{align*}
$$
<strong>(3)</strong></p><p>根据定义，有
$$
M_{X}(\theta) = \mathbb{E}\left[ 1 + \sum_{n=1}^{\infty} \dfrac{\theta^{n}}{n!}X^{n} \right] = 1+\sum_{n=1}^{\infty} \dfrac{\theta^{n}}{n!}\mathbb{E}[X^{n}]
$$
我们考虑计算 $ \mathbb{E}[X^{n}] $。</p><p>根据 $ (1) $ 式，我们得到
$$
\begin{align*}
\mathbb{E}[X^{n}] & = \int_{0}^{1} x^{n}p_{X}(x) \mathrm{d}x \\
& = \int_{0}^{1} x^{n} \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} \mathrm{d}x \\
& = \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \int_{0}^{1} x^{(n+\alpha)-1}(1-x)^{\beta-1} \mathrm{d}x \\
& = \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \cdot \dfrac{\Gamma(n+\alpha)\Gamma(\beta)}{\Gamma(n+\alpha+\beta)} \\
& = \dfrac{\Gamma(n+\alpha)}{\Gamma(\alpha)}\cdot \dfrac{\Gamma(\alpha+\beta)}{\Gamma(n+\alpha+\beta)} \\
& = \prod_{k=0}^{n-1} \dfrac{\alpha+k}{\alpha+\beta+k}
\end{align*}
$$
其中最后一行利用了 $ \Gamma $ 函数递推的性质。</p><p>带入就得到了
$$
M_{X}(\theta) = 1 + \sum_{n=1}^{\infty} \dfrac{\theta^{n}}{n!}\cdot \left( \prod_{k=0}^{n-1} \dfrac{\alpha+k}{\alpha+\beta+k} \right)
$$</p><h2 id=problem-5>Problem 5<a hidden class=anchor aria-hidden=true href=#problem-5>#</a></h2><p><strong>(1)</strong></p><p>设随机变量 $ X\sim\text{Exp}(\lambda) $，它的 pdf 和 cdf 分别为
$$
f(x) = \lambda e^{ -\lambda x }(x\geq 0),\quad F(x) = \int_{0}^{x} \lambda e^{ -\lambda x } \mathrm{d}x =1 - e^{ -\lambda x }
$$
若 $ Y $ 是一个随机变量， CDF 为 $ F_{Y} $。那么利用 CDF 的单调性与定义可知，变换后的变量小于 $ u $ 的概率 $ P(F_Y(Y) \le u) $ 等价于 $ P(Y \le F_Y^{-1}(u)) $，而这恰好等于 $ F_Y(F_Y^{-1}(u)) = u $，这完全符合均匀分布的定义，因此我们知道 $ F_{Y}(Y) $ 服从 $ [0,1] $ 上的均匀分布。</p><p>因此我们有 $ F(X)\sim U[0,1] $，设 $ F(X)=U\sim U[0,1] $，我们就可以得到 $ X=F^{-1}(U) $，从而
$$
\begin{align*}
1 - e^{ -\lambda X } & = U \\
e^{ -\lambda X } & = 1-U \\
\implies X & = - \dfrac{1}{\lambda}\ln(1-U)
\end{align*}
$$
由于 $ 1-U $ 也服从 $ U[0,1] $，因此我们就得到了采样公式为
$$
X = -\dfrac{1}{\lambda}\ln U
$$
<strong>(2)</strong></p><p>根据题设，有
$$
\begin{align*}
f_{X}(x) & = \dfrac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{ -x },\quad x>0 \\
f_{Y}(y) & = \dfrac{1}{\Gamma(\beta)}y^{\beta-1}e^{ -y },\quad y>0
\end{align*}
$$
我们用 $ S,T $ 表示 $ X,Y $，得到
$$
X = ST, \quad Y = S(1-T)
$$
从而 Jacobi 行列式为
$$
J = \begin{vmatrix}
t & s \\
1-t & -s
\end{vmatrix} = -s \implies \left| J \right| = s\quad (S>0)
$$
于是得到
$$
f_{ST}(s,t) = f_{XY}(x(s,t),y(s,t))\cdot \left| J \right| = f_{XY}(st,s-st)\cdot s
$$
其中由于 $ X,Y $ 独立
$$
f_{XY}(x,y) = f_{X}(x)\cdot f_{Y}(y) = \dfrac{1}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}y^{\beta-1}e^{ -(x+y) }
$$</p><p>带入就得到了
$$
f_{ST}(s,t) = \dfrac{1}{\Gamma(\alpha)\Gamma(\beta)} s ^{\alpha+\beta - 1}e^{ -s }t ^{\alpha-1}(1-t)^{\beta-1}
$$
定义域为 $ s>0,0< t< 1 $。</p><p><strong>(3)</strong></p><p>观察 $ f_{ST} $，发现可以拆分成关于 $ s $ 和关于 $ t $ 的函数的乘积：
$$
\begin{align*}
f_{ST}(s,t) & = \dfrac{1}{\Gamma(\alpha)\Gamma(\beta)}(s^{\alpha+\beta-1}e^{ -s })(t ^{\alpha-1}(1-t)^{\beta-1}) \\
& = \underbrace{ \left( \dfrac{1}{\Gamma(\alpha+\beta)}s ^{(\alpha+\beta)-1}e^{ -s } \right) }_{ g(s) }\underbrace{ \left( \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t ^{\alpha-1}(1-t)^{\beta-1} \right) }_{ h(t) }
\end{align*}
$$
从而 $ f_{ST}(s,t)=g(s)h(t) $，并且 $ s $ 和 $ t $ 的范围相互不依赖，从而 $ S $ 和 $ T $ 相互独立。</p><p>我们接着讨论 $ T $ 的分布。根据上面的拆分，我们知道
$$
f_{T}(t)=h(t) = \dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}t ^{\alpha-1}(1-t)^{\beta-1}
$$
这正是 $ T\sim\text{Beta}(\alpha,\beta) $ 的定义。</p><h2 id=problem-6>Problem 6<a hidden class=anchor aria-hidden=true href=#problem-6>#</a></h2><p><strong>(1)</strong></p><p>令 $ Z=X_{0} / \sigma $，于是
$$
\mathbb{E}[Z] = \mathbb{E}\left[ \dfrac{X_{0}}{\sigma} \right] = \dfrac{\mathbb{E}[X_{0}]}{\sigma} = \dfrac{1}{\sigma}\cdot 0 = 0
$$
以及
$$
\text{Var}[Z] = \text{Var}\left[ \dfrac{X_{0}}{\sigma} \right] = \dfrac{\text{Var}[X_{0}]}{\sigma^{2}} = 1
$$
并且由于 $ X_{0} $ 符合正态分布，因此经过线性变换得到 $ Z $ 以后依旧符合正态分布。</p><p>因此 $ Z $ 服从期望为 $ 0 $，方差为 $ 1 $ 的正态分布，即 $ Z\sim \mathcal{N}(0,1) $。</p><p><strong>(2)</strong></p><p>我们展开 $ \sum_{i=1}^{n}(X_{i}-\hat{X}_{n})^{2} $，得到
$$
\begin{align*}
\sum_{i=1}^{n} (X_{i}-\hat{X}_{n})^{2} & = \sum_{i=1}^{n} X_{i}^{2} - 2\hat{X}_{n}\sum_{i=1}^{n} X_{i} + n\hat{X}_{n}^{2} \\
& = \sum_{i=1}^{n} X_{i}^{2} - n\hat{X}_{n}^{2}
\end{align*}
$$
因此我们需要计算 $ \mathbb{E}[\sum_{i=1}^{n} X_{i}^{2} - n\hat{X}_{n}^{2}] $。</p><p>对于 $ \mathbb{E}[X_{i}^{2}] $ 项，我们利用方差公式，得到
$$
\mathbb{E}[X_{i}^{2}] = \text{Var}[X_{i}] + (\mathbb{E}[X_{i}])^{2} = \sigma^{2} + 0 = \sigma^{2}
$$
所以 $ \mathbb{E}\left[ \sum_{i=1}^{n}X_{i}^{2} \right]=\sum_{i=1}^{n}\mathbb{E}[X_{i}^{2}]=n\sigma^{2} $。</p><p>对于 $ \mathbb{E}[\hat{X}_{n}^{2}] $，我们先计算 $ \mathbb{E}[\hat{X}_{n}] $ 和 $ \text{Var}[\hat{X}_{n}] $，得到
$$
\mathbb{E}[\hat{X}_{n}]=0,\quad \text{Var}[\hat{X}_{n}] = \text{Var}\left[ \dfrac{1}{n}\sum X_{i} \right] = \dfrac{\sigma^{2}}{n}
$$
从而
$$
\mathbb{E}[\hat{X}_{n}^{2}] = \mathbb{E}[\hat{X}_{n}] + \text{Var}[\hat{X}_{n}] = \dfrac{\sigma^{2}}{n}
$$
于是
$$
\mathbb{E}\left[ \sum_{i=1}^{n} (X_{i}-\hat{X}_{n})^{2} \right] = n\sigma^{2} - n\left( \dfrac{\sigma^{2}}{n} \right) = (n-1)\sigma^{2}
$$
这样就算出了
$$
\mathbb{E}[S_{n}^{2}] = \mathbb{E}\left[ \dfrac{\sum_{i=1}^{n} (X_{i}-\hat{X}_{n})^{2}}{n-1} \right] = \sigma^{2}
$$
<strong>(3)</strong></p><p>按照定义，
$$
\hat{X}_{2} = \dfrac{X_{1}+X_{2}}{2}\implies S_{2}^{2}=\dfrac{1}{2-1}\sum_{i=1}^{2} (X_{i}-\hat{X}_{2})^{2} = \dfrac{(X_{1}-X_{2})^{2}}{2}
$$
从而得到
$$
S_{2} = \dfrac{\left| X_{1}-X_{2} \right| }{\sqrt{ 2 }}
$$
进而得到
$$
Y_{2} = \dfrac{X_{0}}{S_{2}} = \dfrac{\sqrt{ 2 }X_{0}}{\left| X_{1}-X_{2} \right| }
$$
我们直到 $ X_{0}\sim \mathcal{N}(0,\sigma^{2}) $。令 $ D=X_{1}-X_{2} $，由于 $ X_{1},X_{2} $ 独立且均服从 $ \mathcal{N}(0,\sigma^{2}) $ 分布，因此 $ D\sim \mathcal{N}(0,\sigma^{2}+\sigma^{2})=\mathcal{N}(0,2\sigma^{2}) $。我们将它们写成标准正态分布，就有 $ \dfrac{X_{0}}{\sigma}=U\sim \mathcal{N}(0,1) $ 以及 $ \dfrac{D}{\sqrt{ 2 }\sigma}=V\sim \mathcal{N}(0,1) $。带入就有
$$
Y_{2} = \dfrac{\sqrt{ 2 }(\sigma U)}{\left| \sqrt{ 2\sigma V } \right| } = \dfrac{U}{\left| V \right| }
$$
我们设 $ W=\left| V \right| $，那么 $ W $ 服从分布
$$
p_{W}(w) = \dfrac{2}{\sqrt{ 2\pi }}e^{ -w^{2} / 2 },\quad w>0
$$
带入 $ Y=U / W $，计算 Jacobi 行列式并带入即可得到
$$
\begin{align*}
p_{Y_{2}}(y) & = \int_{-\infty}^{\infty} p_{UW}(yw,w)\cdot \left| w \right| \mathrm{d}w \\
& = \int_{0}^{\infty} w\cdot p_{U}(yw)\cdot p_{W}(w) \mathrm{d}w \\
& = \int_{0}^{\infty} w\cdot\left( \dfrac{1}{\sqrt{ 2\pi }}e^{ -(yw)^{2}/2 } \right)\cdot\left( \dfrac{2}{\sqrt{ 2\pi }}e^{ -\omega^{2}/2 } \right) \mathrm{d}w \\
& = \dfrac{1}{\pi}\int_{0}^{\infty} we^{ -w^{2}(y^{2}+1)/2 } \mathrm{d}w \\
& = \dfrac{1}{\pi(1+y^{2})}[e^{ -(y^{2}+1)w^{2}/2 }]_{0}^{\infty} \\
& = \dfrac{1}{\pi(1+y^{2})}
\end{align*}
$$
说明了 pdf 为 $ p_{Y_{2}}(y)=1 / \pi(y^{2}+1) $，就是标准柯西分布的 pdf。</p><p><strong>(4)</strong></p><p>我们需要证明在 $ \theta\neq 0 $ 时 $ \mathbb{E}[e^{ \theta Y_{2} }] $ 发散。根据定义，我们有
$$
\begin{align*}
\mathbb{E}[e^{ \theta Y_{2} }] & = \int_{-\infty}^{\infty} e^{ \theta y }\cdot p_{Y_{2}}(y) \mathrm{d}y \\
& = \dfrac{1}{\pi}\int_{-\infty}^{\infty} \dfrac{e^{ \theta y }}{1+y^{2}} \mathrm{d}y \\
& > \dfrac{1}{\pi}\int_{0}^{\infty} \dfrac{e^{ \theta y }}{1+y^{2}} \mathrm{d}y
\end{align*}
$$
对于任意 $ \theta>0 $，显然存在 $ M>0 $ 使得当 $ y>M $ 时有
$$
e^{ \theta y }>1+y^{2}
$$
从而
$$
\begin{align*}
\int_{0}^{\infty} \dfrac{e^{ \theta y }}{1+y^{2}} \mathrm{d}y & > \int_{M}^{\infty} \dfrac{e^{ \theta y }}{1+y^{2}} \mathrm{d}y \\
& > \int_{M}^{\infty} 1\cdot \mathrm{d}y \\
& = \infty
\end{align*}
$$
从而得到了 $ \mathbb{E}[e^{ \theta Y_{2} }] $ 发散。这意味着 $ Y_{2} $ 的矩生成函数对于任意 $ \theta\neq 0 $ 均不存在。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://diefish1024.github.io/tags/learning/>Learning</a></li><li><a href=https://diefish1024.github.io/tags/math/>Math</a></li><li><a href=https://diefish1024.github.io/tags/homework/>Homework</a></li><li><a href=https://diefish1024.github.io/tags/probability-theory/>Probability-Theory</a></li></ul><nav class=paginav><a class=prev href=https://diefish1024.github.io/posts/class-notes/cs0901-combinatorics/cs0901-hw8/><span class=title>« Prev</span><br><span>CS0901 HW8</span>
</a><a class=next href=https://diefish1024.github.io/posts/class-notes/math1205h-linear-algebra/math1205h-hw16/><span class=title>Next »</span><br><span>MATH1205H HW16</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on x" href="https://x.com/intent/tweet/?text=MATH2701%20HW5&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f&amp;hashtags=learning%2cmath%2chomework%2cprobability-theory"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f&amp;title=MATH2701%20HW5&amp;summary=MATH2701%20HW5&amp;source=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f&title=MATH2701%20HW5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on whatsapp" href="https://api.whatsapp.com/send?text=MATH2701%20HW5%20-%20https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on telegram" href="https://telegram.me/share/url?text=MATH2701%20HW5&amp;url=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MATH2701 HW5 on ycombinator" href="https://news.ycombinator.com/submitlink?t=MATH2701%20HW5&u=https%3a%2f%2fdiefish1024.github.io%2fposts%2fclass-notes%2fmath2701-probability-theory%2fmath2701-hw5%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://diefish1024.github.io/>diefish's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>