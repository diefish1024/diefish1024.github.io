[{"content":"同步和条件变量 互斥实现了原子性，但是无法实现确定性，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\n因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的发生顺序（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\n实现同步\n实现 $ A\\to B $：\n1 2 3 4 5 6 7 A; can_proceed = true; (signal) while(!can_proceed); B // B: wait until the condition is satisfied 这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\n最理想的 API 是 wait_until(cond) ，但是过去为了简化设计，变成了\n条件不满足时等待：wait - 直接睡眠等待 条件满足时继续：signal/broadcast - 唤醒所有线程 （小时候的 scratch 编程其实已经有了这样的思想😂）\n在 c++ 代码中我们可以把条件放到 $ \\lambda $ 表达式中：\n1 2 3 4 5 6 7 8 9 10 11 12 std::mutex mtx; std::condition_variable cv; void T_player() { std::unique_lock lk(mtx); cv.wait(lk, []{ return can_proceed; } ); cv.notify_all(); lk.unlock(); } 注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\n使用条件变量解决同步问题 大部分的同步问题都可以用经典的生产者 - 消费者问题归纳：\nProducer 和 Consumer 共享一个缓冲区，其中\nProducer 看到缓冲区有空位就会放入，否则等待 Consumer 看到缓冲区有数据就回去走，否则等待 显然一个对象的生产和消费必须满足 \u0026ldquo;happens-before\u0026rdquo; 的关系\n可以等价成打印匹配的括号，并且嵌套深度有上限（缓冲区的深度）\n处理这样的问题首先要想清楚程序继续执行的条件，比如生产的条件是 $ d0 $ ，然后套入固定的模板代码即可：\n1 2 3 4 5 6 mutex_lock(lk); while (!cond) { // cond can be any calculate cond_wait(\u0026amp;cv, lk); } assert(cond); mutex_lock(lk); 1 2 3 4 mutex_lock(lk); cond = true cond_broadcast(\u0026amp;cv); //⚠️ mutex_unlock(lk); 注意：全局广播 cond_broadcast 不能被替换成单独唤醒一个线程 cond_signal ，在这里显然可能会导致所有进程都被锁住无法触发新的同步变量；并发编程很多看起来正确的地方都需要仔细思考\n遇到任何同步问题的核心都是同步条件是什么，比如括号打印可以拓展成打印 \u0026lt;\u0026gt;\u0026lt; 或者 \u0026gt;\u0026lt;\u0026gt; 两种形状，核心也是画出状态机，找到同步条件，再套入模板就解决了问题\n计算图与并发控制 并行计算的模型可以用一个 DAG 计算图去理解，任务之间存在依赖关系，通过拓扑排序的顺序去解决问题，相互不存在 \u0026ldquo;happens-before\u0026rdquo; 依赖关系的任务都可以并发解决\n为了优化效率，我们对计算任务的分配需要保证每个节点计算的消耗是远大于同步和锁的开销的，因此实际上可能是把很多个小的任务聚合成一个大的并发计算节点，交给一个线程去执行\n实现计算图有两种思路，第一种是朴素的为每个节点设置一个线程和条件变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // The dependency edge is u-\u0026gt;v void T_u() { // calculate u mutex_lock(v-\u0026gt;lock); v-\u0026gt;num_done++; cond_signal(v-\u0026gt;cv); // it\u0026#39;s okay mutex_unlock(v-\u0026gt;lock); } void T_v() { mutex_lock(v-\u0026gt;lock); while (!(v-\u0026gt;num_done == v-\u0026gt;num_predecessors)){ cond_wait(v-\u0026gt;cv, v-\u0026gt;lock); } mutex_unlock(v-\u0026gt;lock); // calculate v } 但是这样实际会产生过多的线程，造成不必要的性能开销（比如产生了多余 CPU 的 core 数量的线程），实际上更优的办法是创建一个任务调度器线程 $ T_{\\text{scheduler}} $ 来专门控制产生 $ T_{\\text{worker}} $ ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 mutex_lock(lk); while (!(all_done || has_job(tid))) { cond_wait(\u0026amp;worker_cv[tid], lk); } mutex_unlock(lk); if (all_done) { break; } else { process_job(tid); } signal(\u0026amp;sched_cv); ","permalink":"https://diefish1024.github.io/posts/nju-os-2025/15-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%90%8C%E6%AD%A5%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8F/","summary":"\u003ch2 id=\"同步和条件变量\"\u003e同步和条件变量\u003c/h2\u003e\n\u003cp\u003e互斥实现了\u003cstrong\u003e原子性\u003c/strong\u003e，但是无法实现\u003cstrong\u003e确定性\u003c/strong\u003e，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\u003c/p\u003e\n\u003cp\u003e因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的\u003cstrong\u003e发生顺序\u003c/strong\u003e（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e实现同步\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e实现 $ A\\to B $：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003esignal\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e//\u003c/span\u003e \u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ewait\u003c/span\u003e \u003cspan class=\"n\"\u003euntil\u003c/span\u003e \u003cspan class=\"n\"\u003ethe\u003c/span\u003e \u003cspan class=\"n\"\u003econdition\u003c/span\u003e \u003cspan class=\"n\"\u003eis\u003c/span\u003e \u003cspan class=\"n\"\u003esatisfied\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\u003c/p\u003e\n\u003cp\u003e最理想的 API 是 \u003ccode\u003ewait_until(cond)\u003c/code\u003e ，但是过去为了简化设计，变成了\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e条件不满足时等待：\u003ccode\u003ewait\u003c/code\u003e - 直接睡眠等待\u003c/li\u003e\n\u003cli\u003e条件满足时继续：\u003ccode\u003esignal/broadcast\u003c/code\u003e - 唤醒所有线程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（小时候的 scratch 编程其实已经有了这样的思想😂）\u003c/p\u003e\n\u003cp\u003e在 c++ 代码中我们可以把条件放到 $ \\lambda $ 表达式中：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c++\" data-lang=\"c++\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003emutex\u003c/span\u003e \u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003econdition_variable\u003c/span\u003e \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_player\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eunique_lock\u003c/span\u003e \u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewait\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e[]{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enotify_all\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eunlock\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\u003c/p\u003e","title":"15. 并发控制：同步条件变量"},{"content":"信号量 互斥锁在某种意义上也可以认为实现了 \u0026ldquo;happens-before\u0026rdquo; 的依赖关系—— release 必然发生在 acquire 之前。我们可以试着利用这种依赖关系来实现计算图的调度：为每条边分配一个互斥锁，代表数据或前置任务的完成；一个节点必须获得所有入边对应的互斥锁才能开始计算，计算完成后，就释放所有出边对应的互斥锁，通知下游节点输出就绪（但是这种直接使用互斥锁作为边状态信号的方式是 undefined behavior，因为互斥锁主要用于保护临界区，其释放通常要求由持有它的线程完成，若释放未曾获取的锁，则行为未定义）\n我们可以从这种想法中抽象出其本质，也就是用一个“信号”去获取资源的许可，类似餐厅的取号吃饭\n这种信号的思想很适合用来管理计数类型的同类资源，比如停车场的空位，为了实现这种 producer-customer 的问题，用 条件变量 可以轻易解决，进入的条件就是存在空位 count \u0026lt; capacity ，那我们从减少变量的角度出发，这实际上也就是剩余空位的数量大于零，我们停车相当于消耗了一个车位，离开相当于创造了一个车位，这也就得到了所谓“信号量”的机制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void P(sem_t *sem) { // Prolaag - try + decrease/down/wait/acquire mutex_lock(\u0026amp;sem-\u0026gt;lk); while (!(sem-\u0026gt;count \u0026gt; 0)) { cond_wait(\u0026amp;sem-\u0026gt;cv, \u0026amp;sem-\u0026gt;lk); } sem-\u0026gt;count--; // 消耗一个信号 (车位) mutex_unlock(\u0026amp;sem-\u0026gt;lk); } void V(sem_t *sem) { // Verhoog - increase/up/post/signal/release mutex_lock(\u0026amp;sem-\u0026gt;lk); sem-\u0026gt;count++; // 创建一个信号 (车位) cond_broadcast(\u0026amp;sem-\u0026gt;cv); mutex_unlock(\u0026amp;sem-\u0026gt;lk); } 根据这个一路推出信号量的思路，或许可以认为这是互斥锁的扩展\n信号量：应用 信号量有两种典型的应用：\n实现一个临时的 happens-before：$ A\\to V(s)\\to P(s)\\to B $ 管理计数资源：停车场、餐厅…… 可以利用信号量优雅地实现 producer-customer 的模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sem_t empty = SEM_INIT(depth); sem_t fill = SEM_INIT(0); void T_produce() { P(\u0026amp;empty); printf(\u0026#34;(\u0026#34;); V(\u0026amp;fill); } void T_consume() { P(\u0026amp;fill); printf(\u0026#34;)\u0026#34;); V(\u0026amp;empty); } 信号量、条件变量与同步 信号量对比条件变量：\n信号量：更加干净优雅，但是不一定能很好地表示同步条件（用更 hack 的方式解决问题） 条件变量：更加万能，但是代码比较丑陋（用更标准化的方式解决问题） 尝试用信号量解决更复杂的同步问题：哲学家吃饭，一张圆桌围坐 $ n $ 个哲学家，每两个人之间有一把叉子（筷子更加合适？？），每个哲学家（线程）有时思考有时吃饭，思考时什么也不用做，吃饭时同时需要左手右手的叉子\n用条件变量解决这个问题只需要无脑设置同步条件即可，用信号量解决这个问题有一个初步的想法是 P(\u0026amp;sem[lhs]) \u0026amp;\u0026amp; P(\u0026amp;sem[rhs]) ，乍一看没什么问题，但是实际上这个条件在所有人都同时举起了同一边的叉子时会陷入死锁（所以并发编程一定要仔细再仔细！），所以为了排除这种方案，有两种解决方案：\n从桌子赶走一个人：为上桌吃饭人数设置一个信号量，限制不让所有人同时上桌即可（显然不可能所有人同时吃上饭） 为叉子编号，总是先拿起编号小的一把（最后一个人的顺序会和其他人反过来） 但是这样的解决方案是不够优雅不够通用的，因此更多时候条件变量是一个更好的选择，可以总结为信号量在适合的适合很好用，但不总是很好用\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/16-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%90%8C%E6%AD%A5%E4%BF%A1%E5%8F%B7%E9%87%8F/","summary":"\u003ch2 id=\"信号量\"\u003e信号量\u003c/h2\u003e\n\u003cp\u003e互斥锁在某种意义上也可以认为实现了 \u0026ldquo;happens-before\u0026rdquo; 的依赖关系—— release 必然发生在 acquire 之前。我们可以试着利用这种依赖关系来实现计算图的调度：为每条边分配一个互斥锁，代表数据或前置任务的完成；一个节点必须获得所有入边对应的互斥锁才能开始计算，计算完成后，就释放所有出边对应的互斥锁，通知下游节点输出就绪（但是这种直接使用互斥锁作为边状态信号的方式是 undefined behavior，因为互斥锁主要用于保护临界区，其释放通常要求由持有它的线程完成，若释放未曾获取的锁，则行为未定义）\u003c/p\u003e\n\u003cp\u003e我们可以从这种想法中抽象出其本质，也就是用一个“\u003cstrong\u003e信号\u003c/strong\u003e”去获取资源的许可，类似餐厅的取号吃饭\u003c/p\u003e\n\u003cp\u003e这种\u003cstrong\u003e信号\u003c/strong\u003e的思想很适合用来管理\u003cstrong\u003e计数类型的同类资源\u003c/strong\u003e，比如停车场的空位，为了实现这种 producer-customer 的问题，用 \u003ca href=\"15.%20%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%EF%BC%9A%E5%90%8C%E6%AD%A5%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8F.md\"\u003e条件变量\u003c/a\u003e 可以轻易解决，进入的条件就是存在空位 \u003ccode\u003ecount \u0026lt; capacity\u003c/code\u003e ，那我们从减少变量的角度出发，这实际上也就是剩余空位的数量大于零，我们停车相当于\u003cstrong\u003e消耗\u003c/strong\u003e了一个车位，离开相当于\u003cstrong\u003e创造\u003c/strong\u003e了一个车位，这也就得到了所谓“\u003cstrong\u003e信号量\u003c/strong\u003e”的机制\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eP\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003esem_t\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Prolaag - try + decrease/down/wait/acquire\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nf\"\u003econd_wait\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// 消耗一个信号 (车位)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003esem_t\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Verhoog - increase/up/post/signal/release\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// 创建一个信号 (车位)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003econd_broadcast\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nf\"\u003emutex_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e根据这个一路推出信号量的思路，或许可以认为这是互斥锁的扩展\u003c/p\u003e","title":"16. 并发控制：同步信号量"},{"content":"数据竞争 大多并发 bug 最后都会体现为数据竞争 (Data Race)\n对于顺序程序而言，函数 f() 返回之后就已经完成了所有的状态修改，对于其他部分而言这个修改是立即生效的；如果对于并发程序而言模式的切换也在瞬间完成，那就不会导致并发的问题\n然而实际上模式的切换需要时间，执行的操作在未来一段时间之后才会就绪，但是我们在实际编程时总是容易有“立即生效”的肌肉记忆，这就导致了并发问题的可能性\n不过对于函数式编程而言，操作不存在对外状态的修改，没有副作用（只会操作局部变量），这就不会导致并发问题\nData Race 发生的实质是不同的线程同时访问同一内存，并且至少有一个是写，形象的理解就是不同的内存访问在“赛跑”，跑赢的操作先执行\nNot that easy: 虽然我们将数据竞争形象地比喻为“赛跑”，但实际上，哪一个操作能“跑赢”并没有想象中那么简单和确定，其复杂性主要体现在以下几个方面\n弱内存模型 (Weak memory model)：在现代处理器架构中，为了提升性能，处理器可能会对内存操作进行重排序。这意味着，不同的线程或“观察者”在不同时间点看到共享内存的状态可能是不一致的。一个线程对内存的写入操作，可能不会立即对所有其他线程可见，导致不同线程观察到不同的结果。这种内存模型的一致性问题使得确定哪个操作“先发生”变得非常困难且不确定。\n未定义行为 (Undefined Behavior)：从 C++11 标准开始，数据竞争被明确规定为未定义行为。这意味着，如果你的程序发生了数据竞争，编译器可以自由地产生任何行为，无论是崩溃、产生错误结果，还是看似正常运行但结果不可预测。这使得数据竞争成为非常危险且难以调试的并发错误，因为它的表现可能是不确定、不稳定的。\n多线程与多内存的复杂交互：在实际的并发程序中，通常会有多个线程同时访问多个共享内存位置。这些线程和内存之间存在复杂的读（R）写（W）交互。一个线程对一个内存位置的写入可能影响到其他多个线程对该位置的读取，同时，多个内存位置之间也可能存在复杂的依赖关系和缓存一致性问题。这种错综复杂的交互网络进一步加剧了数据竞争的不可预测性。\n为了消灭数据竞争，我们需要保证程序的 serializability ，可能竞争的内存访问要么互斥，要么同步\n实际编程中遇到的数据竞争 bug 大多属于上错了锁和忘记上锁两种情况的变种\nCase 1: 上错了锁\n1 2 void T_1() { spin_lock(\u0026amp;A); sum++; spin_unlock(\u0026amp;A); } void T_2() { spin_lock(\u0026amp;B); sum++; spin_unlock(\u0026amp;B); } Case 2: 忘记上锁\n1 2 void T_1() { spin_lock(\u0026amp;A); sum++; spin_unlock(\u0026amp;A); } void T_2() { sum++; } 但是实际系统面临的情况比这复杂的多，因为\n内存可以是地址空间的任何内存，比如全局变量、堆内存分配的变量、程序的栈…… 访问可以发生在任何代码，比如自己的代码、框架代码、一行没读到的汇编指令、某条 ret 指令 “一行没读到的汇编指令”造成的访问的情况有编译器优化造成的指令重排、硬件层面弱内存模型的内存访问重排、还有一些高层语言操作的隐式内存访问 实际系统中虽然难以避免，但是会尽可能保证底层的结构对上层尽可能封闭来防止这种错误 死锁 死锁 (Deadlock) 是指一个群体中的每个成员都在等待其他成员（包括自身）采取行动的状态\n死锁有两种：\nAA-Deadlock: 自己等待自己\n1 2 3 4 5 6 7 lock(\u0026amp;lk); // lk-\u0026gt;locked == ✅; proceed ... // Possibly in interrupt handler lock(\u0026amp;lk); // while (lk-\u0026gt;locked == ❌) ; 这样的错误虽然看起来很傻，但是在真实程序复杂的控制流中是可能出现的\nABBA-Deadlock: 两个（多个）锁互相等待\n比如 16. 并发控制：同步信号量 中的哲学家吃饭问题\nHow? 想要消除死锁，可以从死锁产生的必要条件入手，通常称为霍尔德（Holt）条件， 可以形象地把锁看成袋子里的球：\n互斥 (Mutual-exclusion)： 一个口袋一个球，即资源是互斥的，一次只能被一个线程占用 如果资源可以共享（例如，只读文件），则不会发生死锁 请求并保持 (Wait-for)： 得到球的人想要更多的球，即一个线程在持有至少一个资源的同时，又在等待获取其他被占用的资源 如果线程在请求新资源时，必须释放所有已持有的资源，则可以避免死锁 不可抢占 (No-preemption)： 不能抢别人的持有的球，即资源不能被强制从持有者手中抢走，只能由持有者自愿释放 如果系统可以抢占资源（例如，通过中断强制释放），则可以打破此条件 循环等待 (Circular-chain)： 形成循环等待的关系，即存在一个线程链 $ T_1, T_2, \\ldots, T_n $，其中 $ T_1 $ 正在等待 $ T_2 $ 占用的资源，$ T_2 $ 正在等待 $ T_3 $ 占用的资源，依此类推，$ T_n $ 正在等待 $ T_1 $ 占用的资源 这四个条件是死锁发生的必要条件，这意味着只要打破任何一个条件，就不会发生死锁了，理论上，我们可以通过破坏这些条件来预防或避免死锁。\n然而，将这套理论应用于实际复杂系统时，会发现它是一个正确的废话，不能称为一个合理的 argument\n对于玩具系统/模型：可以直接证明系统是 deadlock-free 的，因为其状态空间有限，可以通过穷举或形式化方法进行验证 对于真正的复杂系统：很难判断哪个条件最容易被打破，或者说，在保证系统功能性和性能的前提下，打破某个条件可能带来巨大的复杂性或性能开销 实际编程中通常会采用其他策略来预防死锁：\n一个常见的方法是锁排序，其核心思想是：\n任意时刻系统中的锁都是有限的。 给所有锁编号（或者定义一个全局的获取顺序） 线程在获取多个锁时，严格按照从小数到大的顺序获取锁 这种策略也相对容易检查和验证。 这样在任意时刻总有一个线程获得“编号最大”的锁，于是这个线程总是可以继续运行\n然而这种方法在实践中会遇到问题，代码的文档并不总是可靠，并且对于复杂系统这是难以扩展的；而最好的锁是封装的，并不会暴露出来，这样使用代码的人甚至不需要知道正在使用锁\nSome Methods 为了应对死锁，尤其是 ABBA-Deadlock，一些工具被开发出来，例如 Linux 内核中的 LockDep\n一个简单的想法：\n每次 acquire/release 锁时，都打印一个日志 如果任何线程存在 $ A\\to B $ 和 $ B\\to A $ 的依赖关系，就报告死锁 这可能导致 false positives ，比如存在同步机制 ($ A\\to B\\to \\mathrm{spawn}\\to B\\to A $) 通过优化这个方法可以得到一个相对高效的实现：动态维护“锁依赖图“并检测环路\n原子性和顺序违反 并发编程是本质困难的，我们只能用 sequential 的方式来理解并发：把程序分成若干的块，每一块的操作都是原子的，无法被打断\n并发的机制可以分成两类：\n互斥锁实现原子性 忘记上锁会导致原子性违反 (Atomicity Violation, AV) 条件变量/信号量实现先后顺序同步 忘记同步会导致顺序违反 (Order Violation, OV) 并发的机制完全是“后果自负”的，这也导致了 Threads cannot be implemented as a library ，因为？\n有研究统计了很多真是系统存在的并发 bug，发现 97% 的非死锁并发 bug 都是原子性或顺序错误\nAtomic Violation 代码被别的线程“强行插入”，即使分别上锁消除了数据竞争，还是会导致 AV\n比如图中的例子：\n如果在 Thread 1 结束判断进入 if 之后 Thread 2 再执行，就导致了错误\n并且注意到操作系统的状态也是共享状态，利用一样的原理还可能产生更难发现的 bug\n攻击者在 check 之后马上替换文件为符号链接，就可以造成权限问题等严重安全漏洞\nOrder Violation 事件没有按照预定的顺序发生，就会导致 OV ，比如 如果 Thread 2 中的 S4 发生在了条件变量的初始化之前，那么相当于全局的广播被吞掉了，就可能会导致 Thread 1 可能无法被唤醒\nHowever 我们可以使用加强版的 LockDep 来解决这些问题，比如直接分析程序的日志，检查有没有不相交的锁，事件的 happens-before 关系是否正确……甚至也可以直接用启发式（或者 LLM ）来分析日志是否正确\n然而即使这样也不能解决真实世界的所有并发问题，比如这个 GhostRace 的例子\n实现了正确的互斥、正确的同步 $ \\text{use}\\to\\text{free} $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 void T_A() { lock(\u0026amp;obj-\u0026gt;lock); free(obj-\u0026gt;something); obj-\u0026gt;something = NULL; unlock(\u0026amp;obj-\u0026gt;lock); } void T_B() { lock(\u0026amp;obj-\u0026gt;lock); if (obj-\u0026gt;something) { // changes cache on speculative execution } unlock(\u0026amp;obj-\u0026gt;lock); } 实际上导致问题的是现代 CPU 的推测执行的特性，为了提高效率会提前通过分治预测器预判执行的方向，提前执行指令\n这段代码中线程 $ T_{B} $ 的 CPU 会在正式获得锁并判断 NULL 之前就提前推测性执行 if 块内部的代码，如果此时 $ T_{A} $ 恰好释放了 obj-something 指向的内存，那么 $ T_{B} $ 中就会访问到一块已经被释放的内存，从而通过缓存等方式泄露信息（虽然错误执行的语句已经回滚了，但是被加载到缓存中的数据等不会回滚），通过这种方式就有可能访问到程序本来没有权限访问的内存，从而产生安全漏洞\n这种并发 bug 的根源在于软件层面的同步无法约束硬件层面的行为\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/17-%E5%B9%B6%E5%8F%91-bugs/","summary":"\u003ch2 id=\"数据竞争\"\u003e数据竞争\u003c/h2\u003e\n\u003cp\u003e大多并发 bug 最后都会体现为\u003cstrong\u003e数据竞争 (Data Race)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e对于顺序程序而言，函数 \u003ccode\u003ef()\u003c/code\u003e 返回之后就已经完成了所有的状态修改，对于其他部分而言这个修改是立即生效的；如果对于并发程序而言模式的切换也在瞬间完成，那就不会导致并发的问题\u003c/p\u003e\n\u003cp\u003e然而实际上模式的切换需要时间，执行的操作在未来一段时间之后才会就绪，但是我们在实际编程时总是容易有“立即生效”的肌肉记忆，这就导致了并发问题的可能性\u003c/p\u003e\n\u003cp\u003e不过对于\u003cstrong\u003e函数式编程\u003c/strong\u003e而言，操作不存在对外状态的修改，没有副作用（只会操作局部变量），这就不会导致并发问题\u003c/p\u003e\n\u003cp\u003eData Race 发生的实质是\u003cstrong\u003e不同的线程\u003c/strong\u003e同时访问\u003cstrong\u003e同一内存\u003c/strong\u003e，并且\u003cstrong\u003e至少有一个是写\u003c/strong\u003e，形象的理解就是不同的内存访问在“赛跑”，跑赢的操作先执行\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNot that easy\u003c/strong\u003e: 虽然我们将数据竞争形象地比喻为“赛跑”，但实际上，哪一个操作能“跑赢”并没有想象中那么简单和确定，其复杂性主要体现在以下几个方面\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e弱内存模型 (Weak memory model)\u003c/strong\u003e：在现代处理器架构中，为了提升性能，处理器可能会对内存操作进行重排序。这意味着，不同的线程或“观察者”在不同时间点看到共享内存的状态可能是不一致的。一个线程对内存的写入操作，可能不会立即对所有其他线程可见，导致不同线程观察到不同的结果。这种\u003cstrong\u003e内存模型的一致性问题\u003c/strong\u003e使得确定哪个操作“先发生”变得非常困难且不确定。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e未定义行为 (Undefined Behavior)\u003c/strong\u003e：从 C++11 标准开始，数据竞争被明确规定为\u003cstrong\u003e未定义行为\u003c/strong\u003e。这意味着，如果你的程序发生了数据竞争，编译器可以自由地产生任何行为，无论是崩溃、产生错误结果，还是看似正常运行但结果不可预测。这使得数据竞争成为非常危险且难以调试的并发错误，因为它的表现可能是不确定、不稳定的。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e多线程与多内存的复杂交互\u003c/strong\u003e：在实际的并发程序中，通常会有多个线程同时访问多个共享内存位置。这些线程和内存之间存在复杂的读（R）写（W）交互。一个线程对一个内存位置的写入可能影响到其他多个线程对该位置的读取，同时，多个内存位置之间也可能存在复杂的依赖关系和缓存一致性问题。这种错综复杂的交互网络进一步加剧了数据竞争的不可预测性。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e为了消灭数据竞争，我们需要保证程序的 serializability ，\u003cstrong\u003e可能竞争的内存访问要么互斥，要么同步\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e实际编程中遇到的数据竞争 bug 大多属于\u003cstrong\u003e上错了锁\u003c/strong\u003e和\u003cstrong\u003e忘记上锁\u003c/strong\u003e两种情况的变种\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCase 1: 上错了锁\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_1\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_2\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eCase 2: 忘记上锁\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_1\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_2\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e但是实际系统面临的情况比这复杂的多，因为\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e内存可以是地址空间的任何内存，比如全局变量、堆内存分配的变量、程序的栈……\u003c/li\u003e\n\u003cli\u003e访问可以发生在任何代码，比如自己的代码、框架代码、一行没读到的汇编指令、某条 ret 指令\n\u003cul\u003e\n\u003cli\u003e“一行没读到的汇编指令”造成的访问的情况有编译器优化造成的指令重排、硬件层面弱内存模型的内存访问重排、还有一些高层语言操作的隐式内存访问\u003c/li\u003e\n\u003cli\u003e实际系统中虽然难以避免，但是会尽可能保证底层的结构对上层尽可能封闭来防止这种错误\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"死锁\"\u003e死锁\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e死锁 (Deadlock)\u003c/strong\u003e 是指一个群体中的每个成员都在等待其他成员（包括自身）采取行动的状态\u003c/p\u003e\n\u003cp\u003e死锁有两种：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAA-Deadlock\u003c/strong\u003e: 自己等待自己\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nf\"\u003elock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// lk-\u0026gt;locked == ✅; proceed\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Possibly in interrupt handler\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003elock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// while (lk-\u0026gt;locked == ❌) ;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样的错误虽然看起来很傻，但是在真实程序复杂的控制流中是可能出现的\u003c/p\u003e","title":"17. 并发 Bugs"},{"content":"并发编程的核心抽象是实现一个计算图，计算发生在节点上，边表示节点之间的依赖关系，同时计算图在运行时可能是动态变化的\n使用条件变量、锁、信号量等 api 去实现计算图并不是一个优雅的实现方式，因为这样会在代码中引入众多干扰代码，也可能导致一些问题\n为此可以增加一些功能受限的语法，可以在同样描述计算图的功能下减少了许多潜在的问题\n高性能计算中的并行编程 在高性能计算中，计算图通常易于静态切分，尤其适用于物理模拟的网格划分，为此 HPC 发展出多种高效的并行编程模型，具体学习可以参考 SJTU HPC 学习手册\nMPI: 分布式内存并行 每个 MPI 进程有独立的内存空间，进程间通过显式消息传递（发送/接收）交换数据\nOpenMP: 共享内存并行 多个线程在同一地址空间中并行执行，所有线程可以直接访问相同的数据，使用 #pragma omp 指令实现并行化\n对非计算机专业来说非常友好，只需要在正常的代码上面加上编译指令即可，能轻松实现高效的并行优化\nCUDA: GPU 异构并行 CPU 调度，GPU 执行大规模并行计算\n概念：\n核函数 (Kernel) ：在 GPU 上并行执行的函数 线程层次：线程 (threadIdx) 组成线程块 (blockIdx)，线程块组成网格 (gridDim) 内存层次：寄存器、共享内存（块内高速）、全局内存（所有线程可访问） 我们身边的并发编程 从 Web 1.0 到 Web 2.0 在 Web 时代用的最广泛的是 Javascript\nAsynchronous JavaScript and XML (Ajax; ~1999) 允许网页实现“后台刷新” jQuery $ (2006): A DOM Query Language (编程抽象) Web 2.0 时代的并发编程 线程开销大，并且大多数 Web 开发者难以进行并发编程\n从而有了event-based concurrency (动态计算图) 的机制：禁止任何计算节点并行（和高性能计算的选择不同）\n允许网络请求、sleep 在后台进行（这才是主要开销），执行完之后产生新的计算节点 事件可以在浏览器里看到 直接用这样的方式描述计算图会产生大量屎山代码 (Callback hell) ，现代的选择是动态描述计算图\n数据中心的并发编程 数据中心以海量分布式数据为中心，需要处理的需求有：\n实时的“小数据”处理：内容分发、弹幕…… 离线的“大数据”处理：内容索引、数据挖掘…… 为 AI 提供支持 数据中心里的并发编程需要满足高吞吐量和低延迟的特点，难点在于：\n处理事件可能需要读写持久存储或请求网络上的服务，从而导致延迟不确定 线程维护和上下文切换都会带来开销 在数据中心进行并发编程时，传统的通过手动管理线程、锁等低级并发原语的方式变得越来越复杂且难以扩展，因此今天的解决方案是无服务器计算 (Serverless Computing) ，特别是函数即服务 (Function as a Service, FaaS) ，其优势在于\n开发人员不再需要直接处理底层的并发问题 FaaS 函数通常是短暂的、无状态的，并且每个请求通常触发一个独立的函数实例，不同的函数实例之间通常不共享内存，这从根本上规避了传统多线程编程中由于共享内存问题 这样的范式使得现代存储系统实现高可靠、低延迟的多副本分布式存储和计算变得非常 Challenge ，数据一致性、服务时刻可用和容忍机器离线三个特性不可以兼得 协程 协程是一种程序组件，与线程在概念上有一些相似之处，比如都在进程内部拥有独立的栈帧，并能在运行时维护自己的状态，可以看作是一个共享内存的状态机\n然而，协程与线程最大的不同在于它们的调度方式和上下文切换开销：\n协作式调度：协程的执行是协作式的，会“一直执行” ，直到遇到 yield() 等指令时，才会主动放弃处理器控制权，将执行权交还给调用者或调度器，这与线程的抢占式调度（由操作系统决定何时中断和切换）不同\n操作系统“不感知”的上下文切换：当协程通过 yield() 进行切换时，这仅仅是一个函数调用级别的操作，不需要像线程切换那样，保存和恢复完整的上下文信息，因此这种切换开销非常小，并且是操作系统完全不感知\n阻塞 I/O 的局限性：由于协程是协作式执行的，如果在一个协程内部执行了阻塞性操作（如耗时的 I/O 操作或 sleep()），那么当前所在的整个操作系统线程都会被卡住，这意味着所有在该线程上运行的协程都将停止执行，从而失去了并行\n示例\n1 2 3 4 5 6 7 8 9 10 11 import random def T_worker(name): i = 0 while (i := i + 1): print(f\u0026#39;[{name}] i = {i}\u0026#39;) yield() threads = [T_worker(i) for i in range(1000000)] while True: random.choice(threads).send(None) 这个例子很好地阐释了协程的特点：\n在同一个操作系统线程中执行：虽然创建了非常多个 T_worker 对象，但它们都运行在程序的同一个操作系统线程中，开销远小于创建相同数量个操作系统线程 由程序控制调度：while True: random.choice(threads).send(None) 这段代码是自定义的简单调度器，它随机选择一个协程并激活它，使其执行直到遇到 yield() 暂停，然后再选择下一个协程 资源占用极低：由于不涉及操作系统层面的上下文切换，除了协程本身所需要的一小块内存用于保存其栈帧和状态外，协程不占用额外的操作系统资源，因此可以创建海量的协程实例，非常适合高并发的 I/O 密集型任务。 ","permalink":"https://diefish1024.github.io/posts/nju-os-2025/18-%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-1/","summary":"\u003cp\u003e并发编程的核心抽象是实现一个计算图，计算发生在节点上，边表示节点之间的依赖关系，同时计算图在运行时可能是动态变化的\u003c/p\u003e\n\u003cp\u003e使用条件变量、锁、信号量等 api 去实现计算图并不是一个优雅的实现方式，因为这样会在代码中引入众多\u003cstrong\u003e干扰代码\u003c/strong\u003e，也可能导致一些问题\u003c/p\u003e\n\u003cp\u003e为此可以增加一些功能受限的\u003cstrong\u003e语法\u003c/strong\u003e，可以在同样描述计算图的功能下减少了许多潜在的问题\u003c/p\u003e\n\u003ch2 id=\"高性能计算中的并行编程\"\u003e高性能计算中的并行编程\u003c/h2\u003e\n\u003cp\u003e在高性能计算中，计算图通常易于\u003cstrong\u003e静态切分\u003c/strong\u003e，尤其适用于物理模拟的网格划分，为此 HPC 发展出多种高效的并行编程模型，具体学习可以参考 \u003ca href=\"https://xflops.sjtu.edu.cn/hpc-start-guide/parallel-computing/basic/\"\u003eSJTU HPC 学习手册\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"mpi-分布式内存并行\"\u003eMPI: 分布式内存并行\u003c/h4\u003e\n\u003cp\u003e每个 MPI 进程有独立的内存空间，进程间通过\u003cstrong\u003e显式消息传递\u003c/strong\u003e（发送/接收）交换数据\u003c/p\u003e\n\u003ch4 id=\"openmp-共享内存并行\"\u003eOpenMP: 共享内存并行\u003c/h4\u003e\n\u003cp\u003e多个线程在同一地址空间中并行执行，所有线程可以直接访问相同的数据，使用 \u003ccode\u003e#pragma omp\u003c/code\u003e 指令实现并行化\u003c/p\u003e\n\u003cp\u003e对非计算机专业来说非常友好，只需要在正常的代码上面加上编译指令即可，能轻松实现高效的并行优化\u003c/p\u003e\n\u003ch4 id=\"cuda-gpu-异构并行\"\u003eCUDA: GPU 异构并行\u003c/h4\u003e\n\u003cp\u003eCPU 调度，GPU 执行大规模并行计算\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e概念\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e核函数 (Kernel)\u003c/strong\u003e ：在 GPU 上并行执行的函数\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e线程层次\u003c/strong\u003e：线程 (\u003ccode\u003ethreadIdx\u003c/code\u003e) 组成线程块 (\u003ccode\u003eblockIdx\u003c/code\u003e)，线程块组成网格 (\u003ccode\u003egridDim\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e内存层次\u003c/strong\u003e：寄存器、共享内存（块内高速）、全局内存（所有线程可访问）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"我们身边的并发编程\"\u003e我们身边的并发编程\u003c/h2\u003e\n\u003ch4 id=\"从-web-10-到-web-20\"\u003e从 Web 1.0 到 Web 2.0\u003c/h4\u003e\n\u003cp\u003e在 Web 时代用的最广泛的是 Javascript\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAsynchronous JavaScript and XML\u003c/strong\u003e (Ajax; ~1999)\n\u003cul\u003e\n\u003cli\u003e允许网页实现“后台刷新”\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ejQuery $ (2006): A DOM Query Language\u003c/strong\u003e (编程抽象)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"web-20-时代的并发编程\"\u003eWeb 2.0 时代的并发编程\u003c/h4\u003e\n\u003cp\u003e线程开销大，并且大多数 Web 开发者难以进行并发编程\u003c/p\u003e","title":"18. 真实世界的并发编程 (1)"},{"content":"CPU 内的并行编程 CPU 的功耗 $ P=C\\cdot V^{2}\\cdot f $ 导致纵使有更大的电路，热功耗限制了性能上限，从而有一堵“功耗墙”限制了 CPU 的性能，为此需要考虑如何在降低 $ V $ 和 $ f $ 的同时用面积换性能\n有两个思路：\n让一条指令能处理更多的数据：SIMD (Single Instruction, Multiple Data)\n“一条指令” 浪费的能量大致是定数 处理的数据越多，浪费越少 用更多更简单的处理器：多处理器系统、异构多处理器\n同等面积，处理器越简单，数量越多 异构计算：最经典的例子是大小核架构（如 Apple M1） SIMD SIMD 的核心思想是在硬件层面实现数据级并行，它通过引入专门的硬件单元来达成这个目标：\n宽位寄存器 (Wide Registers)：CPU 内部增加了比通用寄存器宽很多的专用寄存器\nIntel 的 SSE 指令集引入了 128 位的 XMM 寄存器，而最新的 AVX-512 拥有 512 位的 ZMM 寄存器 这些宽位寄存器可以一次性装入多个数据元素，比如一个 128 位的寄存器可以同时容纳 4 个 32 位的浮点数，或者 16 个 8 位的整数 这些被打包在一起的数据被称为 Vector 向量处理单元 (Vector ALU)：CPU 内部也配备了能够对整个向量进行并行计算的 ALU\n当执行一条 SIMD 指令时，这个特殊的 ALU 会在同一个时钟周期内，同时对寄存器中的 4 对浮点数分别执行操作 例子：假设我们要计算两个数组 A 和 B 的和，存入数组 C，每个数组都有 4 个元素。\nTraditional：\nload A[0] load B[0] add -\u0026gt; store C[0] load A[1] load B[1] add -\u0026gt; store C[1] \u0026hellip; 重复 4 次，需要执行 4 轮独立的“load-compute-store”指令 SIMD：\nLOAD：将数组 A 的 4 个元素一次性加载到一个 128 位的 XMM 寄存器中 LOAD：将数组 B 的 4 个元素加载到另一个 XMM 寄存器中 VADDPS：CPU 的向量处理单元对这两个寄存器中的 4 对元素 同时 进行加法运算 STORE：将计算结果寄存器中的 4 个和值一次性存回内存中的数组 C 通过这种方式，原来需要循环多次的计算被压缩成了几条高效的向量指令，极大地提升了吞吐率，尤其是在图像处理、视频编解码、科学计算和人工智能等需要大量重复性计算的领域，效果非常显著\nGPU 和 GPGPU 第二种克服“功耗墙”的思路——使用更多、更简单的处理器——直接催生了 GPU (Graphics Processing Unit) 的发展，它最初为图形渲染这种高度并行的任务而设计，其架构被证明非常有效，最终演变成了一个通用计算的强大引擎\n从 PPU 到 GPU 对专用图形硬件的需求在早期游戏机中就很明显：CPU 在渲染方面效率极低，计算屏幕上每个像素的颜色，每秒重复 60 次，这种“大规模并行”任务会完全压垮为串行任务设计的 CPU\n早期方案 - PPU：早期系统将图形任务交给 PPU (Picture Processing Unit) 处理，这是一种领域专用硬件，它操作的是高级图形对象，如 tiles (8x8 像素块) 和 sprites (可移动的前景角色)，而非单个像素，CPU 的工作被简化为告诉 PPU 该画 哪个 图块以及画在 哪里\n固定功能 Pipeline：随着 3D 图形的出现，简单的 PPU 模型演变为更复杂但仍然固化的 “固定功能管线” (Fixed-Function Pipeline)，它有一系列硬件实现的固定阶段，虽然功能强大，但除了调整预设参数外，几乎没有创造性空间\n随后开发者为了自定义视觉效果，把图形管线中的关键部分被替换为可编程单元，发展出了可编程的特性\n这种可编程性赋予了开发者前所未有的控制力，也标志着现代 GPU 的诞生\nGPGPU 人们很快意识到，一个为像素并行执行数百万个简单程序的芯片，用途远不止于图形\n早期的 hack ：最初的 GPGPU (General-Purpose computing on GPU) 是开发者将一个科学问题（如矩阵乘法）伪装成一个图形任务，例如将矩阵作为“纹理”加载，然后编写一个“片元着色器”来进行乘法计算，最后将结果“颜色”写出 演变为计算平台：这种强大但繁琐的方法证明了其可行性，硬件厂商（比如 Nvidia）随即推出了专用的编程框架（CUDA）和开放标准（OpenCL），这些平台彻底剥离了图形接口，将 GPU 强大的并行计算能力直接暴露给开发者 AI 时代的并行编程 随着 GPGPU 平台的成熟，可编程的 Shader 模型也演变成了更通用的线程模型，最终在 AI 时代大放异彩\nSIMT：单指令，多线程 CUDA 编程模型的核心是 SIMT (Single Instruction, Multiple Threads)，这是对 SIMD 思想的扩展\nGPU 程序中每个像素执行一次的“着色器”，在 CUDA 被看作是一个 Kernel 函数，会被成千上万甚至数百万个 线程 并行执行\nSIMT 的魔法在于，GPU 会将线程分组（通常 32 个线程为一个 Warp），一个 Warp 内的所有线程共享同一个程序计数器 (PC)，在硬件层面，它们在同一时刻执行完全相同的指令\n这实际上创造了一种类似巨型 SIMD 的效果，每个线程虽然有自己独立的寄存器和数据（通过 threadIdx 等内置变量区分），但它们的执行流被捆绑在一起，这使得控制单元的设计可以极其简化，从而在芯片上集成海量的计算核心\nChallenges SIMT 架构在带来巨大并行优势的同时，也给带来了挑战：\n内存合并 (Memory Coalescing)：当一个 Warp 中的多个线程连续访问内存时，GPU 硬件能将这 32 个独立的访问请求合并成一笔或几笔大的内存事务，这是 CUDA 性能优化的关键\n分支发散 (Branch Divergence)：SIMT 最大的难点在于处理分支，如果一个 Warp 内的线程在 if-else 语句上做出不同选择，由于它们共享同一个 PC，硬件必须串行地执行 if 路径和 else 路径，并在执行每个路径时，将另一部分线程暂时屏蔽，这会使 Warp 的执行速度取决于内部执行最慢的线程\n共享内存 (Shared Memory)：虽然 CUDA 线程可以访问共享内存，但如何避免访问冲突 (Bank Conflict)，如何组织数据以最大化并行加载，都会增加 CUDA 程序的编写难度\n尽管编写高效的 CUDA 程序充满挑战，但其回报是巨大的，尤其对于那些计算密集、模式固定的任务，例如深度学习中的矩阵乘法和卷积运算，GPU 能提供比 CPU 高出数个数量级的性能和能效比\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/19-%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-2/","summary":"\u003ch2 id=\"cpu-内的并行编程\"\u003eCPU 内的并行编程\u003c/h2\u003e\n\u003cp\u003eCPU 的功耗 $ P=C\\cdot V^{2}\\cdot f $ 导致纵使有更大的电路，热功耗限制了性能上限，从而有一堵“功耗墙”限制了 CPU 的性能，为此需要考虑如何在降低 $ V $ 和 $ f $ 的同时用面积换性能\u003c/p\u003e\n\u003cp\u003e有两个思路：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e让一条指令能处理更多的数据\u003c/strong\u003e：SIMD (Single Instruction, Multiple Data)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e“一条指令” 浪费的能量大致是定数\u003c/li\u003e\n\u003cli\u003e处理的数据越多，浪费越少\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e用更多更简单的处理器\u003c/strong\u003e：多处理器系统、异构多处理器\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e同等面积，处理器越简单，数量越多\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e异构计算\u003c/strong\u003e：最经典的例子是\u003cstrong\u003e大小核架构\u003c/strong\u003e（如 Apple M1）\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"simd\"\u003eSIMD\u003c/h3\u003e\n\u003cp\u003eSIMD 的核心思想是在硬件层面实现数据级并行，它通过引入专门的硬件单元来达成这个目标：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e宽位寄存器 (Wide Registers)\u003c/strong\u003e：CPU 内部增加了比通用寄存器宽很多的专用寄存器\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIntel 的 SSE 指令集引入了 128 位的 XMM 寄存器，而最新的 AVX-512 拥有 512 位的 ZMM 寄存器\u003c/li\u003e\n\u003cli\u003e这些宽位寄存器可以一次性装入多个数据元素，比如一个 128 位的寄存器可以同时容纳 4 个 32 位的浮点数，或者 16 个 8 位的整数\u003c/li\u003e\n\u003cli\u003e这些被打包在一起的数据被称为 Vector\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e向量处理单元 (Vector ALU)\u003c/strong\u003e：CPU 内部也配备了能够对整个向量进行并行计算的 ALU\u003c/p\u003e","title":"19. 真实世界的并发编程 (2)"},{"content":"输入输出设备 Everything is a File 在 Unix-like 系统中，与外部设备交互的核心思想是 Everything is a File\n文件描述符 (File Descriptor)：操作系统为上层软件提供了一个统一的抽象，即文件描述符，它是一个指向内核中任何 I/O 对象的“指针”或句柄\n统一接口：无论是普通文件、硬件设备（如终端、磁盘）、还是网络连接，都可以通过 open 获得一个文件描述符，然后使用相同的 read/write 等系统调用来进行操作，这极大地简化了应用程序的编写\n设备控制器与 MMIO “文件”这个美好的抽象背后，是具体的硬件工作原理\n设备控制器 (Device Controller)：每个 I/O 设备都有一个控制器，它是一个包含 CPU、内存和寄存器的微型计算机，作为 CPU 和物理设备之间的桥梁\n设备寄存器：控制器通过一组寄存器与 CPU 通信，通常包括：\n状态寄存器：用于表示设备当前是否繁忙、是否准备好等 指令寄存器：CPU 写入指令，告诉设备要做什么 数据寄存器：用于在 CPU 和设备之间传输数据 内存映射 I/O (MMIO)：为了让 CPU 能访问这些寄存器，现代系统普遍采用 MMIO (Memory-Mapped I/O)，操作系统会将设备的寄存器映射到物理内存地址空间中的特定区域，这样一来，CPU 就可以像访问普通内存一样，使用标准的 load/store 指令来读写设备寄存器，从而实现对设备的控制\nGPIO GPIO (General-Purpose Input/Output) 是理解 I/O 设备原理最直观的例子，GPIO 就是一个物理引脚，可以通过编程设置为输入或输出模式\n通过 MMIO，一个 GPIO 引脚的电平状态被映射到一个特定的内存地址，当 CPU 向这个地址写入 1 时，引脚就变为高电平；写入 0 时，则变为低电平，这个过程将一条内存写指令直接转化为了一个物理世界的动作（比如点亮一个 LED）\n输入输出设备案例 串口与键盘 经典的 I/O 设备，展示了最基础的设备交互方式\n端口 I/O：它们通常使用端口 I/O (Port I/O) 与 CPU 通信，设备寄存器被映射到专用的 I/O 端口地址（而非内存地址），CPU 需要使用特殊的 in/out 指令来读写这些端口\n向指定端口写入不同的数值，相当于向设备发送不同的指令（如设置波特率、控制键盘 LED 灯），而从指定端口读取数据则是接收设备的状态或输入（如串口收到的字符、键盘按键的扫描码）\n磁盘控制器与 PIO 早期的磁盘控制器（如 ATA/IDE）展示了一种更复杂但效率较低的交互模式：PIO 协议：全称为Programmed I/O，在这种模式下，数据的传输完全由 CPU 控制\n工作流程：\nCPU 向磁盘控制器的指令寄存器写入命令（如“读取第 N 个扇区”） CPU 进入轮询 (Polling) 状态，反复读取状态寄存器，直到设备报告“数据准备就绪” CPU 在一个循环中，逐个字节或字地将数据从磁盘的数据寄存器读入 CPU 寄存器，再写入内存 缺点：在轮询和数据传输期间，CPU 被完全占用，无法执行其他任务，效率极其低下\n打印机与 DSL 打印机这类设备，将交互模型提升到了一个新的高度\n领域专用语言 (DSL)：打印机不是简单地接收像素数据，而是作为一个独立的计算机，接收并解释用页面描述语言（如 PostScript 或 PCL）编写的“程序”\nCPU（驱动程序）的角色更像是一个编译器，将应用程序的打印请求（如一个 Word 文档）编译成一串 PostScript 指令流，然后发送给打印机\n打印机内部的处理器负责执行这些指令，将抽象的描述（如“在这里画一条线”、“使用这个字体显示文本”）翻译成打印头的物理动作\n总线与可扩展性 单个计算机系统需要连接多种多样的设备，这就需要一个标准化的扩展机制——总线 (Bus)\n总线是一组共享的电子线路，它定义了一套协议，允许 CPU、内存和多个 I/O 设备之间进行通信，它提供了一种“设备虚拟化”，CPU 只需与总线控制器通信，由总线负责将请求转发到正确的设备\n可扩展性：通过标准的扩展插槽（如早期的 ISA、现代的 PCIe），用户可以向系统中添加无穷无尽的新设备，而无需修改主板或 CPU 的设计 PCIe 总线与 DMA PCIe (PCI Express) 是目前主流的高速总线标准，它引入了一项革命性的技术来解决 PIO 的效率问题：DMA，全称为直接内存访问 (Direct Memory Access)，它允许设备控制器在没有 CPU 干预的情况下，直接与主内存进行数据传输\n工作流程：\nCPU 设置 DMA 控制器，告诉它源地址、目标地址和传输大小 CPU 向设备发出“开始传输”的指令后，就可以去执行其他任务了 DMA 控制器全权负责数据的搬运 传输完成后，DMA 控制器通过中断通知 CPU 优势：DMA 极大地解放了 CPU，使其不需要负责搬运数据，从而显著提升了整个系统的 I/O 吞吐量和效率，是所有现代高性能设备（显卡、NVMe 硬盘、高速网卡）的基础\n设备驱动程序 file_operations 结构体 操作系统内核通过名为 file_operations 的结构体落实“Everything is a File”的思想\n核心机制：这个结构体本质上是一个函数指针列表，定义了一系列标准的文件操作，如 read, write, open, llseek, ioctl 等\n驱动的本质：一个设备驱动程序 (Device Driver) 的核心，就是为特定的硬件或虚拟设备，提供一套具体的 file_operations 实现，当一个设备被注册到系统中时，内核就会将这个设备的“文件”与这套操作函数关联起来\n驱动的翻译职责 设备驱动程序的核心职责，就是翻译应用程序和物理硬件之间的交互\n翻译过程：当一个用户程序对文件描述符执行系统调用时（例如 read(fd, buf, size)），内核会：\n通过 fd 找到对应的内核文件对象 从文件对象中找到关联的 file_operations 结构体 调用其中的 .read 函数指针，并将系统调用的参数传递过去 驱动的实现：驱动程序中的 .read 函数则负责执行设备相关的底层操作，比如通过 MMIO 或 PIO 向设备控制器发送指令，等待数据就绪，然后将数据从设备寄存器中读出，最后复制到用户空间的 buf 中\n虚拟设备：这个模型同样适用于虚拟设备，例如对 /dev/null 的 write 操作，其驱动实现仅仅是直接返回写入的字节数，而什么也不做，对 /proc/stat 的 read 操作，则是读取内核中的统计数据并格式化成字符串返回\nioctl 万能接口 对于读写数据流之外的设备控制和配置需求（如设置键盘重复率、获取磁盘健康信息、配置网络参数等），read/write 模型显然不能满足，为此，Unix 系统提供了一个通用的 ioctl (I/O Control) 系统调用\nioctl 是一个高度灵活的接口，它的具体行为完全由设备驱动程序定义，应用程序通过传递一个设备专属的命令码和参数，来执行特定的控制功能\n虽然强大，但 ioctl 也带来了巨大的复杂性，因为每个设备的命令集都不同，形成了一系列隐藏的、非标准的协议，应用程序需要知道这些细节才能与设备深度交互（是巨大的屎山💩）\n实际案例：\nlibc 缓冲：libc 库通过对文件描述符 1 (stdout) 执行一个 tty 设备专属的 ioctl 命令（如 TCGETS），来判断输出目标是否为一个交互式终端，从而决定是采用行缓冲还是全缓冲\nKVM 虚拟化：KVM 就是一个通过 ioctl 暴露全部功能的复杂设备，用户程序打开 /dev/kvm 后，通过一系列 ioctl 命令（如 KVM_CREATE_VM, KVM_SET_REGS, KVM_RUN）来创建虚拟机、设定 CPU 状态并运行虚拟机，直到发生 VM Exit 事件返回到用户态\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/20-%E8%AE%BE%E5%A4%87%E5%92%8C%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F/","summary":"\u003ch2 id=\"输入输出设备\"\u003e输入输出设备\u003c/h2\u003e\n\u003ch3 id=\"everything-is-a-file\"\u003eEverything is a File\u003c/h3\u003e\n\u003cp\u003e在 Unix-like 系统中，与外部设备交互的核心思想是 \u003cstrong\u003eEverything is a File\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e文件描述符 (File Descriptor)\u003c/strong\u003e：操作系统为上层软件提供了一个统一的抽象，即文件描述符，它是一个指向内核中任何 I/O 对象的“指针”或句柄\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e统一接口\u003c/strong\u003e：无论是普通文件、硬件设备（如终端、磁盘）、还是网络连接，都可以通过 \u003ccode\u003eopen\u003c/code\u003e 获得一个文件描述符，然后使用相同的 \u003ccode\u003eread\u003c/code\u003e/\u003ccode\u003ewrite\u003c/code\u003e 等系统调用来进行操作，这极大地简化了应用程序的编写\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"设备控制器与-mmio\"\u003e设备控制器与 MMIO\u003c/h3\u003e\n\u003cp\u003e“文件”这个美好的抽象背后，是具体的硬件工作原理\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e设备控制器 (Device Controller)\u003c/strong\u003e：每个 I/O 设备都有一个控制器，它是一个包含 CPU、内存和寄存器的微型计算机，作为 CPU 和物理设备之间的桥梁\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e设备寄存器\u003c/strong\u003e：控制器通过一组\u003cstrong\u003e寄存器\u003c/strong\u003e与 CPU 通信，通常包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e状态寄存器\u003c/strong\u003e：用于表示设备当前是否繁忙、是否准备好等\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e指令寄存器\u003c/strong\u003e：CPU 写入指令，告诉设备要做什么\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据寄存器\u003c/strong\u003e：用于在 CPU 和设备之间传输数据\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e内存映射 I/O (MMIO)\u003c/strong\u003e：为了让 CPU 能访问这些寄存器，现代系统普遍采用 \u003cstrong\u003eMMIO (Memory-Mapped I/O)\u003c/strong\u003e，操作系统会将设备的寄存器映射到物理内存地址空间中的特定区域，这样一来，CPU 就可以像访问普通内存一样，使用标准的 \u003ccode\u003eload\u003c/code\u003e/\u003ccode\u003estore\u003c/code\u003e 指令来读写设备寄存器，从而实现对设备的控制\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"gpio\"\u003eGPIO\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGPIO (General-Purpose Input/Output)\u003c/strong\u003e 是理解 I/O 设备原理最直观的例子，GPIO 就是一个物理引脚，可以通过编程设置为输入或输出模式\u003c/p\u003e\n\u003cp\u003e通过 MMIO，一个 GPIO 引脚的电平状态被映射到一个特定的内存地址，当 CPU 向这个地址写入 \u003ccode\u003e1\u003c/code\u003e 时，引脚就变为高电平；写入 \u003ccode\u003e0\u003c/code\u003e 时，则变为低电平，这个过程将一条内存写指令直接转化为了一个物理世界的动作（比如点亮一个 LED）\u003c/p\u003e","title":"20. 设备和驱动程序"},{"content":"科普性质，简单记录一下\n1-Bit 的存储：磁铁 要实现“持久化”存储，核心是要找到一个能反复改写的状态，很容易想到能够利用磁的特性，这就有了磁带的初步想法：\n一个长条的带子上面均匀有磁性物质 定位到特定位置之后通过放大感应电流读取 用电磁铁改变磁化方向来写入数据 为了提高存储密度，可以把这样的带子给卷起来，于是就得到了磁带\n这样的存储方式主要缺点是几乎不能随机读写（比如磁带收音机需要倒带），一般用于冷数据的存档和备份\n为了解决这个缺点，可以想到用旋转的二维平面来替代卷起来的带子，这样读写延迟就不会超过旋转的周期，这就得到了磁鼓：\n再在磁鼓的基础上进一步内卷，把用圆盘代替柱面，从而可以堆叠起来，进一步提高了存储密度，这就得到了磁盘：\n磁盘作为存储设备的随机读写性能虽然相比磁带有了很大的改善，但是还是需要等待定位到正确的位置，性能仍然不够优秀，为了读写定位到一个扇区通常需要花费几个毫秒的时间，这一点可以通过缓存和调度算法来缓解，让数据尽可能连续存储\n当我们在磁盘的基础上把读写头和盘片本体分开，我们就实现了数据的移动，这也就得到了软盘，这是上个数据数据发行的主要方式，虽然性能和可靠性都比较低，但是胜在了便捷、可移动\n1-Bit 的存储：挖坑 古人实现持久化存储的方式是在石头上刻字，也就是通过挖坑来存储信息，这种方式可以跨越非常长的时间\n而现代工业使我们可以挖出更加精细的坑，从而可以存储更高密度的信息\n为了读取这样的信息，我们可以从光学的角度考虑：在反射平面上挖粗糙坑，激光扫过表面，在平面会反射回来，在坑里会发生漫反射，于是我们只要检测是否收到反射光就可以识别是坑还是表面，这也就是光盘\n光盘最有趣的特性是容易复制，我们要制造光盘可以先仔细地制造一张反转的盘片，坑的位置对应其表面的突起，之后只需要直接用这个盘片压制加热的塑料再镀上反射膜就可以得到一张光盘，这种方式可以达到极高的写入速度\n当然这种挖坑方式的一个重要特性就是不能修改已经写入的内容的，很难填上一个已经挖了的坑（当然通过特殊的制造材料和工艺也是可以做到的），这也就是说里面存储的数据是 append only 的，想要修改之前的内容可以采用可持久化二叉树的结构\n光盘作为存储设备，价格低的同时容量和可靠性都比较高，同时顺序读性能一般，随机读性能低并且很难写入，一个重要的应用常见就是数字时代的内容分发\n现代这种挖坑的存储方式还有一种应用方式是回归古人石碑的形式，把信息刻在很稳定的材料上来做到永久存储\n1-Bit 的存储：电荷 前两种存储介质都存在比较大的缺陷：\n磁：依赖机械部件，从而无法避免 ms 级别的延迟 坑（光）：挖坑效率低，同时填坑很困难 而电荷则是一种非常理想的存储介质：电子的密度极高，并且电路的速度极快（还天然并行）\n在电路中实现 1-bit 的持久存储，一个想法是我们可以挖一个坑，两种状态分别是：\n在坑里填入电子 从坑里放跑电子 而这就得到了闪存 (Flash Memory) ： 其作为存储设备，价格低，容量和可靠性高，而且读写性能极高（由于电路天然并行，所以容量越大，速度越快）\n然而，闪存的物理原理也带来了其固有的缺陷，即会磨损 (wear out)\n每次放电 (erase) 操作都无法 100% 将电子放干净，这会对存储单元造成微小的、不可逆的损伤 在经历数千或数万次擦写循环后，一些存储单元会因为累积的损伤而失效，无法再可靠地存储数据，这被称为 “死单元 (Dead Cell)” 为了解决闪存的磨损问题，并将其更好地呈现给操作系统，现代固态存储设备（如 SSD、U 盘、SD 卡）内部实际上都集成了一个微型计算机系统\n这个系统运行着一层被称为 FTL (Flash Translation Layer) 的固件，它的核心功能之一是 磨损均衡 (Wear Leveling)\nFTL 会记录每个物理块的擦写次数，当操作系统请求写入某个逻辑地址时，FTL 会避免总是写入同一个物理块，而是将写入请求重定向到一个较少被使用的物理块上，这种机制类似于操作系统中的虚拟内存，通过引入一个间接层（逻辑地址到物理地址的映射）来隐藏底层硬件的复杂性并优化其使用寿命\n这也意味着，即便是便宜的 U 盘 或 SD 卡，其内部也可能包含一个 ARM 芯片来运行 FTL，而高性能的 SSD 则拥有更强大的处理器、缓存和更复杂的 FTL 算法，从而提供更长的寿命和更高的性能\n这也解释了为什么我们不应该购买过于廉价的 U 盘，因为它们可能会在 FTL 上偷工减料，甚至伪造容量和厂商信息，导致数据丢失\n存储设备与操作系统 块设备抽象 无论是旋转的磁盘还是闪存芯片，它们都不适合以单个字节为单位进行寻址，因为定位和元数据（如扇区头、ECC 校验码）的开销太大\n因此，存储设备将它们的存储空间划分为固定大小的块 (Block)，并以块为单位进行读写，这大大摊销了单次 I/O 操作的开销，这些设备在操作系统中被称为块设备 (Block Devices)\n操作系统看到的是一个线性的、从 0 开始编号的块数组 struct block disk[NUM_BLOCKS]，应用程序可以直接像读写文件一样读写块设备（例如 /dev/sda），但这样做的效率很低，如果随机读写一个字节，操作系统和设备硬件最终可能会读取或写入整个块，导致读/写放大 (read/write amplification) 的问题，因此，上层的文件系统被设计为能够感知“块”的存在，并以块为单位来组织和管理数据\n为了高效地管理对块设备的访问，操作系统提供了一个专门的 I/O 栈\n在 Linux 中，上层文件系统或应用程序通过块 I/O (Bio) 层提交请求，Bio 层提供了一个请求/响应接口，它将上层的读写请求封装成 struct bio 结构体，这些请求被放入队列中，等待 I/O 调度器 (I/O Scheduler) 进行处理，调度器会根据策略（例如合并相邻的请求、排序请求以减少磁头寻道时间）来优化队列\n现代 Linux 内核使用多队列块 I/O (Multi-queue block I/O, blk-mq) 机制，为每个 CPU 核心 分配独立的请求队列，充分利用了现代多核处理器和高速 SSD 的并行性\n最终，由设备驱动程序将处理好的请求发送给硬件执行\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/21-%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E5%8E%9F%E7%90%86/","summary":"\u003cp\u003e科普性质，简单记录一下\u003c/p\u003e\n\u003ch2 id=\"1-bit-的存储磁铁\"\u003e1-Bit 的存储：磁铁\u003c/h2\u003e\n\u003cp\u003e要实现“持久化”存储，核心是要找到一个能反复改写的状态，很容易想到能够利用磁的特性，这就有了磁带的初步想法：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e一个长条的带子上面均匀有磁性物质\u003c/li\u003e\n\u003cli\u003e定位到特定位置之后通过放大感应电流读取\u003c/li\u003e\n\u003cli\u003e用电磁铁改变磁化方向来写入数据\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e为了提高存储密度，可以把这样的带子给卷起来，于是就得到了磁带\u003c/p\u003e\n\u003cp\u003e这样的存储方式主要缺点是\u003cstrong\u003e几乎不能随机读写\u003c/strong\u003e（比如磁带收音机需要倒带），一般用于冷数据的存档和备份\u003c/p\u003e\n\u003cp\u003e为了解决这个缺点，可以想到用旋转的二维平面来替代卷起来的带子，这样读写延迟就不会超过旋转的周期，这就得到了\u003cstrong\u003e磁鼓\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/21-%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E5%8E%9F%E7%90%86/pasted-image-20250805012735-png\"\u003e\u003c/p\u003e\n\u003cp\u003e再在磁鼓的基础上进一步内卷，把用圆盘代替柱面，从而可以堆叠起来，进一步提高了存储密度，这就得到了\u003cstrong\u003e磁盘\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/images/21-%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E5%8E%9F%E7%90%86/pasted-image-20250805012958-png\"\u003e\u003c/p\u003e\n\u003cp\u003e磁盘作为存储设备的随机读写性能虽然相比磁带有了很大的改善，但是还是需要等待定位到正确的位置，性能仍然不够优秀，为了读写定位到一个扇区通常需要花费几个毫秒的时间，这一点可以通过缓存和调度算法来缓解，让数据尽可能连续存储\u003c/p\u003e\n\u003cp\u003e当我们在磁盘的基础上把读写头和盘片本体分开，我们就实现了数据的移动，这也就得到了\u003cstrong\u003e软盘\u003c/strong\u003e，这是上个数据数据发行的主要方式，虽然性能和可靠性都比较低，但是胜在了便捷、可移动\u003c/p\u003e\n\u003ch2 id=\"1-bit-的存储挖坑\"\u003e1-Bit 的存储：挖坑\u003c/h2\u003e\n\u003cp\u003e古人实现持久化存储的方式是在石头上刻字，也就是通过挖坑来存储信息，这种方式可以跨越非常长的时间\u003c/p\u003e\n\u003cp\u003e而现代工业使我们可以挖出更加精细的坑，从而可以存储更高密度的信息\u003c/p\u003e\n\u003cp\u003e为了读取这样的信息，我们可以从光学的角度考虑：在反射平面上挖粗糙坑，激光扫过表面，在平面会反射回来，在坑里会发生漫反射，于是我们只要检测是否收到反射光就可以识别是坑还是表面，这也就是\u003cstrong\u003e光盘\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e光盘最有趣的特性是容易复制，我们要制造光盘可以先仔细地制造一张反转的盘片，坑的位置对应其表面的突起，之后只需要直接用这个盘片压制加热的塑料再镀上反射膜就可以得到一张光盘，这种方式可以达到极高的写入速度\u003c/p\u003e\n\u003cp\u003e当然这种挖坑方式的一个重要特性就是不能修改已经写入的内容的，很难填上一个已经挖了的坑（当然通过特殊的制造材料和工艺也是可以做到的），这也就是说里面存储的数据是 append only 的，想要修改之前的内容可以采用可持久化二叉树的结构\u003c/p\u003e\n\u003cp\u003e光盘作为存储设备，价格低的同时容量和可靠性都比较高，同时顺序读性能一般，随机读性能低并且很难写入，一个重要的应用常见就是数字时代的内容分发\u003c/p\u003e\n\u003cp\u003e现代这种挖坑的存储方式还有一种应用方式是回归古人石碑的形式，把信息刻在很稳定的材料上来做到永久存储\u003c/p\u003e\n\u003ch2 id=\"1-bit-的存储电荷\"\u003e1-Bit 的存储：电荷\u003c/h2\u003e\n\u003cp\u003e前两种存储介质都存在比较大的缺陷：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e磁：依赖机械部件，从而无法避免 ms 级别的延迟\u003c/li\u003e\n\u003cli\u003e坑（光）：挖坑效率低，同时填坑很困难\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e而电荷则是一种非常理想的存储介质：电子的密度极高，并且电路的速度极快（还天然并行）\u003c/p\u003e\n\u003cp\u003e在电路中实现 1-bit 的持久存储，一个想法是我们可以挖一个坑，两种状态分别是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在坑里填入电子\u003c/li\u003e\n\u003cli\u003e从坑里放跑电子\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e而这就得到了\u003cstrong\u003e闪存 (Flash Memory)\u003c/strong\u003e ：\n\u003cimg loading=\"lazy\" src=\"/images/21-%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E5%8E%9F%E7%90%86/pasted-image-20250805112704-png\"\u003e\n其作为存储设备，价格低，容量和可靠性高，而且读写性能极高（由于电路天然并行，所以容量越大，速度越快）\u003c/p\u003e\n\u003cp\u003e然而，闪存的物理原理也带来了其固有的缺陷，即\u003cstrong\u003e会磨损 (wear out)\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每次放电 (erase) 操作都无法 100% 将电子放干净，这会对存储单元造成微小的、不可逆的损伤\u003c/li\u003e\n\u003cli\u003e在经历数千或数万次擦写循环后，一些存储单元会因为累积的损伤而失效，无法再可靠地存储数据，这被称为 “\u003cstrong\u003e死单元 (Dead Cell)\u003c/strong\u003e”\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e为了解决闪存的磨损问题，并将其更好地呈现给操作系统，现代固态存储设备（如 SSD、U 盘、SD 卡）内部实际上都集成了一个微型计算机系统\u003c/p\u003e\n\u003cp\u003e这个系统运行着一层被称为 \u003cstrong\u003eFTL (Flash Translation Layer)\u003c/strong\u003e 的固件，它的核心功能之一是 \u003cstrong\u003e磨损均衡 (Wear Leveling)\u003c/strong\u003e\u003c/p\u003e","title":"21. 存储设备原理"},{"content":"目录树 文件的抽象 操作系统将物理存储设备（如磁盘）的复杂性隐藏起来，提供了一个简单、统一的抽象——文件\n文件可以看作是一个虚拟的磁盘，即一个命名的、一维的字节序列，支持 read, write, lseek 等操作\n这种抽象使得上层应用无需关心数据在物理磁盘上的具体位置和存储方式\n目录的引入 当文件数量增多时，需要一种方式来组织和管理它们\n操作系统引入了目录 (Directory) 的概念，它是一种特殊的文件，其内容是其他文件或目录的列表\n通过将文件和目录组织成一个层次化的树状结构，即目录树，可以方便地对文件进行分类、查找和管理\n多数类 Unix 系统遵循 FHS (Filesystem Hierarchy Standard) 的目录结构约定，为软件和用户预测文件位置提供了便利\n目录操作 API 操作系统提供了一系列系统调用来操作目录树，核心操作围绕“增删改查”\nmkdir: 创建一个新的目录 rmdir: 删除一个空的目录 getdents: 读取目录中的条目 (directory entries) link / unlink: 创建或删除文件的链接 链接 链接是文件系统的一个重要特性，它允许一个文件拥有多个名字或存在于目录树的多个位置\n链接主要分为两种类型：硬链接和软链接（符号链接）\n硬链接 Hard Link 定义：硬链接是让多个目录条目（文件名）直接指向磁盘上同一个文件索引节点 (inode)\n每个文件在文件系统中都有一个唯一的 inode，它包含了文件的元数据（如权限、大小、数据块位置等）和数据本身\n创建一个硬链接，相当于为同一个 inode 增加了一个新的入口点（文件名）\n特性：\n所有指向同一个 inode 的硬链接地位平等，没有主次之分 inode 内部维护一个链接计数 (reference count), 只有当这个计数减到 0 时，文件系统才会真正删除该 inode 和对应的数据块，这也是 unlink 系统调用的由来 限制：\n不能为目录创建硬链接，以防止在目录树中产生循环 不能跨越不同的文件系统（因为 inode 号只在当前文件系统内唯一） 软链接 Symbolic Link 定义：软链接，也称符号链接 (symlink)，是一个特殊的文件，它的内容是另一个文件或目录的路径\n软链接本身拥有自己独立的 inode 和数据块，其数据块中存储的是一个文本字符串，即目标对象的路径名，当访问软链接时，操作系统会解析其内容，并将访问请求重定向到它所指向的路径\n特性：\n极其灵活，因为它本质上只是一个路径的“快捷方式” 可以链接到目录 可以跨越不同的文件系统 可以创建一个“悬空”的链接 (dangling link)，即它指向的目标路径当前并不存在 删除软链接本身，对它指向的原始文件没有任何影响 应用:\n常用于管理软件版本，例如让一个通用的命令（如 python）指向一个具体的版本文件（如 /usr/bin/python3.9） 被 NixOS 等系统用来构建高度可复现和隔离的环境，通过大量使用软链接将不同版本的软件包组合成一个虚拟的文件系统结构 文件的元数据 基本元数据 文件作为操作系统中的对象，拥有一系列的属性 (attributes)，这些属性就是元数据 (metadata), 你可以通过 ls -l 命令查看文件的主要元数据，这包括文件的类型、所有者、大小、修改时间等关键信息\n其中，模式 (Mode) 字段定义了文件的访问权限，它分为三组，分别对应所有者 (owner)、所属组 (group) 和其他用户 (other)，每组都包含读 (r)、写 (w)、执行 (x) 三种权限\n一个常见的权限例子是 755，这是一个八进制数，常用于程序或目录\n第一个 7 代表所有者权限, 7=4+2+1, 意味着读、写、执行 (rwx) 权限全开 第二个 5 代表所属组权限, 5=4+0+1, 意味着读、执行 (r-x) 权限 第三个 5 代表其他用户权限, 5=4+0+1, 同样是读、执行 (r-x) 权限 Extended Attributes (xattr) 扩展属性 xattr 是现代文件系统提供的一项强大功能，它允许为文件附加一个灵活的 key-value 键值对字典，用于存储标准元数据无法覆盖的任意信息，操作系统提供了 fsetxattr 和 fgetxattr 等系统调用来操作这些属性\n应用：\nmacOS 的安全隔离机制就是一个典型例子，当从网络下载文件后，系统会自动添加 com.apple.quarantine 属性，记录下载来源（URL）和时间，首次打开时，系统会检查此属性并向用户发出安全警告\n文件内容元信息: 应用程序可以利用 xattr 存储与文件内容相关的元信息，例如，图片浏览器可以存储照片的 EXIF 数据副本，或者音乐播放器可以存储歌曲的演唱者和专辑信息，便于管理和搜索；相比于文件目录只能按照文件标题索引，这种以内容作为索引才是现代文件系统更合理的做法\n缺陷：虽然 xattr 功能强大，但它“好用不火”的原因在于其固有的缺陷\n缺乏标准化与兼容性: xattr 的键名没有统一标准，不同应用和系统间随意定义，导致数据难以互通，例如，com.apple.quarantine 属性在 Linux 或 Windows 上没有意义 工具支持不完善: 许多经典的命令行工具，如 cp, mv, tar, rsync 等，默认不会处理扩展属性，在执行文件复制或打包时，这些重要的元数据可能会被静默丢弃，用户必须显式使用特定参数（如 cp --preserve=xattr, rsync -X）才能保留它们，这对依赖 xattr 的系统（如使用了 SELinux）可能是灾难性的 可见性低: 扩展属性对于普通用户是不可见的，ls -l 命令并不会显示它们，需要使用 getfattr 或 xattr -l 等专用工具才能查看，这使得问题排查变得更加困难 Access Control List (ACL) 传统的 user/group/other 权限模型在处理复杂的共享需求时显得力不从心，例如需要让用户 bob 访问 alice 的一个文件，但 bob 不在 alice 的用户组里，而又不想把文件权限开放给所有“其他用户”，ACL 就是为了解决这类问题而生的\nACL 提供了比传统模型更精细、更灵活的访问控制机制，它允许你为任意指定的用户或用户组设置独立的权限\n文件系统级 API 与针对单个文件或目录的操作不同，文件系统还提供了一系列“系统级”的 API，用于管理整个文件系统的结构和行为\n挂载 Mount 在类 Unix 系统中，所有的文件和目录都组织在一棵以根目录 / 为起点的巨大目录树下，而在 Windows 中，文件系统则分散在不同的“盘符”下（C:、D: 等）\n挂载 (mount) 是构建这棵统一目录树的核心机制, 它的作用是将一个文件系统（通常来自一个独立的存储设备，如硬盘分区、U 盘或光盘）“附加”到现有目录树的一个挂载点 (mount point) 上, 挂载点是一个已存在的空目录\n例如，命令 mount /dev/sdb1 /mnt/data 就将 /dev/sdb1 这个分区上的文件系统挂载到了 /mnt/data 目录, 此后，对 /mnt/data 目录内容的访问，实际上就是对 /dev/sdb1 分区根目录的访问, 整个 Linux 系统的根目录 / 本身也是在系统启动时挂载的第一个文件系统\n回环设备 Loopback Device mount 命令通常操作的是块设备 (block device)，但有时我们需要挂载一个存在于文件中的文件系统镜像（例如一个 .iso 光盘镜像文件）, 文件不是块设备，所以无法直接挂载\n为了解决这个问题，Linux 提供了回环设备 (loopback device), 这是一个虚拟的块设备，它不对应任何物理硬件，而是将一个普通文件作为其后端存储\n它的工作流程可以想象成一个适配器：\n将文件镜像与一个回环设备（如 /dev/loop0）关联起来, 这时，操作系统看待 /dev/loop0 就像看待一个真实的物理磁盘一样 对这个回环设备执行 mount 操作，将其挂载到指定目录 这个过程的底层是通过 ioctl 系统调用实现的，它向 loop 驱动发送 LOOP_SET_FD 等命令，将文件描述符与一个空闲的 loop 设备进行绑定\n联合文件系统 OverlayFS OverlayFS 是一种强大的联合文件系统 (UnionFS)，它允许将多个不同的目录（称为层）“堆叠”起来，对外提供一个统一的、合并后的视图\n我们可以用一个非常形象的比喻来理解它的工作原理：\n底层 (lowerdir): 想象一张已经印刷好的、不可修改的原始画稿, 这就是只读的底层, 它可以有很多张，层层叠放 上层 (upperdir)：在原始画稿上覆盖一张透明的塑料薄膜, 这就是可写的上层 合并视图 (merged view)：你透过这张透明薄膜看到的最终景象，就是 OverlayFS 呈现给你的目录 基于这个模型，所有操作都变得非常直观：\n读取文件: 当你读取一个文件时，相当于透过透明薄膜看画稿, 如果文件只存在于底层，你会直接看到它, 如果文件在上层也存在，那么上层的版本会“遮盖”住底层的版本\n修改文件：你不能直接修改原始画稿（lowerdir），当你第一次尝试修改一个来自底层的文件时，系统会启动写时复制 (Copy-on-Write) 机制，先把这个文件从底层复制一份到上层的透明薄膜上，然后你所有的修改都发生在这份复制品上\n创建文件：这就像直接在透明薄膜上画新的内容，完全不影响下面的原始画稿\n删除文件：你无法擦除原始画稿的内容, 当你删除一个来自底层的文件时，系统会在上层创建一个特殊的“白点”文件 (whiteout)，像贴了一张不透明的小贴纸，刚好遮住底层的文件，让它看起来像是被删除了\n这种分层和写时复制的机制是 Docker 等容器技术的核心, 容器镜像就是只读的 lowerdir，而每个运行的容器都有自己专属的可写 upperdir，这使得成百上千个容器可以共享同一个基础镜像，同时保持各自的隔离性，极大地节省了存储空间和部署时间\n文件系统快照 Snapshot 一些高级的文件系统（如 Btrfs, ZFS）或逻辑卷管理器（LVM）支持快照 (snapshot) 功能，它可以在瞬间“冻结”并创建一个文件系统在某个时间点的完整副本\n快照的实现也常常依赖于写时复制技术，创建快照时，并不会立即复制所有数据，而只是创建了一个指向当前数据块的指针集合, 当之后有数据块被修改时，文件系统不会覆盖旧的数据块，而是将修改写入新的位置，并让旧的快照指针继续指向未修改的旧数据块\n这个功能对于系统备份、快速回滚和创建安全的测试环境非常有用\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/22-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-api/","summary":"\u003ch2 id=\"目录树\"\u003e目录树\u003c/h2\u003e\n\u003ch3 id=\"文件的抽象\"\u003e文件的抽象\u003c/h3\u003e\n\u003cp\u003e操作系统将物理存储设备（如磁盘）的复杂性隐藏起来，提供了一个简单、统一的抽象——\u003cstrong\u003e文件\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e文件可以看作是一个\u003cstrong\u003e虚拟的磁盘\u003c/strong\u003e，即一个命名的、一维的\u003cstrong\u003e字节序列\u003c/strong\u003e，支持 \u003ccode\u003eread\u003c/code\u003e, \u003ccode\u003ewrite\u003c/code\u003e, \u003ccode\u003elseek\u003c/code\u003e 等操作\u003c/p\u003e\n\u003cp\u003e这种抽象使得上层应用无需关心数据在物理磁盘上的具体位置和存储方式\u003c/p\u003e\n\u003ch3 id=\"目录的引入\"\u003e目录的引入\u003c/h3\u003e\n\u003cp\u003e当文件数量增多时，需要一种方式来组织和管理它们\u003c/p\u003e\n\u003cp\u003e操作系统引入了\u003cstrong\u003e目录 (Directory)\u003c/strong\u003e 的概念，它是一种特殊的文件，其内容是其他文件或目录的列表\u003c/p\u003e\n\u003cp\u003e通过将文件和目录组织成一个层次化的\u003cstrong\u003e树状结构\u003c/strong\u003e，即\u003cstrong\u003e目录树\u003c/strong\u003e，可以方便地对文件进行分类、查找和管理\u003c/p\u003e\n\u003cp\u003e多数类 Unix 系统遵循 \u003cstrong\u003eFHS (Filesystem Hierarchy Standard)\u003c/strong\u003e 的目录结构约定，为软件和用户预测文件位置提供了便利\u003c/p\u003e\n\u003ch3 id=\"目录操作-api\"\u003e目录操作 API\u003c/h3\u003e\n\u003cp\u003e操作系统提供了一系列系统调用来操作目录树，核心操作围绕“增删改查”\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emkdir\u003c/code\u003e: 创建一个新的目录\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ermdir\u003c/code\u003e: 删除一个空的目录\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003egetdents\u003c/code\u003e: 读取目录中的条目 (\u003cstrong\u003ed\u003c/strong\u003eirectory \u003cstrong\u003eent\u003c/strong\u003erie\u003cstrong\u003es\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elink\u003c/code\u003e / \u003ccode\u003eunlink\u003c/code\u003e: 创建或删除文件的链接\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"链接\"\u003e链接\u003c/h3\u003e\n\u003cp\u003e链接是文件系统的一个重要特性，它允许一个文件拥有多个名字或存在于目录树的多个位置\u003c/p\u003e\n\u003cp\u003e链接主要分为两种类型：\u003cstrong\u003e硬链接\u003c/strong\u003e和\u003cstrong\u003e软链接（符号链接）\u003c/strong\u003e\u003c/p\u003e\n\u003ch4 id=\"硬链接-hard-link\"\u003e硬链接 Hard Link\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e：\u003cstrong\u003e硬链接\u003c/strong\u003e是让多个目录条目（文件名）直接指向磁盘上同一个\u003cstrong\u003e文件索引节点 (inode)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e每个文件在文件系统中都有一个唯一的 \u003ccode\u003einode\u003c/code\u003e，它包含了文件的元数据（如权限、大小、数据块位置等）和数据本身\u003c/p\u003e\n\u003cp\u003e创建一个硬链接，相当于为同一个 \u003ccode\u003einode\u003c/code\u003e 增加了一个新的入口点（文件名）\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e特性\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e所有指向同一个 \u003ccode\u003einode\u003c/code\u003e 的硬链接地位平等，没有主次之分\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003einode\u003c/code\u003e 内部维护一个\u003cstrong\u003e链接计数 (reference count)\u003c/strong\u003e, 只有当这个计数减到 0 时，文件系统才会真正删除该 \u003ccode\u003einode\u003c/code\u003e 和对应的数据块，这也是 \u003ccode\u003eunlink\u003c/code\u003e 系统调用的由来\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e限制\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e不能为目录创建硬链接，以防止在目录树中产生循环\u003c/li\u003e\n\u003cli\u003e不能跨越不同的文件系统（因为 \u003ccode\u003einode\u003c/code\u003e 号只在当前文件系统内唯一）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"软链接-symbolic-link\"\u003e软链接 Symbolic Link\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e：\u003cstrong\u003e软链接\u003c/strong\u003e，也称\u003cstrong\u003e符号链接 (symlink)\u003c/strong\u003e，是一个\u003cstrong\u003e特殊的文件\u003c/strong\u003e，它的内容是另一个文件或目录的\u003cstrong\u003e路径\u003c/strong\u003e\u003c/p\u003e","title":"22. 文件系统 API"},{"content":"A General Paradigm of Test-Time Adaptation 根据测试数据接收方式和适应过程，TTA 分为三种主要范式：\nTest-Time Batch Adaptation (TTBA) 测试时间批次适应： 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。 Online Test-Time Adaptation (OTTA) 在线测试时间适应： 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。 Test-Time Domain Adaptation (TTDA) 测试时间域适应： 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。 Datasets for Evaluation 论文使用了两种不同类型的分布偏移数据集进行评估：\nCorruption Datasets 损坏数据集： 原始数据集（CIFAR-10，ImageNet）经过人为损坏处理后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。 Natural-shift Datasets 自然偏移数据集： 这些数据集代表数据分布中自然发生的变化，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。 Results on Natural Shift Datasets TTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。 PredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。 T3A 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。 对于自然偏移数据集，TTDA 算法 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。 ","permalink":"https://diefish1024.github.io/posts/literature-notes/benchmarking-tta/","summary":"\u003ch3 id=\"a-general-paradigm-of-test-time-adaptation\"\u003eA General Paradigm of Test-Time Adaptation\u003c/h3\u003e\n\u003cp\u003e根据测试数据接收方式和适应过程，TTA 分为三种主要范式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Batch Adaptation (TTBA) 测试时间批次适应：\u003c/strong\u003e 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOnline Test-Time Adaptation (OTTA) 在线测试时间适应：\u003c/strong\u003e 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Domain Adaptation (TTDA) 测试时间域适应：\u003c/strong\u003e 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"datasets-for-evaluation\"\u003eDatasets for Evaluation\u003c/h3\u003e\n\u003cp\u003e论文使用了两种不同类型的分布偏移数据集进行评估：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCorruption Datasets 损坏数据集：\u003c/strong\u003e 原始数据集（CIFAR-10，ImageNet）经过\u003cstrong\u003e人为损坏处理\u003c/strong\u003e后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNatural-shift Datasets 自然偏移数据集：\u003c/strong\u003e 这些数据集代表数据分布中\u003cstrong\u003e自然发生的变化\u003c/strong\u003e，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"results-on-natural-shift-datasets\"\u003eResults on Natural Shift Datasets\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。\u003c/li\u003e\n\u003cli\u003ePredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eT3A\u003c/strong\u003e 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。\u003c/li\u003e\n\u003cli\u003e对于自然偏移数据集，\u003cstrong\u003eTTDA 算法\u003c/strong\u003e 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。\u003c/li\u003e\n\u003c/ul\u003e","title":"Benchmarking TTA"},{"content":"Setting Continual Test-Time Domain Adaptation 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个非平稳且持续变化的目标环境 。\nCoTTA 与以下方法不同：\nStandard Domain Adaptation：需要同时访问源数据和（静态的）目标数据进行训练。 Standard Test-Time Adaptation / Fully Test-Time Adaptation：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。 Test-Time Training (TTT)：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。 相比之下，CoTTA 专门解决在无源数据的条件下，模型如何在线适应一个持续变化的数据流，同时克服现有方法中常见的错误累积和灾难性遗忘问题。\nMethod 论文的核心贡献是提出了CoTTA (Continual Test-Time Adaptation) 方法，旨在通过减少错误累积和避免灾难性遗忘，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。\n1. 减少错误累积 (Reducing Error Accumulation) 为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。\n权重平均伪标签 (Weight-Averaged Pseudo-Labels) 该方法采用一个教师 - 学生 (teacher-student) 框架。学生模型 (student model) 在线进行学习和更新。 教师模型 (teacher model) 的权重是学生模型权重的指数移动平均 (Exponential Moving Average, EMA)。 由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的一致性损失 (consistency loss) 来进行更新。 数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels) 为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。 它首先使用原始预训练模型评估当前测试数据的预测置信度，以此来近似域差异的大小。 条件性应用： 如果置信度高（域差异小），则直接使用教师模型的预测作为伪标签 16。 如果置信度低（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签 17171717。这可以进一步提高伪标签的鲁棒性。 2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting) 为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了随机恢复 (Stochastic Restoration) 机制。\n核心思想：在每次模型更新后，以一个很小的概率 p，将模型中的一小部分权重参数随机地恢复到其原始的、预训练时的状态。 优势： 这种机制可以看作一种特殊的 Dropout 20。它能有效防止模型在适应新数据时“漂移”得离源模型太远，从而显式地保留了源知识，避免了灾难性遗忘。 通过保留源知识，CoTTA 能够安全地更新网络中的所有参数，而不仅仅是归一化层，这为模型适应提供了更大的容量。 Algorithm CoTTA 算法的在线流程如下：\nInitialization (初始化)： 加载一个“开箱即用”的预训练源模型 $ f_{\\theta_{0}} $ 用源模型权重初始化教师模型 $ f_{\\theta'_{0}} $ Iteration (迭代)：对于在线输入的每个测试数据 $ x_{t} $​： 生成伪标签：使用教师模型 $ f_{\\theta'_{t}} $​​，并结合条件性数据增强，生成权重和增强平均的伪标签。 更新学生模型：通过一致性损失更新学生模型 $ f_{\\theta_{t}} $ 更新教师模型：使用 EMA 更新教师模型的权重 $ f_{\\theta'_{t+1}} $ 随机恢复：对学生模型的权重进行随机恢复 Output (输出)：使用教师模型 $ f_{\\theta'_{t}} $​​ 进行在线预测，并传递更新后的学生和教师模型到下一个时间步。 Experiments 论文在多个图像分类和语义分割任务上对 CoTTA 进行了评估，特别是在一个持续变化的测试环境中。\nContinual Adaptation on Corrupted Images 在 CIFAR10-C、CIFAR100-C 和 ImageNet-C 数据集上，模型被顺序输入 15 种不同类型的损坏图像。\n主要发现： 在 CIFAR10-C 上，CoTTA 的平均错误率仅为 16.2%，显著优于 Source-only 基线 (43.5%) 和 TENT-continual (20.7%)。 在更难的 CIFAR100-C 上，TENT 等方法因错误累积导致性能随时间推移而急剧下降（错误率从 37.2% 恶化到 90.4%），而 CoTTA 表现稳定，平均错误率仅为 32.5% 。 实验表明，TENT 在持续适应的后期会因错误累积而性能崩溃，而 CoTTA 的随机恢复机制成功避免了这一点。 Continual Adaptation on Semantic Segmentation 在一个从 Cityscapes (晴天) 到 ACDC (雾、夜、雨、雪等恶劣天气) 的持续语义分割任务中，模型会循环经历这四种天气条件 10 次，以测试其长期适应和遗忘情况。\n主要发现： CoTTA 将平均 mIoU 提升至 58.6%，优于源模型 (56.7%) 和其他适应方法。 TENT 在此任务上表现不佳，因为其依赖的批量归一化 (Batch Normalization) 层在 Transformer 架构 (Segformer) 中很少。 CoTTA 不依赖于特定层，因此在基于 Transformer 的架构上同样有效，展现了其通用性 38。 Analysis 通过消融实验验证了 CoTTA 各个组件的有效性。\n权重平均的作用：仅使用权重平均的伪标签，就将错误率从 20.7% (TENT-continual) 降至 18.3%，证明了教师模型伪标签的优越性。 数据增强平均的作用：在权重平均的基础上再加入条件性数据增强，错误率进一步降至 17.4%。 随机恢复的作用：最后加入随机恢复机制，错误率最终降至 16.2%，并且解决了长期适应中的性能衰退问题，证明了其在避免灾难性遗忘中的关键作用。 Related Work 论文回顾了与 CoTTA 相关的领域：\nTest-Time Adaptation (TTA)：现有工作大多关注静态目标域，在持续变化的环境中，基于熵最小化或伪标签的方法容易因伪标签噪声而累积错误。 Continuous Domain Adaptation：与 CoTTA 目标相似，但现有方法通常需要访问源数据来对齐分布。 Continual Learning：CoTTA 借鉴了该领域的思想来解决灾难性遗忘问题，但将其应用在了一个无监督、测试时适应的独特场景中。 Source-Free Domain Adaptation：CoTTA 属于此范畴，其新颖之处在于它专为在线和持续变化的环境设计，而这是先前工作很少考虑的。 Discussion CoTTA 成功地解决了在无源数据、非平稳环境下进行持续测试时适应的挑战。它通过创新的机制同时解决了错误累积和灾难性遗忘这两个核心难题。\n优势总结： 稳定与长效：通过随机恢复机制，CoTTA 实现了在长期持续适应过程中的性能稳定，避免了性能崩溃。 实用与通用：无需访问源数据，也无需修改模型训练过程，可直接用于各类“开箱即用”的预训练模型（包括 CNN 和 Transformer） 。 高效：整个适应过程在线进行，模型根据当前数据流即时更新和预测。 总而言之，CoTTA 为模型在真实世界中部署后的持续自我进化提供了一个强大且实用的框架，使模型能够鲁棒地适应不断变化的操作环境。\n","permalink":"https://diefish1024.github.io/posts/literature-notes/cotta/","summary":"\u003ch1 id=\"setting\"\u003eSetting\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eContinual Test-Time Domain Adaptation\u003c/strong\u003e 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个\u003cstrong\u003e非平稳\u003c/strong\u003e且\u003cstrong\u003e持续变化\u003c/strong\u003e的目标环境 。\u003c/p\u003e\n\u003cp\u003eCoTTA 与以下方法不同：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStandard Domain Adaptation\u003c/strong\u003e：需要同时访问源数据和（静态的）目标数据进行训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStandard Test-Time Adaptation / Fully Test-Time Adaptation\u003c/strong\u003e：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Training (TTT)\u003c/strong\u003e：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相比之下，CoTTA 专门解决在\u003cstrong\u003e无源数据\u003c/strong\u003e的条件下，模型如何在线适应一个\u003cstrong\u003e持续变化的\u003c/strong\u003e数据流，同时克服现有方法中常见的\u003cstrong\u003e错误累积\u003c/strong\u003e和\u003cstrong\u003e灾难性遗忘\u003c/strong\u003e问题。\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003e论文的核心贡献是提出了\u003cstrong\u003eCoTTA (Continual Test-Time Adaptation)\u003c/strong\u003e 方法，旨在通过\u003cstrong\u003e减少错误累积\u003c/strong\u003e和\u003cstrong\u003e避免灾难性遗忘\u003c/strong\u003e，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。\u003c/p\u003e\n\u003ch3 id=\"1-减少错误累积-reducing-error-accumulation\"\u003e1. 减少错误累积 (Reducing Error Accumulation)\u003c/h3\u003e\n\u003cp\u003e为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e权重平均伪标签 (Weight-Averaged Pseudo-Labels)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e该方法采用一个\u003cstrong\u003e教师 - 学生 (teacher-student)\u003c/strong\u003e 框架。学生模型 (student model) 在线进行学习和更新。\u003c/li\u003e\n\u003cli\u003e教师模型 (teacher model) 的权重是学生模型权重的\u003cstrong\u003e指数移动平均 (Exponential Moving Average, EMA)\u003c/strong\u003e。\u003c/li\u003e\n\u003cli\u003e由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的\u003cstrong\u003e一致性损失\u003c/strong\u003e (consistency loss) 来进行更新。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。\u003c/li\u003e\n\u003cli\u003e它首先使用\u003cstrong\u003e原始预训练模型\u003c/strong\u003e评估当前测试数据的\u003cstrong\u003e预测置信度\u003c/strong\u003e，以此来近似域差异的大小。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e条件性应用\u003c/strong\u003e：\n\u003cul\u003e\n\u003cli\u003e如果置信度\u003cstrong\u003e高\u003c/strong\u003e（域差异小），则直接使用教师模型的预测作为伪标签 16。\u003c/li\u003e\n\u003cli\u003e如果置信度\u003cstrong\u003e低\u003c/strong\u003e（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签 17171717。这可以进一步提高伪标签的鲁棒性。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-避免灾难性遗忘-avoiding-catastrophic-forgetting\"\u003e2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting)\u003c/h3\u003e\n\u003cp\u003e为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了\u003cstrong\u003e随机恢复 (Stochastic Restoration)\u003c/strong\u003e 机制。\u003c/p\u003e","title":"CoTTA"},{"content":"Introduction 类似 GAN 的对抗训练思想\nDomain Adaptation 给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \\eta: X\\to Y $ 使其在目标域上的目标风险 $$ R_{D_{T}}(\\eta) = \\underset{(\\mathbf{x},y)\\sim D_{T}}{\\mathrm{Pr}}(\\eta(\\mathbf{x}) \\neq y) $$ 最小\nDomain Divergence 需要量化两个领域的“相似度”，从而引出了 H- 散度 的概念： $$ d_{\\mathcal{H}}(D_S, D_T) = 2 \\sup_{\\eta \\in \\mathcal{H}} \\left| \\Pr_{x \\sim D_S}[\\eta(x) = 1] - \\Pr_{x \\sim D_T}[\\eta(x) = 1] \\right| $$ 含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果\n由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度 $$ \\hat{d}_{\\mathcal{H}}(S, T) = 2 \\left(1 - \\min_{\\eta \\in \\mathcal{H}} \\left[ \\dfrac{1}{n}\\sum_{i=1}^n \\mathcal{I}[\\eta(x_i) = 0] + \\dfrac{1}{n'}\\sum_{i=n+1}^N \\mathcal{I}[\\eta(x_i) = 1] \\right] \\right) $$ 其中 $ \\mathcal{I}[\\cdot] $ 表示条件为真时为 1，否则为 0\nProxy Distance 经验 H- 散度也需要直接遍历所有的 $ \\eta $ ，在计算上不现实，需要一个进一步的近似方法，因此考虑 Proxy A-distance (PAD)\n构造用于领域分类的数据集 $$ U = \\{ (\\mathbf{x}_{i},0) \\}_{i=1}^{n} \\cup \\{ (\\mathbf{x}_{i},1) \\}_{i=n+1}^{N} $$ 用这个数据集训练分类器，设 $ \\epsilon $ 为在数据集 $ U $ 上训练出的最优领域分类器所达到的最小错误率，那么可以用 $$ \\hat{d}_{\\mathcal{A}} = 2(1-2\\epsilon) $$ 来近似 H- 散度\nGeneralization Bound on the Target Risk 有效性证明\n理论研究说明模型的目标风险可以通过源风险和两个领域的散度来限制，主要思想是 $$ R_{D_T}(\\eta) \\le R_S(\\eta) + \\text{Domain Divergence Terms} + \\text{Complexity Terms} + \\beta $$ 其中 $ \\text{Domain Divergence Terms}\\approx d_{\\mathcal{H}}(S, T) $ ，可以用上面的 $ \\hat{d}_{\\mathcal{A}} $ 近似；$ \\text{Complexity Terms} $ 是一个比较小的常数项，和模型本身训练有关（原公式没看懂。。）；$ \\beta $ 是一个理想化的项，表示最好情况下在目标域和源域上同时取得的最低错误率\nDANN 优化目标： $$ E(\\theta_f, \\theta_y, \\theta_d) = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_y(\\theta_f, \\theta_y) - \\lambda \\left( \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_d(\\theta_f, \\theta_d) + \\frac{1}{n'} \\sum_{i=n+1}^N \\mathcal{L}_d(\\theta_f, \\theta_d) \\right) $$ 核心是 Saddle Point Problem ，找到需要找到鞍点而非最小值\n如何实现对抗：\n标签预测参数： $$ \\theta_{y} \\leftarrow \\theta_{y} - \\mu \\dfrac{ \\partial \\mathcal{L}_{y} }{ \\partial \\theta_{y} } $$ 领域分类参数： $$ \\theta_{d} \\leftarrow \\theta_{d} - \\mu \\lambda \\dfrac{ \\partial \\mathcal{L}_{d} }{ \\partial \\theta_{d} } $$ 特征提取参数： $$ \\theta_{f} \\leftarrow \\theta_{f} - \\mu\\left( \\dfrac{ \\partial \\mathcal{L}_{y} }{ \\partial \\theta_{f} } - \\lambda \\dfrac{ \\partial \\mathcal{L}_{d} }{ \\partial \\theta_{f} } \\right) $$ 核心需要最大化 $ \\mathcal{L}_{d} $ ，因此需要沿着梯度的正向优化 Gradient Reversal Layer (GRL) 是实现对抗的核心组件，具体原理是在前向传播时表现为 $ R(x)=x $ ，但是反向传播时 $ \\dfrac{\\mathrm{d} R}{\\mathrm{d}x}=-I $ ，这样可以直接利用内置的自动微分优雅实现对抗\n","permalink":"https://diefish1024.github.io/posts/literature-notes/dann/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e类似 GAN 的对抗训练思想\u003c/p\u003e\n\u003ch2 id=\"domain-adaptation\"\u003eDomain Adaptation\u003c/h2\u003e\n\u003cp\u003e给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \\eta: X\\to Y $ 使其在目标域上的目标风险\n$$ \n\nR_{D_{T}}(\\eta) = \\underset{(\\mathbf{x},y)\\sim D_{T}}{\\mathrm{Pr}}(\\eta(\\mathbf{x}) \\neq y)\n\n $$\n最小\u003c/p\u003e\n\u003ch4 id=\"domain-divergence\"\u003eDomain Divergence\u003c/h4\u003e\n\u003cp\u003e需要量化两个领域的“相似度”，从而引出了 \u003cstrong\u003eH- 散度\u003c/strong\u003e 的概念：\n$$ \n\nd_{\\mathcal{H}}(D_S, D_T) = 2 \\sup_{\\eta \\in \\mathcal{H}} \\left| \\Pr_{x \\sim D_S}[\\eta(x) = 1] - \\Pr_{x \\sim D_T}[\\eta(x) = 1] \\right|\n\n $$\n含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果\u003c/p\u003e\n\u003cp\u003e由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度\n$$ \n\n\\hat{d}_{\\mathcal{H}}(S, T) = 2 \\left(1 - \\min_{\\eta \\in \\mathcal{H}} \\left[ \\dfrac{1}{n}\\sum_{i=1}^n \\mathcal{I}[\\eta(x_i) = 0] + \\dfrac{1}{n'}\\sum_{i=n+1}^N \\mathcal{I}[\\eta(x_i) = 1] \\right] \\right)\n\n $$\n其中 $ \\mathcal{I}[\\cdot] $ 表示条件为真时为 1，否则为 0\u003c/p\u003e","title":"DANN"},{"content":"Abstract 问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱 方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构 核心模块： TGC：将 EEG 信号转换为时序图序列 RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心） TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心） TSO：输出分类/回归结果 成果：在多个公开数据集的广义跨被试任务上面超过了 baseline Introduction \u0026amp; Related Work 为什么 EEG 难以使用跨被试 (cross-subject) 的场景？\n个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同 低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……） 目标是学习一种跨被试共享、具有泛化能力的情绪表征\nGpaph Neural Networks 核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理 代表工作： ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层 GCN：通过局部一阶聚合近似光谱滤波 DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补 Temporal Context Learning 核心理念: 情绪是连续认知过程，EEG 信号中嵌入时序上下文信息 代表工作： LSTM / TCN / TESANet / Conformer / AMDET 局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息 EEG Emotion Recognition 核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征 代表工作： GCB-Net / TSception 局限性：没有关注长时序上下文信息 Method EmT 是一个端到端的框架，包含四大模块：\nraw EEG -\u0026gt; TGC -\u0026gt; 时序图 -\u0026gt; RMPG -\u0026gt; Tokens -\u0026gt; TCT -\u0026gt; 深层特征 -\u0026gt; TSO -\u0026gt; Result\nEEG-Temporal-Graph Representations (TGC) 目标：将连续的 EEG 信号转化为结构化的时序图序列\n图：每个“图”是指在一个短的窗口内大脑状态的数学表示，图中的每个节点对应一个 EEG 电极通道，节点的特征是该通道在 7 个不同的频段上的 rPSD 时序序列：序列是通过滑动窗口技术截取的一段较长的 EEG 数据里面切分成许多重叠的短的子片段，每个子片段生成的图所形成的序列 双层滑动窗口分段：为了捕捉不同时间尺度上的信息，TGC 采用了一种双层分段策略 首先将一次完整实验的 (trail) 的 EEG 数据，表示为 $ X \\in \\mathbb{R}^{c \\times L} $ （其中 $ c $ 为通道数，$ L $ 为总采样点数），通过一个较长的滑动窗口（长度为 $ l $ ，步长为 $ s $ ）分割成多个重叠的长片段 $ \\overline{X} \\in \\mathbb{R}^{c \\times l} $ 接着对于每一个长片段 $ \\overline{X} $ 使用一个更短的滑动窗口（长度为 $ l' $ ，步长为 $ s' $ ）将其分割为一系列子片段 $ \\tilde{X} \\in \\mathbb{R}^{c \\times l'} $ 这使得模型能够在一个长片段的标签下，观察到内部更精细的信号动态变化，为后续的 Transformer 模块捕捉时间上下文提供了基础 节点特征提取：对于每一个子片段 $ \\tilde{X} $，需要为其对应的图节点（即 EEG 通道）提取有意义的特征，论文选择了相对功率谱密度 (Relative PSD, rPSD) 作为节点属性 具体地，使用 welch\u0026rsquo;s method 计算每个 EEG 通道在七个经典频带上的 rPSD 这样，每个子片段 $ \\tilde{X} $ 都对应一个特征矩阵 $ F \\in \\mathbb{R}^{c \\times f} $，其中 $ f=7 $ 最终，一个长片段 $ \\overline{X} $ 被转换成一个按时间顺序排列的图序列 $ G_{T} = \\{\\mathcal{G}^{i}\\} \\in \\mathbb{R}^{seq \\times c \\times f} $，其中 $ seq $ 是子片段的数量。这个时间图序列就是 RMPG 模块的输入\nResidual Multiview Pyramid GCN (RMPG) 核心：解决传统 GNN“单一视角”问题，为时间图序列 $ G_{T} $​ 中的每一个图 $ \\mathcal{G}^i $ 学习一个丰富的、多层次的空间表征，并将其压缩成一个单一的 token ，以供后续的 TCT 模块处理\nRMPG 模块由一个基础的图编码器 $ \\Phi_{g}(\\cdot) $ 构成，文中采用了 ChebyNet 作为基础图编码器，对于给定特征输入 $ F^{m-1} $ 和邻接矩阵 $ A $ （通过拉普拉斯算子 $ \\hat{L} $ 转换） $$ \\Phi_{g}(F^{m}, A) = \\sigma\\left( \\sum_{k=0}^{K-1}\\theta_{k}^{m}T_{k}(\\hat{L})F^{m-1} - b^{m}\\right) $$ 其中 $ m $ 为 GCN 层的索引，$ \\sigma $ 为 ReLU 激活函数，$ \\theta $ 为参数，$ T_{k} $ 为 $ k $ 阶 Chebyshev 多项式\n多视图学习 (Multiview Learning) ：为了模拟情绪背后多种认知子过程驱动的不同大脑连接模式，RMPG 并非使用单一的图卷积网络，而是并行地使用了多个 GCN 分支，$ \\{ \\Phi_{g}^{0}(\\cdot), \\Phi_{g}^{1}(\\cdot), \\dots, \\Phi_{g}^{i}(\\cdot) \\} $ 每个分支都拥有一个独立的可学习邻接矩阵 $ A^{i} \\in \\mathbb{R}^{c \\times c} $ ，能在模型训练过程中通过梯度反向传播进行端到端的优化 这意味着每个 GCN 分支都能从数据中学习到一种独特的大脑功能连接“视图” 金字塔学习 (Pyramid Learning) ：为了捕捉不同尺度的空间信息，并行的 GCN 分支被设计成具有不同的深度（即 GCN 层数） 较浅的 GCN 能够有效地聚合全局的、跨脑区的功能连接信息，聚合远距离节点的信息而不过度平滑 较深的 GCN 能够更好地聚合局部邻域内的信息，在脑区内部形成一致的表征 GCN 的深度越深，其输出所代表的特征金字塔层级越高。 残差连接 (Residual) ：除了并行的 GCN 分支外，RMPG 还包含一个并行的线性残差分支。该分支直接对原始的时序图 $ G $ （或者对应的特征矩阵 $ F $ ）进行线性投影，不经过任何图卷积，从而保留最原始的节点信息，作为特征金字塔的“基座” 线性投影层 $ LP(\\cdot) $ 将扁平化的图表示投影到隐藏嵌入 $ H_{g}^{i} \\in \\mathbb{R}^{d_{g}} $ 堆叠来自不同层的 GCN 的并行输出，得到多金字塔视图嵌入 $$ \\{ H_{g}^{i} \\} = \\{ LP^{i}(\\Gamma(\\Phi_{g}^{i}(F, A^{i}))) \\} $$ 其中 $ \\Gamma(\\cdot) $ 是扁平操作，$ \\{ \\cdot \\} $ 是堆叠操作 特征融合与 Token 生成：最终，对于图序列中的每一个图 $ G_{i} $，其所有 GCN 视图的输出 $ \\{ H_{i}^{g} \\} $ 和残差基座 $ H_{g-\\text{base}} $​ 通过一个 mean fusion 操作合并成当前时间步 $ i $ 的最终 token $ s_{i} $ $$ s_{i} = \\text{mean}(\\{ H_{g-\\text{base}}, H_{g}^{0}, H_{g}^{1}, \\dots, H_{g}^{i} \\}) $$\n通过 RMPG 模块，输入的图序列 $ G_{T} $ 被高效地转化为一个 token 序列 $ S_{T}=\\{ s^{i} \\} \\in \\mathbb{R}^{seq \\times d_{g}} $ ，这个序列既蕴含了每个时刻丰富的多视图、多层次空间信息，又具备了适合 Transformer 处理的格式\nTemporal Contextual Transformer (TCT) 核心：接受由 RMPG 生成的 token 序列 $ S_{T} $ ，并利用 Transformer 的结构来高效捕捉这些 token 之间的时间依赖关系；与标准的 Transformer 不同，TCT 引入了两种为 EEG 情绪识别任务定制的 Token Mixer，分别用于分类和回归任务\nTCT 模块由多个堆叠的 Transformer Block 组成，对于输入 token 序列 $ Z^{m} $ （ $ Z^{0} = S_{T} $ ），每个 Block 的计算过程为 $$ Z^{m'} = \\text{TokenMixer}(\\text{Norm}(Z^{m})) + Z^{m} $$ $$ Z^{m+1} = \\text{MLP}(\\text{Norm}(Z^{m'})) + Z^{m'} $$ 其中 $ m $ 是层的索引，MLP 是带 ReLU 激活函数的两层感知机\n$ \\text{TokenMixer}_{\\text{clas}} $ for Classification Tasks 旨在捕捉随时间变化的长短时序上下文信息\n多头自注意力 (Multi-head Self-Attention, MSA) ：用于全局地关注序列中与整体情绪状态高度相关的部分 $$ \\text{Attn}(Q,K,V) = \\text{softmax}\\left( \\frac{QK^{T}}{\\sqrt{ d }} \\right)V $$ 并行应用多个注意力头（每个头有独立的 $ LP_h(\\cdot) $ 来生成 $ Q, K, V $），然后将所有头的输出拼接：$$ \\text{MSA}(S_{T}) = \\text{Concat}(\\text{Attn}(LP_0(S_{T})), ..., \\text{Attn}(LP_{n_{\\text{head}}-1}(S_{T}))) $$ 其中 $ S_{T} $ 是 token 序列，$ n_{\\text{head}} $ 是注意力头的数量 短期聚合层 (Short-Time Aggregation, STA) ： 基于“情绪短期连续而长期变化”的先验知识，STA 在 MSA 之后应用来学习短期的上下文信息 首先对 MSA 的输出 $ H_{\\text{attn}} \\in \\mathbb{R}^{n_{\\text{head}} \\times \\text{seq} \\times d_{\\text{head}}} $ 应用一个带有比例因子 $ \\alpha $ 的 Dropout 层 $ \\text{dp}(\\alpha) $ 接着，通过 Conv2D 聚合 $ n_{\\text{anchor}} $ 个时序近邻 Conv2D 的卷积核 $ K_{\\text{cnn}} $ 的尺寸为 $ (n_{\\text{anchor}}, 1) $，步长为 $ (1,1) $ 最后，卷积的输出会被 Reshape 并进行线性投影（$ W_{\\text{sta}} $） 可以描述为 $$ \\text{STA}(H_{\\text{attn}}) = \\text{Reshape}(\\text{Conv2D}(\\text{dp}(H_{\\text{attn}}), K_{\\text{cnn}})) W_{\\text{sta}} $$ $ \\text{Reshape}(\\cdot) $ 将维度从 $ (n_{\\text{head}},\\text{seq},d_{\\text{head}}) $ 转换为 $ (\\text{seq}, n_{\\text{head}} \\cdot d_{\\text{head}}) $ ， $ W_{\\text{sta}} \\in \\mathbb{R}^{n_{\\text{head}} \\cdot d_{\\text{head}} \\times d_g} $ 是投影权重矩阵 $ \\text{TokenMixer}_{\\text{Clas}} $ 最终由上述两个模块串联构成 $$ \\text{TokenMixer}_{\\text{clas}}(S_{T}) = \\text{STA}(\\text{MSA}(S_{T})) $$ $ \\text{TokenMixer}_{\\text{regr}} $ for Regression Tasks 旨在预测序列中情绪状态的连续变化\n不同于分类任务：分类任务的目标通常是从整个序列中提取几个核心特征来判断整体情绪状态，这通过 MSA 聚焦于重要部分是有效的；然而回归任务需要模型对序列中每个时步的连续情绪变化进行预测，因此不使用全局的 MSA，而是采用了一种基于 RNN 的混合器（ RNN family ） RNN Mixer ： RNN 结构天然适合处理连续序列的演变过程，能够更好地建模情绪值的平滑变化 经验性选择的两层双向 GRU (bi-directional GRU) 作为 $ \\text{TokenMixer}_{\\text{regr}} $ ，输出长度为 $ 2 \\times d_{\\text{head}} $ 计算过程为 $$ \\text{TokenMixer}_{\\text{regr}}(S_T) = \\text{RNNs}(\\text{LP}(S_T)) $$ 其中 $ LP(S_{T}) = S_{T}W_{v} $ ，把 token 序列投影为 $ V $ 值 TSO Module 头部接收来自不同 Token Mixer 的输出：用于分类的 $ S_{\\text{clas}} $ 和用于回归的 $ S_{\\text{regr}} $\n分类任务：对所有 token 进行 mean fusion ，再通过线性层得到最终 logits $$ \\hat{Y}_{\\text{clas}} = \\text{mean}(S_{\\text{clas}})W_{\\text{clas}} + b_{\\text{clas}} $$ 其中 $ S_{\\text{clas}} \\in \\mathbb{R}^{\\text{seq} \\times d_{\\text{head}}} $ ，$ W_{\\text{clas}} \\in \\mathbb{R}^{d_{\\text{head}} \\times d_{\\text{class}}} $ ，$ b_{\\text{clas}} \\in \\mathbb{R}^{n_{\\text{class}}} $ 回归任务：直接将每个时间步的 token 特征通过线性层，得到对应每个时刻的回归值 $$ \\hat{Y}_{\\text{regr}} = S_{\\text{regr}}W_{\\text{regr}} + b_{\\text{regr}} $$ 其中 $ S_{\\text{regr}} \\in \\mathbb{R}^{\\text{seq} \\times 2\\cdot d_{\\text{head}}} $ （双向），$ W_{\\text{regr}} \\in \\mathbb{R}^{2\\cdot d_{\\text{head}} \\times 1} $ ，$ b_{\\text{regr}} \\in \\mathbb{R} $ Experiment Datasets 使用 SEED, THU-EP, FACED, MAHNOB-HCI 四个公开数据集\n其中 SEED 数据集使用了 0.3-50 Hz 的带通滤波\nEEG-Temporal-Graph 长片段：窗口长度 $ l=20\\,\\mathrm{s} $ （即 $ 20 \\times f_{s} $ ），步长 $ s=4\\,\\mathrm{s} $ 子片段： 对于 SEED 和 THU-EP： $ l'=2\\,\\mathrm{s},s'=0.5\\,\\mathrm{s} $ 对于 FACED： $ l'=4\\,\\mathrm{s},s'=1\\,\\mathrm{s} $ Settings 数据划分： SEED：采用留一被试交叉验证，每次迭代一个被试的数据作为测试集，剩余数据中 80% 作为训练集，20% 作为验证集 THU-EP 和 FACED：采用留 $ n $ 被试交叉验证，其中 $ n_{\\text{THU-EP}}=8,n_{\\text{FACED}}=12 $ ，训练数据中 10% 作为验证集 分类任务：对 SEED、THU-EP 和 FACED 进行积极/消极情绪的二分类，THU-EP 和 FACED 的效价分数通过阈值 3.0 划分为高/低效价 回归任务：MAHNOB-HCI 并进行 LOSO 验证 Evaluation Metrics Classfication Accuracy：$$ \\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} $$ F1 Score：$$ \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}} = \\frac{\\text{TP}}{\\text{TP} + \\frac{1}{2}(\\text{FP} + \\text{FN})} $$ 其中 $ \\text{TP} $ 表示真阳性，$ \\text{TN} $ 表示真阴性，$ \\text{FP} $ 表示假阳性，$ \\text{FN} $ 表示假阴性\nRegression 给定预测值 $ \\hat{y} $ 和连续标签 $ y $ ：\n均方根误差 (RMSE) ：$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} (\\hat{y}_i - y_i)^2} $$ 皮尔逊相关系数 (PCC) ：$$ \\text{PCC} = \\frac{\\sigma_{\\hat{y}y}}{\\sigma_{\\hat{y}} \\sigma_y} = \\frac{\\sum_{i=0}^{N-1} (\\hat{y}_i - \\mu_{\\hat{y}})(\\hat{y}_i - \\mu_y)}{\\sqrt{\\sum_{i=0}^{N-1} (\\hat{y}_i - \\mu_{\\hat{y}})^2} \\sqrt{\\sum_{i=0}^{N-1} (y_i - \\mu_y)^2}} $$ 一致性相关系数 (CCC) ：$$ \\text{CCC} = \\frac{2\\sigma_{\\hat{y}y}}{\\sigma^2_{\\hat{y}} + \\sigma^2_y + (\\mu_{\\hat{y}} - \\mu_y)^2} $$ 其中 $ N $ 是向量中的元素数量，$ \\sigma_{\\hat{y}y} $ 是协方差，$ \\sigma_{\\hat{y}} $ 和 $ \\sigma_y $ 是标准差，$ \\mu_{\\hat{y}} $ 和 $ \\mu_y $ 是均值\nImplementation Details 模型的三种变体： Analyses Classification SEED： EmT-D 表现最佳，EmT-B 和 EmT-S 表现也良好，RGNN 表现第二佳 使用特征作为输入的模型通常优于直接使用 EEG 信号作为输入的模型，从时序特征而非直接特征学习通常能取得更好的性能（RGNN 除外），这表明了学习时序上下文信息的有效性，EmT 借助基于 GCN 的模块，能更好地学习空间信息 THU-EP / FACED： THU-EP：EmT-B 取得了最佳 F1 分数，Conformer 取得了最佳 ACC FACED：EmT-B 取得了第二佳 ACC 和最佳 F1 分数 由于类不平衡，F1 分数比 ACC 更重要，EmT-B 在这两个数据集上均取得了最高 F1 分数 与 SEED 不同，直接使用 EEG 作为输入的 baseline 模型表现更好，这可能因为这两个数据集的被试人数更多 Features： SEED (62 channels，15 subjects) ：EmT-D (8 层) 表现最佳，说明 Transformer 层数越多，学到的空间信息越丰富 THU-EP 和 FACED (32 channels，more subjects) ：EmT-B (4 层) 表现更好，通道数少且被试间变异性大时，更深的模型（EmT-D）容易过拟合 Regression 在 MAHNOB-HCI 数据集上，EmT-Regr (LP+LSTM) 取得了最低的 RMSE，而 EmT-Regr (LP+GRU) 取得了最佳的 PCC 和 CCC 使用 MSA 作为 Token Mixer 时，模型性能急剧下降，甚至低于所有基线模型 这表明对于回归任务，融合所有片段的信息至关重要，而 RNN 的顺序信息融合能力比 MSA 更适合建模连续的情绪变化 Ablation Study Effect of EEG Features Effect of The Depth and Width of GCNs in RMPG Depth：增加 GCN 层的数量会导致性能显著下降，这与更深 GCN 中存在的过平滑问题一致 Width：宽度从 8 增加到 32 时，性能呈正相关；当宽度进一步增加时，性能下降，这可能是由更大的模型尺寸导致的过拟合 Effect of The Number of TCT Blocks Classfication (SEED) ：TCT 块的数量从 2 增加到 8 时，ACC 和 F1 分数均显著提高，这表明增加 TCT 块数能增强模型捕捉时序上下文信息的能力，从而提升分类性能 Regression (MAHNOB-HCI) ：TCT 块的数量对性能指标几乎没有影响 Visualization Learned Connections 在 SEED 数据集上学习到的两种不同连接模式的可视化证据 在 SEED 数据集上，两个可学习的邻接矩阵揭示了情绪认知过程中不同的连接模式：\n(a) 中主要关注额叶、顶叶和颞叶区域之间的连接，这些区域与心理注意力密切相关 (b) 中则包括额叶、颞叶和顶叶区域之间的互动（与情绪相关），以及枕叶和顶叶区域的互动（与视觉过程相关，因为刺激为视频） Learned Temporal Contextual Information Classfication (FACED) Regression 分类任务在 TCT 块之前，特征随时间变化，TCT 块之后，激活变得更加一致；这可能是因为自注意力机制关注与整体情绪状态高度相关的部分，而 STA 层通过聚合邻近的时序信息来平滑波动 回归任务：特征空间也显示出时序变化；与分类不同，回归特征并非简单平滑，形成了更复杂的表示 总结：TCT 块处理分类和回归任务的方式不同：分类中，特征被平滑以增强可分离性；回归中，RNN Token Mixer 保留了时序变化，从而实现连续情绪预测 ","permalink":"https://diefish1024.github.io/posts/literature-notes/emt/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e问题\u003c/strong\u003e：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e方案\u003c/strong\u003e：提出 \u003cstrong\u003eEmotion Transformer (EmT)\u003c/strong\u003e ，为 Graph-Transformer 混和架构\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e核心模块\u003c/strong\u003e：\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eTGC\u003c/strong\u003e：将 EEG 信号转换为时序图序列\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRMPG\u003c/strong\u003e：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTCT\u003c/strong\u003e：使用任务自适应的 Transformer，学习 token 序列上下文（核心）\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTSO\u003c/strong\u003e：输出分类/回归结果\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e成果\u003c/strong\u003e：在多个公开数据集的广义跨被试任务上面超过了 baseline\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction--related-work\"\u003eIntroduction \u0026amp; Related Work\u003c/h2\u003e\n\u003cp\u003e为什么 EEG 难以使用跨被试 (cross-subject) 的场景？\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e个体差异\u003c/strong\u003e：不同被试生理结构和认知策略差异，导致 EEG 模式不同\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e低信噪比\u003c/strong\u003e：EEG 信号容易受到外源噪声干扰（肌电、眼电……）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e目标是学习一种\u003cstrong\u003e跨被试共享\u003c/strong\u003e、具有\u003cstrong\u003e泛化能力\u003c/strong\u003e的情绪表征\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"gpaph-neural-networks\"\u003eGpaph Neural Networks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e核心思想\u003c/strong\u003e：EEG 数据具有非欧图结构，适合使用 GNN 来处理\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e代表工作\u003c/strong\u003e：\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eChebyNet\u003c/strong\u003e：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGCN\u003c/strong\u003e：通过局部一阶聚合近似光谱滤波\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDGCNN / RGNN\u003c/strong\u003e：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有\u003cstrong\u003e局限性\u003c/strong\u003e；而 EmT 通过\u003cstrong\u003e多视图可学习邻接矩阵\u003c/strong\u003e和\u003cstrong\u003e时序图\u003c/strong\u003e来弥补\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"temporal-context-learning\"\u003eTemporal Context Learning\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e核心理念\u003c/strong\u003e: 情绪是连续认知过程，EEG 信号中嵌入时序上下文信息\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e代表工作\u003c/strong\u003e：\n\u003cul\u003e\n\u003cli\u003eLSTM / TCN / TESANet / Conformer / AMDET\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e局限性\u003c/strong\u003e：这些方法通常从扁平化的 EEG 特征向量学习，可能\u003cstrong\u003e未能有效学习空间关系\u003c/strong\u003e；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"eeg-emotion-recognition\"\u003eEEG Emotion Recognition\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e核心理念\u003c/strong\u003e：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e代表工作\u003c/strong\u003e：\n\u003cul\u003e\n\u003cli\u003eGCB-Net / TSception\u003c/li\u003e\n\u003cli\u003e局限性：没有关注长时序上下文信息\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003eEmT 是一个端到端的框架，包含四大模块：\u003c/p\u003e","title":"EmT"},{"content":"推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\n1. What is KV Cache? KV Cache，全称 Key-Value Cache，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是缓存并重用在注意力机制中计算得到的 Key (K) 和 Value (V) 向量。\n2. Transformer Attention Mechanism Review 要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\n每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\nQ 向量：代表当前 token 的“查询”信息 K 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配 V 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出 自注意力机制的计算过程为以下步骤：\n计算 Query 与所有 Key 的点积，得到注意力分数 将注意力分数进行缩放，除以 $ \\sqrt{d_k} $（$ d_k $ 是 Key 向量的维度) 对缩放后的分数进行 Softmax，将其转换为注意力权重，表示每个 token 对当前 token 的重要性 将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出 公式为： $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$ 其中矩阵 $ Q,K,V \\in \\mathbb{R}^{L \\times d} $ ，$ L $ 为当前上下文长度\n（处于简洁性的考虑，忽略了 Causal Mask ，实际上 $ QK^{T} $ 应该 Mask 成下三角矩阵来强制不能看到序列未来的信息）\n3. The Problem KV Cache Solves 在大型语言模型中，当模型以自回归方式生成文本时（每次生成一个新 token，并将其添加到输入序列中，然后根据整个序列生成下一个 token），会遇到一个效率问题：\n假设我们要生成“中华人民”\n输入：“中” 模型计算“中”的 $ Q, K, V $ 计算 attention ，生成“华” 输入：“中华” 模型再次计算“中”和“华”的 $ Q, K, V $ 计算 attention ，生成“人” 输入：“中华人” 模型再次计算“中”、“华”和“人”的 $ Q, K, V $ 计算 attention ，生成“民” 可以看到，在每一步生成新 token 时，都需要重新计算之前已经处理过的所有 token 的 $ K $ 和 $ V $ 向量。这种重复计算在序列较长时会消耗大量的计算资源和时间，效率低下。\n4. How KV Cache Works 根据上面分析得到的问题，很容易想到 KV Cache 的核心思想：将已经计算过的 Key 和 Value 向量缓存起来，在后续的生成步骤中直接重用，而不是重新计算。\n以生成“中华人民”为例，使用 KV Cache 的流程如下：\n输入：“中” 计算“中”的 $ K_1, V_1 $ 将 $ K_1, V_1 $ 存入 KV Cache 使用 $ Q_1, K_1, V_1 $ 计算 attention ，生成“华” 输入：“华”（当前 token 只有“华”，但注意力要关注整个序列“中华”） 计算“华”的 $ K_2, V_2 $ 将 $ K_2, V_2 $ 添加到 KV Cache。此时 KV Cache 包含 $ [K_1, K_2] $ 和 $ [V_1, V_2] $ 使用当前 $ Q_2 $ 和缓存中的 $ [K_1, K_2], [V_1, V_2] $ 计算 attention ，生成“人” 输入：“人” 计算“人”的 $ K_3, V_3 $ 将 $ K_3, V_3 $ 添加到 KV Cache。此时 KV Cache 包含 $ [K_1, K_2, K_3] $ 和 $ [V_1, V_2, V_3] $ 使用当前 $ Q_3 $ 和缓存中的 $ [K_1, K_2, K_3], [V_1, V_2, V_3] $ 计算 attention ，生成“民” 通过这种方式，每一步只需要计算当前新生成 token 的 $ K, V $ 向量，而无需重新计算之前所有 token 的 $ K, V $。\n5. Why Not QKV Cache? 可能会好奇，既然 K 和 V 都需要缓存，为什么不也缓存 Q 呢？也就是说，为什么是 KV Cache 而不是 QKV Cache？\n原因在于 Q 向量的性质：\nQ 向量是用来“查询”当前 token 与序列中其他 token 的相关性的。在自回归生成过程中，每一步生成一个新的 token，这个新 token 对应的 Query 向量是新的，它基于当前步的隐藏状态计算得出。换句话说，每次生成新 token 时，其对应的 $ Q $ 向量都是独一无二的，并且需要重新计算以反映最新的生成上下文。 K 和 V 向量则代表了序列中每个 token 的“内容”信息。对于已经处理过的 token，它们的 $ K $ 和 $ V $ 向量一旦计算出来，其内容信息就是固定不变的。因此，这些 $ K $ 和 $ V $ 向量可以直接被缓存并反复使用，而无需重新计算。 因此，不缓存 Q 是因为它在每一步都是一个新的计算结果；而缓存 K 和 V 则可以显著减少重复计算，从而提高效率。\n6. KV Cache in Attention Mechanism 在数学上，当使用 KV Cache 进行自回归解码时，注意力公式中的 $ K $ 和 $ V $ 矩阵会随着生成过程的进行而不断增长。\n假设我们正在生成第 $ t $ 个 token。\n当前 token 的 Q 向量是 $ Q_t $ ，这是一个行向量，代表当前第 $ t $ 个 token 的 Query ，维度为 $ 1 \\times d_k $ K 矩阵 $ K_{\\text{cached}} $ 将包含从第一个 token 到第 $ t $ 个 token 的所有 K 向量： $ K_{\\text{cached}} = [K_1^T, K_2^T, \\dots, K_t^T]^T $ ，维度为 $ t \\times d_k $ V 矩阵 $ V_{\\text{cached}} $ 将包含从第一个 token 到第 $ t $ 个 token 的所有 V 向量： $ V_{\\text{cached}} = [V_1^T, V_2^T, \\dots, V_t^T]^T $ 。其维度为 $ t \\times d_v $ 那么，第 $ t $ 个 token 的注意力计算变为： $$ \\text{Attention}_{t}(Q_t, K_{\\text{cached}}, V_{\\text{cached}}) = \\text{softmax}\\left(\\frac{Q_t K_{\\text{cached}}^T}{\\sqrt{d_k}}\\right)V_{\\text{cached}} $$ 其中\n$ Q_t K_{\\text{cached}}^T $ 是一个 $ 1 \\times t $ 的行向量，代表当前 Query 与所有历史 Key 的相关性分数 $ \\text{softmax} $ 操作将这个 $ 1 \\times t $ 的向量转化为注意力权重 这个 $ 1 \\times t $ 的注意力权重向量再与 $ V_{\\text{cached}} $ 矩阵（维度 $ t \\times d_v $）相乘，得到最终的注意力输出，维度是 $ 1 \\times d_v $ 每次生成新的 token $ t+1 $ 时，我们只需要计算新的 $ Q_{t+1} $，将新计算的 $ K_{t+1} $ 和 $ V_{t+1} $ 拼接到 $ K_{\\text{cached}} $ 和 $ V_{\\text{cached}} $ 末尾，形成 $ K'_{\\text{cached}} = \\text{concat}(K_{\\text{cached}}, K_{t+1}) $ 和 $ V'_{\\text{cached}} = \\text{concat}(V_{\\text{cached}}, V_{t+1}) $\n7. Limitations and Considerations 尽管 KV Cache 带来了巨大的性能提升，但也存在一些问题：\n内存占用：KV Cache 需要存储所有已处理 token 的 Key 和 Value 向量。对于大型模型和长上下文序列，这些缓存可能非常大，导致显存（GPU Memory）成为瓶颈。 上下文长度限制：由于内存限制，KV Cache 会限制模型能够处理的最大上下文长度。一旦达到内存上限，就需要采取策略来管理缓存，例如丢弃最早的 Key/Value 对（类似于循环缓冲区），但这可能会影响模型对长距离依赖的理解。 Summary KV Cache 是 Transformer 模型在自回归推理过程中非常重要的一种优化技术。通过缓存并重用已经计算过的 Key 和 Value 向量，它极大地减少了重复计算，从而显著提升了大型语言模型的生成速度。\nReferences KV Cache 原理讲解 （Bilibili） 注意：此视频内容存在部分错误 看图学KV Cache（知乎） 为什么没有Q Cache（知乎） ","permalink":"https://diefish1024.github.io/posts/ai-infra/kv-cache-%E5%85%A5%E9%97%A8/","summary":"\u003cp\u003e推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\u003c/p\u003e\n\u003ch3 id=\"1-what-is-kv-cache\"\u003e1. What is KV Cache?\u003c/h3\u003e\n\u003cp\u003eKV Cache，全称 \u003cstrong\u003eKey-Value Cache\u003c/strong\u003e，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是\u003cstrong\u003e缓存\u003c/strong\u003e并\u003cstrong\u003e重用\u003c/strong\u003e在注意力机制中计算得到的 \u003cstrong\u003eKey (K)\u003c/strong\u003e 和 \u003cstrong\u003eValue (V)\u003c/strong\u003e 向量。\u003c/p\u003e\n\u003ch3 id=\"2-transformer-attention-mechanism-review\"\u003e2. Transformer Attention Mechanism Review\u003c/h3\u003e\n\u003cp\u003e要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\u003c/p\u003e\n\u003cp\u003e每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ 向量：代表当前 token 的“查询”信息\u003c/li\u003e\n\u003cli\u003eK 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配\u003c/li\u003e\n\u003cli\u003eV 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e自注意力机制的计算过程为以下步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e计算 Query 与所有 Key 的点积，得到\u003cstrong\u003e注意力分数\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e将注意力分数进行缩放，除以 $ \\sqrt{d_k} $（$ d_k $ 是 Key 向量的维度)\u003c/li\u003e\n\u003cli\u003e对缩放后的分数进行 Softmax，将其转换为\u003cstrong\u003e注意力权重\u003c/strong\u003e，表示每个 token 对当前 token 的重要性\u003c/li\u003e\n\u003cli\u003e将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e公式为：\n$$ \n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\n $$\n其中矩阵 $ Q,K,V \\in \\mathbb{R}^{L \\times d} $ ，$ L $ 为当前上下文长度\u003c/p\u003e","title":"KV Cache 入门"},{"content":"Introduction TTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征\nProblem Setting 考虑一个回归模型 $ f_\\theta: \\mathcal{X} \\to \\mathbb{R} $，可以进一步分解为特征提取器 $ g_\\phi: \\mathcal{X} \\to \\mathbb{R}^D $（从输入 $ \\mathcal{X} $ 提取 $ D $ 维特征 $ z $）和线性回归器 $ h_\\psi(z) = w^T z + b $（或者 $ h_{\\psi}(z)=Wz+b $）\n$ f_\\theta $ 首先在一个有标签的源数据集 $ S = \\{(x_i, y_i)\\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样\n目标是使用一个无标签的目标数据集 $ T = \\{x_j\\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\\theta $ 到目标域\n我们假设存在 covariate shift ，这意味着：\n输入数据的分布在源域和目标域之间是不同的：$ p_s(x) \\neq p_t(x) $ 但给定输入后，输出的条件分布是相同的：$ p_s(y|x) = p_t(y|x) $ Test-time Adaptation for Regression Basic Idea: Feature Alignment 朴素实现：\n计算源域特征统计量：在源域训练后，计算源域特征的均值 $ \\mu^s $ 和元素级方差 $ \\sigma^{s2} $ $$ \\mu^s = \\frac{1}{N_s} \\sum_{i=1}^{N_s} z_i^s, \\quad \\sigma^{s2} = \\frac{1}{N_s} \\sum_{i=1}^{N_s} (z_i^s - \\mu^s) \\odot (z_i^s - \\mu^s) \\quad \\text{(1)} $$ 其中 $ z_i^s = g_\\phi(x_i) $ 是源特征，$ N_s $ 是源数据样本数，$ \\odot $ 表示元素级乘积\n目标域特征统计量：在目标域，对每个迷你批次（mini-batch）$ B = \\{x_j\\}_{j=1}^{N_B} $，计算其特征均值 $ \\hat{\\mu}^t $ 和方差 $ \\hat{\\sigma}^{t2} $，计算方式与公式 (1) 类似\n对齐损失函数：使用 KL 散度来衡量两个对角高斯分布 $ N(\\mu^s, \\sigma^{s2}) $ 和 $ N(\\hat{\\mu}^t, \\hat{\\sigma}^{t2}) $ 之间的差异，并最小化该差异。 $$ L_{TTA} (\\phi) = \\frac{1}{2} \\sum_{d=1}^D \\left\\{ D_{KL} (N(\\mu^s_d, \\sigma^s_{d}{}^2)||N(\\hat{\\mu}^t_d, \\hat{\\sigma}^t_{d}{}^2)) + D_{KL} (N(\\hat{\\mu}^t_d, \\hat{\\sigma}^t_{d}{}^2)||N(\\mu^s_d, \\sigma^s_{d}{}^2)) \\right\\} \\quad \\text{(2)} $$ 这里的 $ d $ 表示向量的第 $ d $ 个元素。之所以使用双向的 KL 散度，是为了经验上获得更好的结果\n一维高斯 KL 散度公式： $$ D_{KL} (N(\\mu_1, \\sigma_1^2)||N(\\mu_2, \\sigma_2^2)) = \\dfrac{\\left[ \\log(\\sigma_2^2/\\sigma_1^2) + \\dfrac{(\\mu_1 - \\mu_2)^2 + \\sigma_1^2}{\\sigma_2^2} - 1 \\right]}{2} \\quad \\text{(3)} $$\n朴素对齐的问题：\n回归模型特征倾向于分布在一个小型的子空间中，许多特征维度方差为零或接近零 公式 (3) 中涉及到方差在分母上，使得这种朴素对齐在面对零方差维度时变得不稳定 对所有维度“一视同仁”地对齐不适用于回归任务的特性，因为许多维度对最终输出影响很小 Significant-subspace Alignment SSA 的三个步骤：\n子空间检测 (Subspace detection)：\n在源数据集 $ S $ 上进行训练后，检测源特征分布所在的子空间。不计算每个维度的方差，而是计算协方差矩阵： $$ \\Sigma^s = \\frac{1}{N_s} \\sum_{i=1}^{N_s} (z_i^s - \\mu^s) (z_i^s - \\mu^s)^T \\quad \\text{(4)} $$ 其中 $ \\mu^s $ 是源特征的均值向量（同理 (1)） 基于 PCA 的思想，通过对 $ \\Sigma^s $ 进行特征分解，得到特征向量 $ v_d^s $ 和对应的特征值 $ \\lambda_d^s $ 选取前 K 个最大的特征值 $ \\lambda_1^s, \\dots, \\lambda_K^s $ 及其对应的源基向量 $ v_1^s, \\dots, v_K^s $ 来定义源子空间，这些基向量张成的子空间代表了源特征数据最有代表性和最重要的变化方向 维度加权 (Dimension weighting)：\n考虑到回归模型 $ h_\\psi(z)=w^T z + b $，子空间维度 $ v_d^s $ 对最终输出的影响由 $ w^T v_d^s $ 决定（即特征向量与回归器权重向量的点积） 为了优先对齐那些对输出影响更大的子空间维度，为每个子空间维度 $ d $ 定义权重 $ a_d $： $$ a_d = 1 + |w^T v_d^s| \\quad \\text{(5)} $$ 这个权重 $ a_d $ 会在对应的子空间基方向对输出有较大影响时值更大（最小为 1）。 特征对齐 (Feature alignment)：\n这一步在目标域进行。对于目标域的迷你批次 $ B $，首先将目标特征 $ z^t = g_\\phi(x^t) $ 投影到源子空间。 $$ \\tilde{z}^t = V_s^T (z^t - \\mu^s) \\quad \\text{(6)} $$ 其中 $ V_s = [v_1^s, \\dots, v_K^s] \\in \\mathbb{R}^{D \\times K} $ 是由前 K 个源基向量构成的矩阵，$ \\tilde{z}^t \\in \\mathbb{R}^K $ 是投影后的目标特征。 然后，计算投影后目标特征的迷你批次均值 $ \\tilde{\\mu}^t $ 和方差 $ \\tilde{\\sigma}^{t2} $ （同理公式 (1) ） 最后，使用结合子空间检测和维度加权的新损失函数来最小化目标特征分布与源特征分布在子空间中的差异。源域投影后的均值是 0，方差是其特征值 $ \\Lambda^s = [\\lambda_1^s, \\dots, \\lambda_K^s] $。 $$ \\begin{align}L_{TTA}(\\phi) = \u0026 \\frac{1}{2} \\sum_{d=1}^K a_d \\left\\{ D_{KL} (N(0, \\lambda^s_d)||N(\\tilde{\\mu}^t_d, \\tilde{\\sigma}^t_{d}{}^2)) + D_{KL} (N(\\tilde{\\mu}^t_d, \\tilde{\\sigma}^t_{d}{}^2)||N(0, \\lambda^s_d)) \\right\\} \\\\ = \u0026 \\sum_{d=1}^K a_d \\left\\{ \\frac{(\\tilde{\\mu}^t_d)^2 + \\lambda^s_d}{2\\tilde{\\sigma}^t_{d}{}^2} + \\frac{(\\tilde{\\mu}^t_d)^2 + \\tilde{\\sigma}^t_{d}{}^2}{2\\lambda^s_d} - 1 \\right\\} \\quad \\text{(7)} \\end{align} $$ 其中 $ a_d $ 是维度权重，$ \\lambda_d^s $ 是源域子空间的第 $ d $ 个特征值，$ \\tilde{\\mu}_d^t $ 和 $ \\tilde{\\sigma}_{d}{}^2 $ 是投影后的目标特征在第 $ d $ 个维度上的均值和方差 伪代码：\n输入：预训练好的源模型 $ f_\\theta $、源基向量 $ V_s $、源均值 $ \\mu^s $、源方差 $ \\Lambda^s $、目标数据集 $ T $ 输出：适应后的模型 $ f_\\phi^t $ 步骤： 计算源子空间中每个维度的权重 $ a_d $ 对于目标数据集 $ T $ 中的每个 mini batch $ \\{x\\}_i^B $： 提取目标特征 $ z = g_\\phi(x) $。 将目标特征投影到源子空间 $ \\tilde{z} $ 计算投影后目标特征的均值 $ \\tilde{\\mu}^t $ 和方差 $ \\tilde{\\sigma}^{t2} $ 更新特征提取器 $ g_\\phi $ 以最小化损失函数 $ L_{TTA}(\\phi) $ 重复直到收敛。 对角高斯分布的合理性 为什么假设特征分布为对角高斯分布是合理的：\n中心极限定理：当特征被投影到子空间后，如果原始特征维度 $ D $ 足够大，根据中心极限定理，投影后的特征分布会倾向于高斯分布。 PCA 的去相关性：由于子空间检测使用了 PCA，投影到主成分上的特征是去相关的，这意味着不同维度之间是独立的，这使得对角高斯分布的假设（即各维度独立）变得合理。 Appendix A. LIMITATION：SSA 假设是协变量偏移，即 $ p(y|x) $ 不变，未来工作将考虑 $ p(y|x) $ 变化的情况\nB. EVALUATION METRIC：R²接近 1 表示模型拟合效果好 $$ R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} \\quad \\text{(10)} $$ 其中 $ \\hat{y}_i $ 是预测值，$ y_i $ 是真实值，$ \\bar{y} $ 是真实值的平均值。\nD. ADDITIONAL EXPERIMENTAL RESULTS：\nD.1 特征对齐的度量：比较了 KL 散度、2WD 和 L1 范数作为特征对齐损失的效果，结果显示 KL 散度结合子空间检测（SSA）表现最佳。 公式 (11)：2-Wasserstein Distance for Gaussians $$ W_2^2 (N(\\mu_1, \\sigma_1^2), N(\\mu_2, \\sigma_2^2)) = (\\mu_1 - \\mu_2)^2 + (\\sigma_1 - \\sigma_2)^2 $$ 公式 (12)：L1 Norm of Statistics $$ L_1 (N(\\mu_1, \\sigma_1^2), N(\\mu_2, \\sigma_2^2)) = |\\mu_1 - \\mu_2| + |\\sigma_1 - \\sigma_2| $$ 公式 (13)：SSA Loss with 2WD $$ L_{TTA-2WD} = \\sum_{d=1}^K a_d \\left\\{ (\\tilde{\\mu}^t_d)^2 + (\\tilde{\\sigma}^t_d - \\sqrt{\\lambda^s_d})^2 \\right\\} $$ 公式 (14)：SSA Loss with L1 Norm $$ L_{TTA-L1} = \\sum_{d=1}^K a_d \\left\\{ |\\tilde{\\mu}^t_d| + |\\tilde{\\sigma}^t_d - \\sqrt{\\lambda^s_d}| \\right\\} $$ D.2 特征可视化：通过 PCA 和 UMAP 等降维技术可视化了源域和目标域特征分布（图 4-5），直观地展示了 SSA 如何成功地将目标特征分布拉近源域。 D.3 原始特征维度对子空间的影响：分析了原始特征维度对子空间的重要性。 公式 (15)：Gradient Norm $ s_d $ $$ s_d = ||\\frac{\\partial \\tilde{z}}{\\partial z_d}||_2 = ||(V_s^T)_d||_2 = ||[v_{1,d}^s, \\dots, v_{K,d}^s]||_2, $$ 其中 $ (V_s^T)_d $ 是 $ V_s^T $ 的第 $ d $ 行。 发现：回归模型的特征子空间确实受许多原始特征维度影响很小（图 6），这进一步确认了子空间检测的必要性。 D.4 附加消融实验：进一步证实了子空间检测对于 SSA 性能的重要性（表 13-14）。 D.5 Vision Transformer 实验：在 Vision Transformer 上验证了 SSA 的有效性（表 15-16），表明该方法对不同模型架构也适用。 D.6 多任务回归模型：将 SSA 应用于多任务回归，模型同时输出多个预测值（如头部姿态的俯仰、偏航、滚转角度），结果表明 SSA 同样有效（表 17）。 D.7 与分类 TTA 结合：探索了 SSA 与分类 TTA 结合的可能性（表 18-20）。 D.8 超参数敏感性：分析了学习率和批次大小等超参数对 SSA 性能的影响（表 21-26），发现 SSA 在典型参数范围内表现稳定。 D.9 额外结果：提供了 MAE 等其他指标的性能数据（表 27-28）。 D.10 在线设置：SSA 在分批在线（batched online）设置下也表现出色（表 29-31）。 ","permalink":"https://diefish1024.github.io/posts/literature-notes/ssa/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eTTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征\u003c/p\u003e\n\u003ch2 id=\"problem-setting\"\u003eProblem Setting\u003c/h2\u003e\n\u003cp\u003e考虑一个回归模型 $ f_\\theta: \\mathcal{X} \\to \\mathbb{R} $，可以进一步分解为\u003cstrong\u003e特征提取器\u003c/strong\u003e $ g_\\phi: \\mathcal{X} \\to \\mathbb{R}^D $（从输入 $ \\mathcal{X} $ 提取 $ D $ 维特征 $ z $）和\u003cstrong\u003e线性回归器\u003c/strong\u003e $ h_\\psi(z) = w^T z + b $（或者 $ h_{\\psi}(z)=Wz+b $）\u003c/p\u003e\n\u003cp\u003e$ f_\\theta $ 首先在一个有标签的\u003cstrong\u003e源数据集\u003c/strong\u003e $ S = \\{(x_i, y_i)\\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样\u003c/p\u003e\n\u003cp\u003e目标是使用一个\u003cstrong\u003e无标签的\u003c/strong\u003e目标数据集 $ T = \\{x_j\\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\\theta $ 到目标域\u003c/p\u003e\n\u003cp\u003e我们假设存在 \u003cstrong\u003ecovariate shift\u003c/strong\u003e ，这意味着：\u003c/p\u003e","title":"SSA"},{"content":"Method Problem Set EEG 数据 $ \\{ X_{s,l}^{i},y_{s,l}^{i} \\}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类\nSource Model Training 对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异\nEA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值 $$ R_{s,l} = \\dfrac{1}{n}\\sum_{i=1}^{n} X_{i}(X_{i})^{T} \\implies \\bar{X}_{i} = R_{s,l}^{-1/2}X_{i} $$ 之后再整合经过对齐的受试者数据，形成“源域”\n在整合后的数据上独立训练 $ M $ 个模型\nIncremental EA on Target Data 对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据\nTarget Label Prediction 用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $\n新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}' $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}') $\n之后结合这 $ M $ 个概率向量来获得最终的预测标签 $ \\hat{y}_{a} $\n$ a\\leq M $ 数据量较少：直接对所有模型的预测向量平均 $ a\u003eM $ 数据量较多：使用谱元学习器对各个模型进行加权平均，根据历史表现（预测的协方差矩阵）分配不同的权重 Target Model Update 在数据量足够以后（$ a\u003eB $）使用一个滑动批次的数据更新模型，在此之前模型不变\n组合损失函数： $$ L_{M} = L_{CEM}(f_{m};\\{ X'_{i} \\}_{i=a-B+1}^{a}) + L_{MDR}(f_{m};\\{ X'_{i} \\}_{i=a-B+1}^{a}) $$ 有两个部分\n1) Conditional Entropy Minimization 条件熵最小化\n使分类边界更加清晰 通过最小化每个预测的条件熵（使用温度缩放因子 $ T $ 进行校准），使模型倾向于输出接近 0 或 1 的概率 2) Adaptive Marginal Distribution Regularization 自适应边缘分布正则化\n防止出现所有数据都在单类别和对错误结果过于自信的不良结果 计算当前批次每个类别的平均预测概率 $ p_{k} $ 通过设置阈值得到伪标签，估计目标域的类别评论 $ z_{k} $ 校准平均预测概率 $ q'_{k} $ $$ q_{k} = \\dfrac{p_{k}}{c+z_{k}},\\quad q'_{k} = \\dfrac{q_{k}}{\\sum q} $$ $ L_{MDR} = \\sum_{k=1}^{K}q'_{k}\\log q'_{k} $ （采用负熵的形式） Complete T-TIME Algorithm 先预测，后台并行地更新模型\nExperiment 使用三个运动想象数据集\n每次把一个受试者的数据作为目标域，其余作为源域\nClassification Accuracies on Balanced Classes 过于复杂的算法由于数据不足，性能反而下降 基于熵的方法普遍表现良好，MCC 在离线迁移学习中表现最好 T-TIME 在所有在线迁移学习算法中表现最佳，并且其性能与表现最佳的离线迁移学习方法相当 Classification Performance Under Class-Imbalance 使用随机移除数据来创建不平衡数据集\n传统方法表现较弱 T-TIME 表现突出 ","permalink":"https://diefish1024.github.io/posts/literature-notes/t-time/","summary":"\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003ch3 id=\"problem-set\"\u003eProblem Set\u003c/h3\u003e\n\u003cp\u003eEEG 数据 $ \\{ X_{s,l}^{i},y_{s,l}^{i} \\}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类\u003c/p\u003e\n\u003ch3 id=\"source-model-training\"\u003eSource Model Training\u003c/h3\u003e\n\u003cp\u003e对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异\u003c/p\u003e\n\u003cp\u003eEA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值\n$$ \n\nR_{s,l} = \\dfrac{1}{n}\\sum_{i=1}^{n} X_{i}(X_{i})^{T} \\implies \\bar{X}_{i} = R_{s,l}^{-1/2}X_{i}\n\n $$\n之后再整合经过对齐的受试者数据，形成“源域”\u003c/p\u003e\n\u003cp\u003e在整合后的数据上独立训练 $ M $ 个模型\u003c/p\u003e\n\u003ch3 id=\"incremental-ea-on-target-data\"\u003eIncremental EA on Target Data\u003c/h3\u003e\n\u003cp\u003e对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据\u003c/p\u003e\n\u003ch3 id=\"target-label-prediction\"\u003eTarget Label Prediction\u003c/h3\u003e\n\u003cp\u003e用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $\u003c/p\u003e\n\u003cp\u003e新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}' $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}') $\u003c/p\u003e","title":"T-TIME"},{"content":"Setting Fully Test-Time Adaptation 是一种独特的模型适应设定。在此设定下，模型 $ f_\\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \\theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。\nFTT-Adaptation 与以下方法不同：\nFine-tuning：需要目标标签进行重新训练。 Domain Adaptation：需要源数据和目标数据进行联合训练。 Test-Time Training (TTT)：需要修改训练过程并共同优化有监督及自监督损失。 相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。\nMethod 论文的核心贡献是提出了 Tent 方法，其核心思想是通过最小化测试熵（Test Entropy Minimization）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。\nEntropy Objective Tent 的测试时目标函数是最小化模型预测 $ \\hat{y} = f_\\theta(x^t) $ 的熵 $ H(\\hat{y}) $。论文中使用的香农熵计算公式如下：\n$$ H(\\hat{y}) = - \\sum_c p(\\hat{y}_c) \\log p(\\hat{y}_c) $$ 其中， $ p(\\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。\n最小化熵促使模型输出更“尖锐”或更“确定”的预测分布。 优势：熵是一种无监督目标，仅依赖于模型预测，不需要真实标签。最小化熵与减少预测误差和数据漂移之间存在内在联系，因为更确定的预测通常意味着更正确的预测。 Modulation Parameters Tent 不直接修改原始模型的全部参数 $ \\theta $。相反，它仅更新模型内部归一化层（如Batch Normalization layers）中的线性且低维度的仿射变换参数：尺度参数 $ \\gamma $ 和偏移参数 $ \\beta $。\n这一选择的理由是：这些参数只占模型总参数的极小部分（\u0026lt;1%），优化效率高且稳定。 特征调制过程包含两个步骤： 1.Normalization (标准化)：根据当前批次测试数据的均值 $ \\mu $ 和标准差 $ \\sigma $ 来标准化特征 $ x $，即 $ \\hat{x} = (x - \\mu)/\\sigma $。这里的 $ \\mu, \\sigma $ 是在测试时从当前批次数据中估计的。 2.Transformation (仿射变换)：对标准化后的特征 $ \\hat{x} $ 应用仿射变换，即 $ x' = \\gamma \\hat{x} + \\beta $。参数 $ \\gamma $ 和 $ \\beta $ 通过最小化熵目标函数进行优化。 Algorithm Tent 算法的流程如下：\nInitialization： 加载预训练好的源模型参数 $ \\theta $。 固定所有非仿射变换的参数。 丢弃源数据中估计的归一化统计量。 优化器收集所有归一化层的通道级仿射变换参数 $ \\{\\gamma_{l,k}, \\beta_{l,k}\\} $。 Iteration：在线处理数据批次。 Forward Pass：对每个数据批次，逐层估计该批次数据的归一化统计量 ($ \\mu, \\sigma $)。 Backward Pass：计算预测熵 $ H(\\hat{y}) $ 相对于仿射变换参数 $ \\gamma, \\beta $ 的梯度 $ \\nabla H(\\hat{y}) $。 Update：使用梯度更新 $ \\gamma, \\beta $ 参数。Tent 采用高效的在线更新策略，每次更新只影响下一个批次的数据处理。 Termination：对于在线适应，适应过程只要有测试数据就持续进行。对于离线适应，模型会先进行更新，然后重复推断，适应可以持续多个Epochs。 Experiments 论文在多种计算机视觉任务和数据集上对 Tent 进行了全面评估。\nRobustness To Corruptions 在图像分类的鲁棒性基准测试中，使用受损版本的 CIFAR-10/100-C 和 ImageNet-C 数据集（15 种损坏类型，不同严重程度）。\n主要发现： Tent 在 ImageNet-C 上达到了 44.0% 的最低错误率，优于 SOTA 鲁棒性训练方法（如Adversarial Noise Training (ANT) 的 50.2%）和Test-Time Normalization (BN) 基线（49.9%）。 在 CIFAR-10/100-C 上，Tent 也显著优于其他 TTA baseline（BN, Pseudo-Labeling (PL)）以及需要联合训练源域和目标域的Domain Adaptation（RG, UDA-SS）和Test-Time Training (TTT) 方法。 这些改进仅通过一次Epoch的测试时优化实现，且未改变原始模型训练。 Source-Free Domain Adaptation 评估 Tent 在无源域适应场景下的性能，包括数字识别（从 SVHN 到 MNIST/MNIST-M/USPS）和语义分割（从 GTA 到 Cityscapes）。\n主要发现： 在数字识别任务中，Tent 大多数情况下错误率低于源模型和BN，部分情况甚至优于需要源数据的Domain Adaptation方法（RG, UDA-SS）。 语义分割任务中，Tent 将Intersection-Over-Union (IOU) 分数从源模型的 28.8% 提高到 35.8%，显著优于 BN 的 31.4%。 Analysis 论文通过多项分析实验探究了 Tent 的工作原理和特性：\nTent 降低熵和误差：实验证实，Tent 成功降低了预测的熵值和任务损失（如Softmax Cross-Entropy），印证了熵最小化与误差减少之间的正相关性。 Tent 需要特征调制：不更新归一化统计量或不优化仿射变换参数会显著降低 Tent 性能，说明这些特征调制步骤对于适应不可或缺。 Tent 泛化到不同的目标数据：适应过程对未用于更新的其他测试数据点同样有效，表明其学习到的调制是通用的。 Tent 调制与归一化不同：对比分析显示，Tent 的特征调制使特征更接近在目标标签上优化的Oracle模型（理想模型），而非仅像Batch Normalization那样接近原始参考分布。 Tent 适应其他网络架构：Tent 在基于Self-Attention 和Equilibrium Solving (MDEQ) 的模型上也能有效降低误差，展现了其普适性。 Related Work 论文回顾了与 Tent 相关的现有工作：\nTrain-Time Adaptation 方法：传统的Domain Adaptation、Test-Time Training (TTT) 等，通常需要源数据或训练阶段修改模型。 Source-Free Adaptation 方法：近期一些不依赖源数据的方法，但通常需要更复杂的设计、离线优化或修改训练过程。Tent 的优势在于其在线、高效且不改变训练过程。 Entropy Minimization：熵最小化已被广泛用于Semi-Supervised Learning和Domain Adaptation的正则化项，但 Tent 首次将其作为Fully Test-Time Adaptation中唯一的无监督损失来驱动模型适应。 Feature Modulation：归一化层和仿射变换已被用于各种任务的特征调制，但 Tent 将其作为在测试时通过无监督目标进行优化的核心机制。 Discussion Tent 通过Test Entropy Minimization实现了在数据漂移情况下的泛化误差降低。其核心在于模型的自监督自我改进，即依据自身的预测反馈进行调整。\n优势总结： 高效：仅通过在线优化少数参数（$ \\gamma, \\beta $）实现。 实用：无需源数据访问，不改变模型训练过程。 通用：适用于多种数据漂移类型和不同网络架构。 尽管 Tent 在广泛的场景中表现出色，但仍存在挑战，例如在特定困难的数据漂移（如 SVHN 到 MNIST-M/USPS）上仍有提升空间。未来研究方向可探索更全面的参数调整、更通用的Test-Time Adaptation Loss以及进一步提升效率的方法。总而言之，Tent 为Fully Test-Time Adaptation 提供了一个创新且实用的范式，使得模型能够在部署后，在面对未知且无标签的测试数据时，具备强大的自我适应能力。\n","permalink":"https://diefish1024.github.io/posts/literature-notes/tent/","summary":"\u003ch1 id=\"setting\"\u003eSetting\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eFully Test-Time Adaptation\u003c/strong\u003e 是一种独特的模型适应设定。在此设定下，模型 $ f_\\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \\theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。\u003c/p\u003e\n\u003cp\u003eFTT-Adaptation 与以下方法不同：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e：需要目标标签进行重新训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain Adaptation\u003c/strong\u003e：需要源数据和目标数据进行联合训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Training (TTT)\u003c/strong\u003e：需要修改训练过程并共同优化有监督及自监督损失。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003e论文的核心贡献是提出了 \u003cstrong\u003eTent\u003c/strong\u003e 方法，其核心思想是通过\u003cstrong\u003e最小化测试熵\u003c/strong\u003e（\u003cstrong\u003eTest Entropy Minimization\u003c/strong\u003e）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。\u003c/p\u003e\n\u003ch3 id=\"entropy-objective\"\u003eEntropy Objective\u003c/h3\u003e\n\u003cp\u003eTent 的测试时目标函数是最小化模型预测 $ \\hat{y} = f_\\theta(x^t) $ 的\u003cstrong\u003e熵 $ H(\\hat{y}) $\u003c/strong\u003e。论文中使用的\u003cstrong\u003e香农熵\u003c/strong\u003e计算公式如下：\u003c/p\u003e\n$$ \n\nH(\\hat{y}) = - \\sum_c p(\\hat{y}_c) \\log p(\\hat{y}_c)\n\n $$\n\u003cp\u003e其中， $ p(\\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。\u003c/p\u003e","title":"Tent"}]