[{"content":"同步和条件变量 互斥实现了原子性，但是无法实现确定性，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\n因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的发生顺序（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\n实现同步\n实现 $ A\\to B $：\n1 2 3 4 5 6 7 A; can_proceed = true; (signal) while(!can_proceed); B // B: wait until the condition is satisfied 这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\n最理想的 API 是 wait_until(cond) ，但是过去为了简化设计，变成了\n条件不满足时等待：wait - 直接睡眠等待 条件满足时继续：signal/broadcast - 唤醒所有线程 （小时候的 scratch 编程其实已经有了这样的思想😂）\n在 c++ 代码中我们可以把条件放到 $ \\lambda $ 表达式中：\n1 2 3 4 5 6 7 8 9 10 11 12 std::mutex mtx; std::condition_variable cv; void T_player() { std::unique_lock lk(mtx); cv.wait(lk, []{ return can_proceed; } ); cv.notify_all(); lk.unlock(); } 注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\n使用条件变量解决同步问题 大部分的同步问题都可以用经典的生产者 - 消费者问题归纳：\nProducer 和 Consumer 共享一个缓冲区，其中\nProducer 看到缓冲区有空位就会放入，否则等待 Consumer 看到缓冲区有数据就回去走，否则等待 显然一个对象的生产和消费必须满足 \u0026ldquo;happens-before\u0026rdquo; 的关系\n可以等价成打印匹配的括号，并且嵌套深度有上限（缓冲区的深度）\n处理这样的问题首先要想清楚程序继续执行的条件，比如生产的条件是 $ d0 $ ，然后套入固定的模板代码即可：\n1 2 3 4 5 6 mutex_lock(lk); while (!cond) { // cond can be any calculate cond_wait(\u0026amp;cv, lk); } assert(cond); mutex_lock(lk); 1 2 3 4 mutex_lock(lk); cond = true cond_broadcast(\u0026amp;cv); //⚠️ mutex_unlock(lk); 注意：全局广播 cond_broadcast 不能被替换成单独唤醒一个线程 cond_signal ，在这里显然可能会导致所有进程都被锁住无法触发新的同步变量；并发编程很多看起来正确的地方都需要仔细思考\n遇到任何同步问题的核心都是同步条件是什么，比如括号打印可以拓展成打印 \u0026lt;\u0026gt;\u0026lt; 或者 \u0026gt;\u0026lt;\u0026gt; 两种形状，核心也是画出状态机，找到同步条件，再套入模板就解决了问题\n计算图与并发控制 并行计算的模型可以用一个 DAG 计算图去理解，任务之间存在依赖关系，通过拓扑排序的顺序去解决问题，相互不存在 \u0026ldquo;happens-before\u0026rdquo; 依赖关系的任务都可以并发解决\n为了优化效率，我们对计算任务的分配需要保证每个节点计算的消耗是远大于同步和锁的开销的，因此实际上可能是把很多个小的任务聚合成一个大的并发计算节点，交给一个线程去执行\n实现计算图有两种思路，第一种是朴素的为每个节点设置一个线程和条件变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // The dependency edge is u-\u0026gt;v void T_u() { // calculate u mutex_lock(v-\u0026gt;lock); v-\u0026gt;num_done++; cond_signal(v-\u0026gt;cv); // it\u0026#39;s okay mutex_unlock(v-\u0026gt;lock); } void T_v() { mutex_lock(v-\u0026gt;lock); while (!(v-\u0026gt;num_done == v-\u0026gt;num_predecessors)){ cond_wait(v-\u0026gt;cv, v-\u0026gt;lock); } mutex_unlock(v-\u0026gt;lock); // calculate v } 但是这样实际会产生过多的线程，造成不必要的性能开销（比如产生了多余 CPU 的 core 数量的线程），实际上更优的办法是创建一个任务调度器线程 $ T_{\\text{scheduler}} $ 来专门控制产生 $ T_{\\text{worker}} $ ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 mutex_lock(lk); while (!(all_done || has_job(tid))) { cond_wait(\u0026amp;worker_cv[tid], lk); } mutex_unlock(lk); if (all_done) { break; } else { process_job(tid); } signal(\u0026amp;sched_cv); ","permalink":"https://diefish1024.github.io/posts/nju-os-2025/15-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%90%8C%E6%AD%A5%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8F/","summary":"\u003ch2 id=\"同步和条件变量\"\u003e同步和条件变量\u003c/h2\u003e\n\u003cp\u003e互斥实现了\u003cstrong\u003e原子性\u003c/strong\u003e，但是无法实现\u003cstrong\u003e确定性\u003c/strong\u003e，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\u003c/p\u003e\n\u003cp\u003e因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的\u003cstrong\u003e发生顺序\u003c/strong\u003e（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e实现同步\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e实现 $ A\\to B $：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003esignal\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e//\u003c/span\u003e \u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ewait\u003c/span\u003e \u003cspan class=\"n\"\u003euntil\u003c/span\u003e \u003cspan class=\"n\"\u003ethe\u003c/span\u003e \u003cspan class=\"n\"\u003econdition\u003c/span\u003e \u003cspan class=\"n\"\u003eis\u003c/span\u003e \u003cspan class=\"n\"\u003esatisfied\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\u003c/p\u003e\n\u003cp\u003e最理想的 API 是 \u003ccode\u003ewait_until(cond)\u003c/code\u003e ，但是过去为了简化设计，变成了\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e条件不满足时等待：\u003ccode\u003ewait\u003c/code\u003e - 直接睡眠等待\u003c/li\u003e\n\u003cli\u003e条件满足时继续：\u003ccode\u003esignal/broadcast\u003c/code\u003e - 唤醒所有线程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（小时候的 scratch 编程其实已经有了这样的思想😂）\u003c/p\u003e\n\u003cp\u003e在 c++ 代码中我们可以把条件放到 $ \\lambda $ 表达式中：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c++\" data-lang=\"c++\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003emutex\u003c/span\u003e \u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003econdition_variable\u003c/span\u003e \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_player\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eunique_lock\u003c/span\u003e \u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewait\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e[]{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enotify_all\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eunlock\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\u003c/p\u003e","title":"15. 并发控制：同步条件变量"},{"content":"信号量 互斥锁在某种意义上也可以认为实现了 \u0026ldquo;happens-before\u0026rdquo; 的依赖关系—— release 必然发生在 acquire 之前。我们可以试着利用这种依赖关系来实现计算图的调度：为每条边分配一个互斥锁，代表数据或前置任务的完成；一个节点必须获得所有入边对应的互斥锁才能开始计算，计算完成后，就释放所有出边对应的互斥锁，通知下游节点输出就绪（但是这种直接使用互斥锁作为边状态信号的方式是 undefined behavior，因为互斥锁主要用于保护临界区，其释放通常要求由持有它的线程完成，若释放未曾获取的锁，则行为未定义）\n我们可以从这种想法中抽象出其本质，也就是用一个“信号”去获取资源的许可，类似餐厅的取号吃饭\n这种信号的思想很适合用来管理计数类型的同类资源，比如停车场的空位，为了实现这种 producer-customer 的问题，用 条件变量 可以轻易解决，进入的条件就是存在空位 count \u0026lt; capacity ，那我们从减少变量的角度出发，这实际上也就是剩余空位的数量大于零，我们停车相当于消耗了一个车位，离开相当于创造了一个车位，这也就得到了所谓“信号量”的机制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void P(sem_t *sem) { // Prolaag - try + decrease/down/wait/acquire mutex_lock(\u0026amp;sem-\u0026gt;lk); while (!(sem-\u0026gt;count \u0026gt; 0)) { cond_wait(\u0026amp;sem-\u0026gt;cv, \u0026amp;sem-\u0026gt;lk); } sem-\u0026gt;count--; // 消耗一个信号 (车位) mutex_unlock(\u0026amp;sem-\u0026gt;lk); } void V(sem_t *sem) { // Verhoog - increase/up/post/signal/release mutex_lock(\u0026amp;sem-\u0026gt;lk); sem-\u0026gt;count++; // 创建一个信号 (车位) cond_broadcast(\u0026amp;sem-\u0026gt;cv); mutex_unlock(\u0026amp;sem-\u0026gt;lk); } 根据这个一路推出信号量的思路，或许可以认为这是互斥锁的扩展\n信号量：应用 信号量有两种典型的应用：\n实现一个临时的 happens-before：$ A\\to V(s)\\to P(s)\\to B $ 管理计数资源：停车场、餐厅…… 可以利用信号量优雅地实现 producer-customer 的模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sem_t empty = SEM_INIT(depth); sem_t fill = SEM_INIT(0); void T_produce() { P(\u0026amp;empty); printf(\u0026#34;(\u0026#34;); V(\u0026amp;fill); } void T_consume() { P(\u0026amp;fill); printf(\u0026#34;)\u0026#34;); V(\u0026amp;empty); } 信号量、条件变量与同步 信号量对比条件变量：\n信号量：更加干净优雅，但是不一定能很好地表示同步条件（用更 hack 的方式解决问题） 条件变量：更加万能，但是代码比较丑陋（用更标准化的方式解决问题） 尝试用信号量解决更复杂的同步问题：哲学家吃饭，一张圆桌围坐 $ n $ 个哲学家，每两个人之间有一把叉子（筷子更加合适？？），每个哲学家（线程）有时思考有时吃饭，思考时什么也不用做，吃饭时同时需要左手右手的叉子\n用条件变量解决这个问题只需要无脑设置同步条件即可，用信号量解决这个问题有一个初步的想法是 P(\u0026amp;sem[lhs]) \u0026amp;\u0026amp; P(\u0026amp;sem[rhs]) ，乍一看没什么问题，但是实际上这个条件在所有人都同时举起了同一边的叉子时会陷入死锁（所以并发编程一定要仔细再仔细！），所以为了排除这种方案，有两种解决方案：\n从桌子赶走一个人：为上桌吃饭人数设置一个信号量，限制不让所有人同时上桌即可（显然不可能所有人同时吃上饭） 为叉子编号，总是先拿起编号小的一把（最后一个人的顺序会和其他人反过来） 但是这样的解决方案是不够优雅不够通用的，因此更多时候条件变量是一个更好的选择，可以总结为信号量在适合的适合很好用，但不总是很好用\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/16-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%90%8C%E6%AD%A5%E4%BF%A1%E5%8F%B7%E9%87%8F/","summary":"\u003ch2 id=\"信号量\"\u003e信号量\u003c/h2\u003e\n\u003cp\u003e互斥锁在某种意义上也可以认为实现了 \u0026ldquo;happens-before\u0026rdquo; 的依赖关系—— release 必然发生在 acquire 之前。我们可以试着利用这种依赖关系来实现计算图的调度：为每条边分配一个互斥锁，代表数据或前置任务的完成；一个节点必须获得所有入边对应的互斥锁才能开始计算，计算完成后，就释放所有出边对应的互斥锁，通知下游节点输出就绪（但是这种直接使用互斥锁作为边状态信号的方式是 undefined behavior，因为互斥锁主要用于保护临界区，其释放通常要求由持有它的线程完成，若释放未曾获取的锁，则行为未定义）\u003c/p\u003e\n\u003cp\u003e我们可以从这种想法中抽象出其本质，也就是用一个“\u003cstrong\u003e信号\u003c/strong\u003e”去获取资源的许可，类似餐厅的取号吃饭\u003c/p\u003e\n\u003cp\u003e这种\u003cstrong\u003e信号\u003c/strong\u003e的思想很适合用来管理\u003cstrong\u003e计数类型的同类资源\u003c/strong\u003e，比如停车场的空位，为了实现这种 producer-customer 的问题，用 \u003ca href=\"15.%20%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%EF%BC%9A%E5%90%8C%E6%AD%A5%E6%9D%A1%E4%BB%B6%E5%8F%98%E9%87%8F.md\"\u003e条件变量\u003c/a\u003e 可以轻易解决，进入的条件就是存在空位 \u003ccode\u003ecount \u0026lt; capacity\u003c/code\u003e ，那我们从减少变量的角度出发，这实际上也就是剩余空位的数量大于零，我们停车相当于\u003cstrong\u003e消耗\u003c/strong\u003e了一个车位，离开相当于\u003cstrong\u003e创造\u003c/strong\u003e了一个车位，这也就得到了所谓“\u003cstrong\u003e信号量\u003c/strong\u003e”的机制\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e14\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e15\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e16\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e17\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eP\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003esem_t\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Prolaag - try + decrease/down/wait/acquire\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"nf\"\u003econd_wait\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// 消耗一个信号 (车位)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003esem_t\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Verhoog - increase/up/post/signal/release\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003emutex_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecount\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// 创建一个信号 (车位)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003econd_broadcast\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nf\"\u003emutex_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003esem\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e根据这个一路推出信号量的思路，或许可以认为这是互斥锁的扩展\u003c/p\u003e","title":"16. 并发控制：同步信号量"},{"content":"数据竞争 大多并发 bug 最后都会体现为数据竞争 (Data Race)\n对于顺序程序而言，函数 f() 返回之后就已经完成了所有的状态修改，对于其他部分而言这个修改是立即生效的；如果对于并发程序而言模式的切换也在瞬间完成，那就不会导致并发的问题\n然而实际上模式的切换需要时间，执行的操作在未来一段时间之后才会就绪，但是我们在实际编程时总是容易有“立即生效”的肌肉记忆，这就导致了并发问题的可能性\n不过对于函数式编程而言，操作不存在对外状态的修改，没有副作用（只会操作局部变量），这就不会导致并发问题\nData Race 发生的实质是不同的线程同时访问同一内存，并且至少有一个是写，形象的理解就是不同的内存访问在“赛跑”，跑赢的操作先执行\nNot that easy: 虽然我们将数据竞争形象地比喻为“赛跑”，但实际上，哪一个操作能“跑赢”并没有想象中那么简单和确定，其复杂性主要体现在以下几个方面\n弱内存模型 (Weak memory model)：在现代处理器架构中，为了提升性能，处理器可能会对内存操作进行重排序。这意味着，不同的线程或“观察者”在不同时间点看到共享内存的状态可能是不一致的。一个线程对内存的写入操作，可能不会立即对所有其他线程可见，导致不同线程观察到不同的结果。这种内存模型的一致性问题使得确定哪个操作“先发生”变得非常困难且不确定。\n未定义行为 (Undefined Behavior)：从 C++11 标准开始，数据竞争被明确规定为未定义行为。这意味着，如果你的程序发生了数据竞争，编译器可以自由地产生任何行为，无论是崩溃、产生错误结果，还是看似正常运行但结果不可预测。这使得数据竞争成为非常危险且难以调试的并发错误，因为它的表现可能是不确定、不稳定的。\n多线程与多内存的复杂交互：在实际的并发程序中，通常会有多个线程同时访问多个共享内存位置。这些线程和内存之间存在复杂的读（R）写（W）交互。一个线程对一个内存位置的写入可能影响到其他多个线程对该位置的读取，同时，多个内存位置之间也可能存在复杂的依赖关系和缓存一致性问题。这种错综复杂的交互网络进一步加剧了数据竞争的不可预测性。\n为了消灭数据竞争，我们需要保证程序的 serializability ，可能竞争的内存访问要么互斥，要么同步\n实际编程中遇到的数据竞争 bug 大多属于上错了锁和忘记上锁两种情况的变种\nCase 1: 上错了锁\n1 2 void T_1() { spin_lock(\u0026amp;A); sum++; spin_unlock(\u0026amp;A); } void T_2() { spin_lock(\u0026amp;B); sum++; spin_unlock(\u0026amp;B); } Case 2: 忘记上锁\n1 2 void T_1() { spin_lock(\u0026amp;A); sum++; spin_unlock(\u0026amp;A); } void T_2() { sum++; } 但是实际系统面临的情况比这复杂的多，因为\n内存可以是地址空间的任何内存，比如全局变量、堆内存分配的变量、程序的栈…… 访问可以发生在任何代码，比如自己的代码、框架代码、一行没读到的汇编指令、某条 ret 指令 “一行没读到的汇编指令”造成的访问的情况有编译器优化造成的指令重排、硬件层面弱内存模型的内存访问重排、还有一些高层语言操作的隐式内存访问 实际系统中虽然难以避免，但是会尽可能保证底层的结构对上层尽可能封闭来防止这种错误 死锁 死锁 (Deadlock) 是指一个群体中的每个成员都在等待其他成员（包括自身）采取行动的状态\n死锁有两种：\nAA-Deadlock: 自己等待自己\n1 2 3 4 5 6 7 lock(\u0026amp;lk); // lk-\u0026gt;locked == ✅; proceed ... // Possibly in interrupt handler lock(\u0026amp;lk); // while (lk-\u0026gt;locked == ❌) ; 这样的错误虽然看起来很傻，但是在真实程序复杂的控制流中是可能出现的\nABBA-Deadlock: 两个（多个）锁互相等待\n比如 16. 并发控制：同步信号量 中的哲学家吃饭问题\nHow? 想要消除死锁，可以从死锁产生的必要条件入手，通常称为霍尔德（Holt）条件， 可以形象地把锁看成袋子里的球：\n互斥 (Mutual-exclusion)： 一个口袋一个球，即资源是互斥的，一次只能被一个线程占用 如果资源可以共享（例如，只读文件），则不会发生死锁 请求并保持 (Wait-for)： 得到球的人想要更多的球，即一个线程在持有至少一个资源的同时，又在等待获取其他被占用的资源 如果线程在请求新资源时，必须释放所有已持有的资源，则可以避免死锁 不可抢占 (No-preemption)： 不能抢别人的持有的球，即资源不能被强制从持有者手中抢走，只能由持有者自愿释放 如果系统可以抢占资源（例如，通过中断强制释放），则可以打破此条件 循环等待 (Circular-chain)： 形成循环等待的关系，即存在一个线程链 $ T_1, T_2, \\ldots, T_n $，其中 $ T_1 $ 正在等待 $ T_2 $ 占用的资源，$ T_2 $ 正在等待 $ T_3 $ 占用的资源，依此类推，$ T_n $ 正在等待 $ T_1 $ 占用的资源 这四个条件是死锁发生的必要条件，这意味着只要打破任何一个条件，就不会发生死锁了，理论上，我们可以通过破坏这些条件来预防或避免死锁。\n然而，将这套理论应用于实际复杂系统时，会发现它是一个正确的废话，不能称为一个合理的 argument\n对于玩具系统/模型：可以直接证明系统是 deadlock-free 的，因为其状态空间有限，可以通过穷举或形式化方法进行验证 对于真正的复杂系统：很难判断哪个条件最容易被打破，或者说，在保证系统功能性和性能的前提下，打破某个条件可能带来巨大的复杂性或性能开销 实际编程中通常会采用其他策略来预防死锁：\n一个常见的方法是锁排序，其核心思想是：\n任意时刻系统中的锁都是有限的。 给所有锁编号（或者定义一个全局的获取顺序） 线程在获取多个锁时，严格按照从小数到大的顺序获取锁 这种策略也相对容易检查和验证。 这样在任意时刻总有一个线程获得“编号最大”的锁，于是这个线程总是可以继续运行\n然而这种方法在实践中会遇到问题，代码的文档并不总是可靠，并且对于复杂系统这是难以扩展的；而最好的锁是封装的，并不会暴露出来，这样使用代码的人甚至不需要知道正在使用锁\nSome Methods 为了应对死锁，尤其是 ABBA-Deadlock，一些工具被开发出来，例如 Linux 内核中的 LockDep\n一个简单的想法：\n每次 acquire/release 锁时，都打印一个日志 如果任何线程存在 $ A\\to B $ 和 $ B\\to A $ 的依赖关系，就报告死锁 这可能导致 false positives ，比如存在同步机制 ($ A\\to B\\to \\mathrm{spawn}\\to B\\to A $) 通过优化这个方法可以得到一个相对高效的实现：动态维护“锁依赖图“并检测环路\n原子性和顺序违反 并发编程是本质困难的，我们只能用 sequential 的方式来理解并发：把程序分成若干的块，每一块的操作都是原子的，无法被打断\n并发的机制可以分成两类：\n互斥锁实现原子性 忘记上锁会导致原子性违反 (Atomicity Violation, AV) 条件变量/信号量实现先后顺序同步 忘记同步会导致顺序违反 (Order Violation, OV) 并发的机制完全是“后果自负”的，这也导致了 Threads cannot be implemented as a library ，因为？\n有研究统计了很多真是系统存在的并发 bug，发现 97% 的非死锁并发 bug 都是原子性或顺序错误\nAtomic Violation 代码被别的线程“强行插入”，即使分别上锁消除了数据竞争，还是会导致 AV\n比如图中的例子：\n如果在 Thread 1 结束判断进入 if 之后 Thread 2 再执行，就导致了错误\n并且注意到操作系统的状态也是共享状态，利用一样的原理还可能产生更难发现的 bug\n攻击者在 check 之后马上替换文件为符号链接，就可以造成权限问题等严重安全漏洞\nOrder Violation 事件没有按照预定的顺序发生，就会导致 OV ，比如 如果 Thread 2 中的 S4 发生在了条件变量的初始化之前，那么相当于全局的广播被吞掉了，就可能会导致 Thread 1 可能无法被唤醒\nHowever 我们可以使用加强版的 LockDep 来解决这些问题，比如直接分析程序的日志，检查有没有不相交的锁，事件的 happens-before 关系是否正确……甚至也可以直接用启发式（或者 LLM ）来分析日志是否正确\n然而即使这样也不能解决真实世界的所有并发问题，比如这个 GhostRace 的例子\n实现了正确的互斥、正确的同步 $ \\text{use}\\to\\text{free} $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 void T_A() { lock(\u0026amp;obj-\u0026gt;lock); free(obj-\u0026gt;something); obj-\u0026gt;something = NULL; unlock(\u0026amp;obj-\u0026gt;lock); } void T_B() { lock(\u0026amp;obj-\u0026gt;lock); if (obj-\u0026gt;something) { // changes cache on speculative execution } unlock(\u0026amp;obj-\u0026gt;lock); } 实际上导致问题的是现代 CPU 的推测执行的特性，为了提高效率会提前通过分治预测器预判执行的方向，提前执行指令\n这段代码中线程 $ T_{B} $ 的 CPU 会在正式获得锁并判断 NULL 之前就提前推测性执行 if 块内部的代码，如果此时 $ T_{A} $ 恰好释放了 obj-something 指向的内存，那么 $ T_{B} $ 中就会访问到一块已经被释放的内存，从而通过缓存等方式泄露信息（虽然错误执行的语句已经回滚了，但是被加载到缓存中的数据等不会回滚），通过这种方式就有可能访问到程序本来没有权限访问的内存，从而产生安全漏洞\n这种并发 bug 的根源在于软件层面的同步无法约束硬件层面的行为\n","permalink":"https://diefish1024.github.io/posts/nju-os-2025/17-%E5%B9%B6%E5%8F%91-bugs/","summary":"\u003ch2 id=\"数据竞争\"\u003e数据竞争\u003c/h2\u003e\n\u003cp\u003e大多并发 bug 最后都会体现为\u003cstrong\u003e数据竞争 (Data Race)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e对于顺序程序而言，函数 \u003ccode\u003ef()\u003c/code\u003e 返回之后就已经完成了所有的状态修改，对于其他部分而言这个修改是立即生效的；如果对于并发程序而言模式的切换也在瞬间完成，那就不会导致并发的问题\u003c/p\u003e\n\u003cp\u003e然而实际上模式的切换需要时间，执行的操作在未来一段时间之后才会就绪，但是我们在实际编程时总是容易有“立即生效”的肌肉记忆，这就导致了并发问题的可能性\u003c/p\u003e\n\u003cp\u003e不过对于\u003cstrong\u003e函数式编程\u003c/strong\u003e而言，操作不存在对外状态的修改，没有副作用（只会操作局部变量），这就不会导致并发问题\u003c/p\u003e\n\u003cp\u003eData Race 发生的实质是\u003cstrong\u003e不同的线程\u003c/strong\u003e同时访问\u003cstrong\u003e同一内存\u003c/strong\u003e，并且\u003cstrong\u003e至少有一个是写\u003c/strong\u003e，形象的理解就是不同的内存访问在“赛跑”，跑赢的操作先执行\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNot that easy\u003c/strong\u003e: 虽然我们将数据竞争形象地比喻为“赛跑”，但实际上，哪一个操作能“跑赢”并没有想象中那么简单和确定，其复杂性主要体现在以下几个方面\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e弱内存模型 (Weak memory model)\u003c/strong\u003e：在现代处理器架构中，为了提升性能，处理器可能会对内存操作进行重排序。这意味着，不同的线程或“观察者”在不同时间点看到共享内存的状态可能是不一致的。一个线程对内存的写入操作，可能不会立即对所有其他线程可见，导致不同线程观察到不同的结果。这种\u003cstrong\u003e内存模型的一致性问题\u003c/strong\u003e使得确定哪个操作“先发生”变得非常困难且不确定。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e未定义行为 (Undefined Behavior)\u003c/strong\u003e：从 C++11 标准开始，数据竞争被明确规定为\u003cstrong\u003e未定义行为\u003c/strong\u003e。这意味着，如果你的程序发生了数据竞争，编译器可以自由地产生任何行为，无论是崩溃、产生错误结果，还是看似正常运行但结果不可预测。这使得数据竞争成为非常危险且难以调试的并发错误，因为它的表现可能是不确定、不稳定的。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e多线程与多内存的复杂交互\u003c/strong\u003e：在实际的并发程序中，通常会有多个线程同时访问多个共享内存位置。这些线程和内存之间存在复杂的读（R）写（W）交互。一个线程对一个内存位置的写入可能影响到其他多个线程对该位置的读取，同时，多个内存位置之间也可能存在复杂的依赖关系和缓存一致性问题。这种错综复杂的交互网络进一步加剧了数据竞争的不可预测性。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e为了消灭数据竞争，我们需要保证程序的 serializability ，\u003cstrong\u003e可能竞争的内存访问要么互斥，要么同步\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e实际编程中遇到的数据竞争 bug 大多属于\u003cstrong\u003e上错了锁\u003c/strong\u003e和\u003cstrong\u003e忘记上锁\u003c/strong\u003e两种情况的变种\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCase 1: 上错了锁\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_1\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_2\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eCase 2: 忘记上锁\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_1\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_lock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"nf\"\u003espin_unlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_2\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003esum\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e但是实际系统面临的情况比这复杂的多，因为\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e内存可以是地址空间的任何内存，比如全局变量、堆内存分配的变量、程序的栈……\u003c/li\u003e\n\u003cli\u003e访问可以发生在任何代码，比如自己的代码、框架代码、一行没读到的汇编指令、某条 ret 指令\n\u003cul\u003e\n\u003cli\u003e“一行没读到的汇编指令”造成的访问的情况有编译器优化造成的指令重排、硬件层面弱内存模型的内存访问重排、还有一些高层语言操作的隐式内存访问\u003c/li\u003e\n\u003cli\u003e实际系统中虽然难以避免，但是会尽可能保证底层的结构对上层尽可能封闭来防止这种错误\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"死锁\"\u003e死锁\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e死锁 (Deadlock)\u003c/strong\u003e 是指一个群体中的每个成员都在等待其他成员（包括自身）采取行动的状态\u003c/p\u003e\n\u003cp\u003e死锁有两种：\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAA-Deadlock\u003c/strong\u003e: 自己等待自己\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nf\"\u003elock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e// lk-\u0026gt;locked == ✅; proceed\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// Possibly in interrupt handler\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e    \u003cspan class=\"nf\"\u003elock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"c1\"\u003e// while (lk-\u0026gt;locked == ❌) ;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样的错误虽然看起来很傻，但是在真实程序复杂的控制流中是可能出现的\u003c/p\u003e","title":"17. 并发 Bugs"},{"content":"A General Paradigm of Test-Time Adaptation 根据测试数据接收方式和适应过程，TTA 分为三种主要范式：\nTest-Time Batch Adaptation (TTBA) 测试时间批次适应： 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。 Online Test-Time Adaptation (OTTA) 在线测试时间适应： 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。 Test-Time Domain Adaptation (TTDA) 测试时间域适应： 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。 Datasets for Evaluation 论文使用了两种不同类型的分布偏移数据集进行评估：\nCorruption Datasets 损坏数据集： 原始数据集（CIFAR-10，ImageNet）经过人为损坏处理后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。 Natural-shift Datasets 自然偏移数据集： 这些数据集代表数据分布中自然发生的变化，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。 Results on Natural Shift Datasets TTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。 PredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。 T3A 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。 对于自然偏移数据集，TTDA 算法 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。 ","permalink":"https://diefish1024.github.io/posts/literature-notes/benchmarking-tta/","summary":"\u003ch3 id=\"a-general-paradigm-of-test-time-adaptation\"\u003eA General Paradigm of Test-Time Adaptation\u003c/h3\u003e\n\u003cp\u003e根据测试数据接收方式和适应过程，TTA 分为三种主要范式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Batch Adaptation (TTBA) 测试时间批次适应：\u003c/strong\u003e 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOnline Test-Time Adaptation (OTTA) 在线测试时间适应：\u003c/strong\u003e 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Domain Adaptation (TTDA) 测试时间域适应：\u003c/strong\u003e 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"datasets-for-evaluation\"\u003eDatasets for Evaluation\u003c/h3\u003e\n\u003cp\u003e论文使用了两种不同类型的分布偏移数据集进行评估：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCorruption Datasets 损坏数据集：\u003c/strong\u003e 原始数据集（CIFAR-10，ImageNet）经过\u003cstrong\u003e人为损坏处理\u003c/strong\u003e后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNatural-shift Datasets 自然偏移数据集：\u003c/strong\u003e 这些数据集代表数据分布中\u003cstrong\u003e自然发生的变化\u003c/strong\u003e，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"results-on-natural-shift-datasets\"\u003eResults on Natural Shift Datasets\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。\u003c/li\u003e\n\u003cli\u003ePredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eT3A\u003c/strong\u003e 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。\u003c/li\u003e\n\u003cli\u003e对于自然偏移数据集，\u003cstrong\u003eTTDA 算法\u003c/strong\u003e 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。\u003c/li\u003e\n\u003c/ul\u003e","title":"Benchmarking TTA"},{"content":"Setting Continual Test-Time Domain Adaptation 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个非平稳且持续变化的目标环境 。\nCoTTA 与以下方法不同：\nStandard Domain Adaptation：需要同时访问源数据和（静态的）目标数据进行训练。 Standard Test-Time Adaptation / Fully Test-Time Adaptation：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。 Test-Time Training (TTT)：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。 相比之下，CoTTA 专门解决在无源数据的条件下，模型如何在线适应一个持续变化的数据流，同时克服现有方法中常见的错误累积和灾难性遗忘问题。\nMethod 论文的核心贡献是提出了CoTTA (Continual Test-Time Adaptation) 方法，旨在通过减少错误累积和避免灾难性遗忘，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。\n1. 减少错误累积 (Reducing Error Accumulation) 为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。\n权重平均伪标签 (Weight-Averaged Pseudo-Labels) 该方法采用一个教师 - 学生 (teacher-student) 框架。学生模型 (student model) 在线进行学习和更新。 教师模型 (teacher model) 的权重是学生模型权重的指数移动平均 (Exponential Moving Average, EMA)。 由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的一致性损失 (consistency loss) 来进行更新。 数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels) 为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。 它首先使用原始预训练模型评估当前测试数据的预测置信度，以此来近似域差异的大小。 条件性应用： 如果置信度高（域差异小），则直接使用教师模型的预测作为伪标签 16。 如果置信度低（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签 17171717。这可以进一步提高伪标签的鲁棒性。 2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting) 为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了随机恢复 (Stochastic Restoration) 机制。\n核心思想：在每次模型更新后，以一个很小的概率 p，将模型中的一小部分权重参数随机地恢复到其原始的、预训练时的状态。 优势： 这种机制可以看作一种特殊的 Dropout 20。它能有效防止模型在适应新数据时“漂移”得离源模型太远，从而显式地保留了源知识，避免了灾难性遗忘。 通过保留源知识，CoTTA 能够安全地更新网络中的所有参数，而不仅仅是归一化层，这为模型适应提供了更大的容量。 Algorithm CoTTA 算法的在线流程如下：\nInitialization (初始化)： 加载一个“开箱即用”的预训练源模型 $ f_{\\theta_{0}} $ 用源模型权重初始化教师模型 $ f_{\\theta'_{0}} $ Iteration (迭代)：对于在线输入的每个测试数据 $ x_{t} $​： 生成伪标签：使用教师模型 $ f_{\\theta'_{t}} $​​，并结合条件性数据增强，生成权重和增强平均的伪标签。 更新学生模型：通过一致性损失更新学生模型 $ f_{\\theta_{t}} $ 更新教师模型：使用 EMA 更新教师模型的权重 $ f_{\\theta'_{t+1}} $ 随机恢复：对学生模型的权重进行随机恢复 Output (输出)：使用教师模型 $ f_{\\theta'_{t}} $​​ 进行在线预测，并传递更新后的学生和教师模型到下一个时间步。 Experiments 论文在多个图像分类和语义分割任务上对 CoTTA 进行了评估，特别是在一个持续变化的测试环境中。\nContinual Adaptation on Corrupted Images 在 CIFAR10-C、CIFAR100-C 和 ImageNet-C 数据集上，模型被顺序输入 15 种不同类型的损坏图像。\n主要发现： 在 CIFAR10-C 上，CoTTA 的平均错误率仅为 16.2%，显著优于 Source-only 基线 (43.5%) 和 TENT-continual (20.7%)。 在更难的 CIFAR100-C 上，TENT 等方法因错误累积导致性能随时间推移而急剧下降（错误率从 37.2% 恶化到 90.4%），而 CoTTA 表现稳定，平均错误率仅为 32.5% 。 实验表明，TENT 在持续适应的后期会因错误累积而性能崩溃，而 CoTTA 的随机恢复机制成功避免了这一点。 Continual Adaptation on Semantic Segmentation 在一个从 Cityscapes (晴天) 到 ACDC (雾、夜、雨、雪等恶劣天气) 的持续语义分割任务中，模型会循环经历这四种天气条件 10 次，以测试其长期适应和遗忘情况。\n主要发现： CoTTA 将平均 mIoU 提升至 58.6%，优于源模型 (56.7%) 和其他适应方法。 TENT 在此任务上表现不佳，因为其依赖的批量归一化 (Batch Normalization) 层在 Transformer 架构 (Segformer) 中很少。 CoTTA 不依赖于特定层，因此在基于 Transformer 的架构上同样有效，展现了其通用性 38。 Analysis 通过消融实验验证了 CoTTA 各个组件的有效性。\n权重平均的作用：仅使用权重平均的伪标签，就将错误率从 20.7% (TENT-continual) 降至 18.3%，证明了教师模型伪标签的优越性。 数据增强平均的作用：在权重平均的基础上再加入条件性数据增强，错误率进一步降至 17.4%。 随机恢复的作用：最后加入随机恢复机制，错误率最终降至 16.2%，并且解决了长期适应中的性能衰退问题，证明了其在避免灾难性遗忘中的关键作用。 Related Work 论文回顾了与 CoTTA 相关的领域：\nTest-Time Adaptation (TTA)：现有工作大多关注静态目标域，在持续变化的环境中，基于熵最小化或伪标签的方法容易因伪标签噪声而累积错误。 Continuous Domain Adaptation：与 CoTTA 目标相似，但现有方法通常需要访问源数据来对齐分布。 Continual Learning：CoTTA 借鉴了该领域的思想来解决灾难性遗忘问题，但将其应用在了一个无监督、测试时适应的独特场景中。 Source-Free Domain Adaptation：CoTTA 属于此范畴，其新颖之处在于它专为在线和持续变化的环境设计，而这是先前工作很少考虑的。 Discussion CoTTA 成功地解决了在无源数据、非平稳环境下进行持续测试时适应的挑战。它通过创新的机制同时解决了错误累积和灾难性遗忘这两个核心难题。\n优势总结： 稳定与长效：通过随机恢复机制，CoTTA 实现了在长期持续适应过程中的性能稳定，避免了性能崩溃。 实用与通用：无需访问源数据，也无需修改模型训练过程，可直接用于各类“开箱即用”的预训练模型（包括 CNN 和 Transformer） 。 高效：整个适应过程在线进行，模型根据当前数据流即时更新和预测。 总而言之，CoTTA 为模型在真实世界中部署后的持续自我进化提供了一个强大且实用的框架，使模型能够鲁棒地适应不断变化的操作环境。\n","permalink":"https://diefish1024.github.io/posts/literature-notes/cotta/","summary":"\u003ch1 id=\"setting\"\u003eSetting\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eContinual Test-Time Domain Adaptation\u003c/strong\u003e 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个\u003cstrong\u003e非平稳\u003c/strong\u003e且\u003cstrong\u003e持续变化\u003c/strong\u003e的目标环境 。\u003c/p\u003e\n\u003cp\u003eCoTTA 与以下方法不同：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eStandard Domain Adaptation\u003c/strong\u003e：需要同时访问源数据和（静态的）目标数据进行训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStandard Test-Time Adaptation / Fully Test-Time Adaptation\u003c/strong\u003e：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Training (TTT)\u003c/strong\u003e：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相比之下，CoTTA 专门解决在\u003cstrong\u003e无源数据\u003c/strong\u003e的条件下，模型如何在线适应一个\u003cstrong\u003e持续变化的\u003c/strong\u003e数据流，同时克服现有方法中常见的\u003cstrong\u003e错误累积\u003c/strong\u003e和\u003cstrong\u003e灾难性遗忘\u003c/strong\u003e问题。\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003e论文的核心贡献是提出了\u003cstrong\u003eCoTTA (Continual Test-Time Adaptation)\u003c/strong\u003e 方法，旨在通过\u003cstrong\u003e减少错误累积\u003c/strong\u003e和\u003cstrong\u003e避免灾难性遗忘\u003c/strong\u003e，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。\u003c/p\u003e\n\u003ch3 id=\"1-减少错误累积-reducing-error-accumulation\"\u003e1. 减少错误累积 (Reducing Error Accumulation)\u003c/h3\u003e\n\u003cp\u003e为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e权重平均伪标签 (Weight-Averaged Pseudo-Labels)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e该方法采用一个\u003cstrong\u003e教师 - 学生 (teacher-student)\u003c/strong\u003e 框架。学生模型 (student model) 在线进行学习和更新。\u003c/li\u003e\n\u003cli\u003e教师模型 (teacher model) 的权重是学生模型权重的\u003cstrong\u003e指数移动平均 (Exponential Moving Average, EMA)\u003c/strong\u003e。\u003c/li\u003e\n\u003cli\u003e由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的\u003cstrong\u003e一致性损失\u003c/strong\u003e (consistency loss) 来进行更新。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels)\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。\u003c/li\u003e\n\u003cli\u003e它首先使用\u003cstrong\u003e原始预训练模型\u003c/strong\u003e评估当前测试数据的\u003cstrong\u003e预测置信度\u003c/strong\u003e，以此来近似域差异的大小。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e条件性应用\u003c/strong\u003e：\n\u003cul\u003e\n\u003cli\u003e如果置信度\u003cstrong\u003e高\u003c/strong\u003e（域差异小），则直接使用教师模型的预测作为伪标签 16。\u003c/li\u003e\n\u003cli\u003e如果置信度\u003cstrong\u003e低\u003c/strong\u003e（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签 17171717。这可以进一步提高伪标签的鲁棒性。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-避免灾难性遗忘-avoiding-catastrophic-forgetting\"\u003e2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting)\u003c/h3\u003e\n\u003cp\u003e为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了\u003cstrong\u003e随机恢复 (Stochastic Restoration)\u003c/strong\u003e 机制。\u003c/p\u003e","title":"CoTTA"},{"content":"Introduction 类似 GAN 的对抗训练思想\nDomain Adaptation 给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \\eta: X\\to Y $ 使其在目标域上的目标风险 $$ R_{D_{T}}(\\eta) = \\underset{(\\mathbf{x},y)\\sim D_{T}}{\\mathrm{Pr}}(\\eta(\\mathbf{x}) \\neq y) $$ 最小\nDomain Divergence 需要量化两个领域的“相似度”，从而引出了 H- 散度 的概念： $$ d_{\\mathcal{H}}(D_S, D_T) = 2 \\sup_{\\eta \\in \\mathcal{H}} \\left| \\Pr_{x \\sim D_S}[\\eta(x) = 1] - \\Pr_{x \\sim D_T}[\\eta(x) = 1] \\right| $$ 含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果\n由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度 $$ \\hat{d}_{\\mathcal{H}}(S, T) = 2 \\left(1 - \\min_{\\eta \\in \\mathcal{H}} \\left[ \\dfrac{1}{n}\\sum_{i=1}^n \\mathcal{I}[\\eta(x_i) = 0] + \\dfrac{1}{n'}\\sum_{i=n+1}^N \\mathcal{I}[\\eta(x_i) = 1] \\right] \\right) $$ 其中 $ \\mathcal{I}[\\cdot] $ 表示条件为真时为 1，否则为 0\nProxy Distance 经验 H- 散度也需要直接遍历所有的 $ \\eta $ ，在计算上不现实，需要一个进一步的近似方法，因此考虑 Proxy A-distance (PAD)\n构造用于领域分类的数据集 $$ U = \\{ (\\mathbf{x}_{i},0) \\}_{i=1}^{n} \\cup \\{ (\\mathbf{x}_{i},1) \\}_{i=n+1}^{N} $$ 用这个数据集训练分类器，设 $ \\epsilon $ 为在数据集 $ U $ 上训练出的最优领域分类器所达到的最小错误率，那么可以用 $$ \\hat{d}_{\\mathcal{A}} = 2(1-2\\epsilon) $$ 来近似 H- 散度\nGeneralization Bound on the Target Risk 有效性证明\n理论研究说明模型的目标风险可以通过源风险和两个领域的散度来限制，主要思想是 $$ R_{D_T}(\\eta) \\le R_S(\\eta) + \\text{Domain Divergence Terms} + \\text{Complexity Terms} + \\beta $$ 其中 $ \\text{Domain Divergence Terms}\\approx d_{\\mathcal{H}}(S, T) $ ，可以用上面的 $ \\hat{d}_{\\mathcal{A}} $ 近似；$ \\text{Complexity Terms} $ 是一个比较小的常数项，和模型本身训练有关（原公式没看懂。。）；$ \\beta $ 是一个理想化的项，表示最好情况下在目标域和源域上同时取得的最低错误率\nDANN 优化目标： $$ E(\\theta_f, \\theta_y, \\theta_d) = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_y(\\theta_f, \\theta_y) - \\lambda \\left( \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_d(\\theta_f, \\theta_d) + \\frac{1}{n'} \\sum_{i=n+1}^N \\mathcal{L}_d(\\theta_f, \\theta_d) \\right) $$ 核心是 Saddle Point Problem ，找到需要找到鞍点而非最小值\n如何实现对抗：\n标签预测参数： $$ \\theta_{y} \\leftarrow \\theta_{y} - \\mu \\dfrac{ \\partial \\mathcal{L}_{y} }{ \\partial \\theta_{y} } $$ 领域分类参数： $$ \\theta_{d} \\leftarrow \\theta_{d} - \\mu \\lambda \\dfrac{ \\partial \\mathcal{L}_{d} }{ \\partial \\theta_{d} } $$ 特征提取参数： $$ \\theta_{f} \\leftarrow \\theta_{f} - \\mu\\left( \\dfrac{ \\partial \\mathcal{L}_{y} }{ \\partial \\theta_{f} } - \\lambda \\dfrac{ \\partial \\mathcal{L}_{d} }{ \\partial \\theta_{f} } \\right) $$ 核心需要最大化 $ \\mathcal{L}_{d} $ ，因此需要沿着梯度的正向优化 Gradient Reversal Layer (GRL) 是实现对抗的核心组件，具体原理是在前向传播时表现为 $ R(x)=x $ ，但是反向传播时 $ \\dfrac{\\mathrm{d} R}{\\mathrm{d}x}=-I $ ，这样可以直接利用内置的自动微分优雅实现对抗\n","permalink":"https://diefish1024.github.io/posts/literature-notes/dann/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e类似 GAN 的对抗训练思想\u003c/p\u003e\n\u003ch2 id=\"domain-adaptation\"\u003eDomain Adaptation\u003c/h2\u003e\n\u003cp\u003e给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \\eta: X\\to Y $ 使其在目标域上的目标风险\n$$ \n\nR_{D_{T}}(\\eta) = \\underset{(\\mathbf{x},y)\\sim D_{T}}{\\mathrm{Pr}}(\\eta(\\mathbf{x}) \\neq y)\n\n $$\n最小\u003c/p\u003e\n\u003ch4 id=\"domain-divergence\"\u003eDomain Divergence\u003c/h4\u003e\n\u003cp\u003e需要量化两个领域的“相似度”，从而引出了 \u003cstrong\u003eH- 散度\u003c/strong\u003e 的概念：\n$$ \n\nd_{\\mathcal{H}}(D_S, D_T) = 2 \\sup_{\\eta \\in \\mathcal{H}} \\left| \\Pr_{x \\sim D_S}[\\eta(x) = 1] - \\Pr_{x \\sim D_T}[\\eta(x) = 1] \\right|\n\n $$\n含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果\u003c/p\u003e\n\u003cp\u003e由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度\n$$ \n\n\\hat{d}_{\\mathcal{H}}(S, T) = 2 \\left(1 - \\min_{\\eta \\in \\mathcal{H}} \\left[ \\dfrac{1}{n}\\sum_{i=1}^n \\mathcal{I}[\\eta(x_i) = 0] + \\dfrac{1}{n'}\\sum_{i=n+1}^N \\mathcal{I}[\\eta(x_i) = 1] \\right] \\right)\n\n $$\n其中 $ \\mathcal{I}[\\cdot] $ 表示条件为真时为 1，否则为 0\u003c/p\u003e","title":"DANN"},{"content":"推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\n1. What is KV Cache? KV Cache，全称 Key-Value Cache，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是缓存并重用在注意力机制中计算得到的 Key (K) 和 Value (V) 向量。\n2. Transformer Attention Mechanism Review 要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\n每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\nQ 向量：代表当前 token 的“查询”信息 K 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配 V 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出 自注意力机制的计算过程为以下步骤：\n计算 Query 与所有 Key 的点积，得到注意力分数 将注意力分数进行缩放，除以 $ \\sqrt{d_k} $（$ d_k $ 是 Key 向量的维度) 对缩放后的分数进行 Softmax，将其转换为注意力权重，表示每个 token 对当前 token 的重要性 将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出 公式为： $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$ 其中矩阵 $ Q,K,V \\in \\mathbb{R}^{L \\times d} $ ，$ L $ 为当前上下文长度\n（处于简洁性的考虑，忽略了 Causal Mask ，实际上 $ QK^{T} $ 应该 Mask 成下三角矩阵来强制不能看到序列未来的信息）\n3. The Problem KV Cache Solves 在大型语言模型中，当模型以自回归方式生成文本时（每次生成一个新 token，并将其添加到输入序列中，然后根据整个序列生成下一个 token），会遇到一个效率问题：\n假设我们要生成“中华人民”\n输入：“中” 模型计算“中”的 $ Q, K, V $ 计算 attention ，生成“华” 输入：“中华” 模型再次计算“中”和“华”的 $ Q, K, V $ 计算 attention ，生成“人” 输入：“中华人” 模型再次计算“中”、“华”和“人”的 $ Q, K, V $ 计算 attention ，生成“民” 可以看到，在每一步生成新 token 时，都需要重新计算之前已经处理过的所有 token 的 $ K $ 和 $ V $ 向量。这种重复计算在序列较长时会消耗大量的计算资源和时间，效率低下。\n4. How KV Cache Works 根据上面分析得到的问题，很容易想到 KV Cache 的核心思想：将已经计算过的 Key 和 Value 向量缓存起来，在后续的生成步骤中直接重用，而不是重新计算。\n以生成“中华人民”为例，使用 KV Cache 的流程如下：\n输入：“中” 计算“中”的 $ K_1, V_1 $ 将 $ K_1, V_1 $ 存入 KV Cache 使用 $ Q_1, K_1, V_1 $ 计算 attention ，生成“华” 输入：“华”（当前 token 只有“华”，但注意力要关注整个序列“中华”） 计算“华”的 $ K_2, V_2 $ 将 $ K_2, V_2 $ 添加到 KV Cache。此时 KV Cache 包含 $ [K_1, K_2] $ 和 $ [V_1, V_2] $ 使用当前 $ Q_2 $ 和缓存中的 $ [K_1, K_2], [V_1, V_2] $ 计算 attention ，生成“人” 输入：“人” 计算“人”的 $ K_3, V_3 $ 将 $ K_3, V_3 $ 添加到 KV Cache。此时 KV Cache 包含 $ [K_1, K_2, K_3] $ 和 $ [V_1, V_2, V_3] $ 使用当前 $ Q_3 $ 和缓存中的 $ [K_1, K_2, K_3], [V_1, V_2, V_3] $ 计算 attention ，生成“民” 通过这种方式，每一步只需要计算当前新生成 token 的 $ K, V $ 向量，而无需重新计算之前所有 token 的 $ K, V $。\n5. Why Not QKV Cache? 可能会好奇，既然 K 和 V 都需要缓存，为什么不也缓存 Q 呢？也就是说，为什么是 KV Cache 而不是 QKV Cache？\n原因在于 Q 向量的性质：\nQ 向量是用来“查询”当前 token 与序列中其他 token 的相关性的。在自回归生成过程中，每一步生成一个新的 token，这个新 token 对应的 Query 向量是新的，它基于当前步的隐藏状态计算得出。换句话说，每次生成新 token 时，其对应的 $ Q $ 向量都是独一无二的，并且需要重新计算以反映最新的生成上下文。 K 和 V 向量则代表了序列中每个 token 的“内容”信息。对于已经处理过的 token，它们的 $ K $ 和 $ V $ 向量一旦计算出来，其内容信息就是固定不变的。因此，这些 $ K $ 和 $ V $ 向量可以直接被缓存并反复使用，而无需重新计算。 因此，不缓存 Q 是因为它在每一步都是一个新的计算结果；而缓存 K 和 V 则可以显著减少重复计算，从而提高效率。\n6. KV Cache in Attention Mechanism 在数学上，当使用 KV Cache 进行自回归解码时，注意力公式中的 $ K $ 和 $ V $ 矩阵会随着生成过程的进行而不断增长。\n假设我们正在生成第 $ t $ 个 token。\n当前 token 的 Q 向量是 $ Q_t $ ，这是一个行向量，代表当前第 $ t $ 个 token 的 Query ，维度为 $ 1 \\times d_k $ K 矩阵 $ K_{\\text{cached}} $ 将包含从第一个 token 到第 $ t $ 个 token 的所有 K 向量： $ K_{\\text{cached}} = [K_1^T, K_2^T, \\dots, K_t^T]^T $ ，维度为 $ t \\times d_k $ V 矩阵 $ V_{\\text{cached}} $ 将包含从第一个 token 到第 $ t $ 个 token 的所有 V 向量： $ V_{\\text{cached}} = [V_1^T, V_2^T, \\dots, V_t^T]^T $ 。其维度为 $ t \\times d_v $ 那么，第 $ t $ 个 token 的注意力计算变为： $$ \\text{Attention}_{t}(Q_t, K_{\\text{cached}}, V_{\\text{cached}}) = \\text{softmax}\\left(\\frac{Q_t K_{\\text{cached}}^T}{\\sqrt{d_k}}\\right)V_{\\text{cached}} $$ 其中\n$ Q_t K_{\\text{cached}}^T $ 是一个 $ 1 \\times t $ 的行向量，代表当前 Query 与所有历史 Key 的相关性分数 $ \\text{softmax} $ 操作将这个 $ 1 \\times t $ 的向量转化为注意力权重 这个 $ 1 \\times t $ 的注意力权重向量再与 $ V_{\\text{cached}} $ 矩阵（维度 $ t \\times d_v $）相乘，得到最终的注意力输出，维度是 $ 1 \\times d_v $ 每次生成新的 token $ t+1 $ 时，我们只需要计算新的 $ Q_{t+1} $，将新计算的 $ K_{t+1} $ 和 $ V_{t+1} $ 拼接到 $ K_{\\text{cached}} $ 和 $ V_{\\text{cached}} $ 末尾，形成 $ K'_{\\text{cached}} = \\text{concat}(K_{\\text{cached}}, K_{t+1}) $ 和 $ V'_{\\text{cached}} = \\text{concat}(V_{\\text{cached}}, V_{t+1}) $\n7. Limitations and Considerations 尽管 KV Cache 带来了巨大的性能提升，但也存在一些问题：\n内存占用：KV Cache 需要存储所有已处理 token 的 Key 和 Value 向量。对于大型模型和长上下文序列，这些缓存可能非常大，导致显存（GPU Memory）成为瓶颈。 上下文长度限制：由于内存限制，KV Cache 会限制模型能够处理的最大上下文长度。一旦达到内存上限，就需要采取策略来管理缓存，例如丢弃最早的 Key/Value 对（类似于循环缓冲区），但这可能会影响模型对长距离依赖的理解。 Summary KV Cache 是 Transformer 模型在自回归推理过程中非常重要的一种优化技术。通过缓存并重用已经计算过的 Key 和 Value 向量，它极大地减少了重复计算，从而显著提升了大型语言模型的生成速度。\nReferences KV Cache 原理讲解 （Bilibili） 注意：此视频内容存在部分错误 看图学KV Cache（知乎） 为什么没有Q Cache（知乎） ","permalink":"https://diefish1024.github.io/posts/ai-infra/kv-cache-%E5%85%A5%E9%97%A8/","summary":"\u003cp\u003e推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\u003c/p\u003e\n\u003ch3 id=\"1-what-is-kv-cache\"\u003e1. What is KV Cache?\u003c/h3\u003e\n\u003cp\u003eKV Cache，全称 \u003cstrong\u003eKey-Value Cache\u003c/strong\u003e，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是\u003cstrong\u003e缓存\u003c/strong\u003e并\u003cstrong\u003e重用\u003c/strong\u003e在注意力机制中计算得到的 \u003cstrong\u003eKey (K)\u003c/strong\u003e 和 \u003cstrong\u003eValue (V)\u003c/strong\u003e 向量。\u003c/p\u003e\n\u003ch3 id=\"2-transformer-attention-mechanism-review\"\u003e2. Transformer Attention Mechanism Review\u003c/h3\u003e\n\u003cp\u003e要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\u003c/p\u003e\n\u003cp\u003e每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ 向量：代表当前 token 的“查询”信息\u003c/li\u003e\n\u003cli\u003eK 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配\u003c/li\u003e\n\u003cli\u003eV 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e自注意力机制的计算过程为以下步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e计算 Query 与所有 Key 的点积，得到\u003cstrong\u003e注意力分数\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e将注意力分数进行缩放，除以 $ \\sqrt{d_k} $（$ d_k $ 是 Key 向量的维度)\u003c/li\u003e\n\u003cli\u003e对缩放后的分数进行 Softmax，将其转换为\u003cstrong\u003e注意力权重\u003c/strong\u003e，表示每个 token 对当前 token 的重要性\u003c/li\u003e\n\u003cli\u003e将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e公式为：\n$$ \n\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\n $$\n其中矩阵 $ Q,K,V \\in \\mathbb{R}^{L \\times d} $ ，$ L $ 为当前上下文长度\u003c/p\u003e","title":"KV Cache 入门"},{"content":"Introduction TTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征\nProblem Setting 考虑一个回归模型 $ f_\\theta: \\mathcal{X} \\to \\mathbb{R} $，可以进一步分解为特征提取器 $ g_\\phi: \\mathcal{X} \\to \\mathbb{R}^D $（从输入 $ \\mathcal{X} $ 提取 $ D $ 维特征 $ z $）和线性回归器 $ h_\\psi(z) = w^T z + b $（或者 $ h_{\\psi}(z)=Wz+b $）\n$ f_\\theta $ 首先在一个有标签的源数据集 $ S = \\{(x_i, y_i)\\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样\n目标是使用一个无标签的目标数据集 $ T = \\{x_j\\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\\theta $ 到目标域\n我们假设存在 covariate shift ，这意味着：\n输入数据的分布在源域和目标域之间是不同的：$ p_s(x) \\neq p_t(x) $ 但给定输入后，输出的条件分布是相同的：$ p_s(y|x) = p_t(y|x) $ Test-time Adaptation for Regression Basic Idea: Feature Alignment 朴素实现：\n计算源域特征统计量：在源域训练后，计算源域特征的均值 $ \\mu^s $ 和元素级方差 $ \\sigma^{s2} $ $$ \\mu^s = \\frac{1}{N_s} \\sum_{i=1}^{N_s} z_i^s, \\quad \\sigma^{s2} = \\frac{1}{N_s} \\sum_{i=1}^{N_s} (z_i^s - \\mu^s) \\odot (z_i^s - \\mu^s) \\quad \\text{(1)} $$ 其中 $ z_i^s = g_\\phi(x_i) $ 是源特征，$ N_s $ 是源数据样本数，$ \\odot $ 表示元素级乘积\n目标域特征统计量：在目标域，对每个迷你批次（mini-batch）$ B = \\{x_j\\}_{j=1}^{N_B} $，计算其特征均值 $ \\hat{\\mu}^t $ 和方差 $ \\hat{\\sigma}^{t2} $，计算方式与公式 (1) 类似\n对齐损失函数：使用 KL 散度来衡量两个对角高斯分布 $ N(\\mu^s, \\sigma^{s2}) $ 和 $ N(\\hat{\\mu}^t, \\hat{\\sigma}^{t2}) $ 之间的差异，并最小化该差异。 $$ L_{TTA} (\\phi) = \\frac{1}{2} \\sum_{d=1}^D \\left\\{ D_{KL} (N(\\mu^s_d, \\sigma^s_{d}{}^2)||N(\\hat{\\mu}^t_d, \\hat{\\sigma}^t_{d}{}^2)) + D_{KL} (N(\\hat{\\mu}^t_d, \\hat{\\sigma}^t_{d}{}^2)||N(\\mu^s_d, \\sigma^s_{d}{}^2)) \\right\\} \\quad \\text{(2)} $$ 这里的 $ d $ 表示向量的第 $ d $ 个元素。之所以使用双向的 KL 散度，是为了经验上获得更好的结果\n一维高斯 KL 散度公式： $$ D_{KL} (N(\\mu_1, \\sigma_1^2)||N(\\mu_2, \\sigma_2^2)) = \\dfrac{\\left[ \\log(\\sigma_2^2/\\sigma_1^2) + \\dfrac{(\\mu_1 - \\mu_2)^2 + \\sigma_1^2}{\\sigma_2^2} - 1 \\right]}{2} \\quad \\text{(3)} $$\n朴素对齐的问题：\n回归模型特征倾向于分布在一个小型的子空间中，许多特征维度方差为零或接近零 公式 (3) 中涉及到方差在分母上，使得这种朴素对齐在面对零方差维度时变得不稳定 对所有维度“一视同仁”地对齐不适用于回归任务的特性，因为许多维度对最终输出影响很小 Significant-subspace Alignment SSA 的三个步骤：\n子空间检测 (Subspace detection)：\n在源数据集 $ S $ 上进行训练后，检测源特征分布所在的子空间。不计算每个维度的方差，而是计算协方差矩阵： $$ \\Sigma^s = \\frac{1}{N_s} \\sum_{i=1}^{N_s} (z_i^s - \\mu^s) (z_i^s - \\mu^s)^T \\quad \\text{(4)} $$ 其中 $ \\mu^s $ 是源特征的均值向量（同理 (1)） 基于 PCA 的思想，通过对 $ \\Sigma^s $ 进行特征分解，得到特征向量 $ v_d^s $ 和对应的特征值 $ \\lambda_d^s $ 选取前 K 个最大的特征值 $ \\lambda_1^s, \\dots, \\lambda_K^s $ 及其对应的源基向量 $ v_1^s, \\dots, v_K^s $ 来定义源子空间，这些基向量张成的子空间代表了源特征数据最有代表性和最重要的变化方向 维度加权 (Dimension weighting)：\n考虑到回归模型 $ h_\\psi(z)=w^T z + b $，子空间维度 $ v_d^s $ 对最终输出的影响由 $ w^T v_d^s $ 决定（即特征向量与回归器权重向量的点积） 为了优先对齐那些对输出影响更大的子空间维度，为每个子空间维度 $ d $ 定义权重 $ a_d $： $$ a_d = 1 + |w^T v_d^s| \\quad \\text{(5)} $$ 这个权重 $ a_d $ 会在对应的子空间基方向对输出有较大影响时值更大（最小为 1）。 特征对齐 (Feature alignment)：\n这一步在目标域进行。对于目标域的迷你批次 $ B $，首先将目标特征 $ z^t = g_\\phi(x^t) $ 投影到源子空间。 $$ \\tilde{z}^t = V_s^T (z^t - \\mu^s) \\quad \\text{(6)} $$ 其中 $ V_s = [v_1^s, \\dots, v_K^s] \\in \\mathbb{R}^{D \\times K} $ 是由前 K 个源基向量构成的矩阵，$ \\tilde{z}^t \\in \\mathbb{R}^K $ 是投影后的目标特征。 然后，计算投影后目标特征的迷你批次均值 $ \\tilde{\\mu}^t $ 和方差 $ \\tilde{\\sigma}^{t2} $ （同理公式 (1) ） 最后，使用结合子空间检测和维度加权的新损失函数来最小化目标特征分布与源特征分布在子空间中的差异。源域投影后的均值是 0，方差是其特征值 $ \\Lambda^s = [\\lambda_1^s, \\dots, \\lambda_K^s] $。 $$ \\begin{align}L_{TTA}(\\phi) = \u0026 \\frac{1}{2} \\sum_{d=1}^K a_d \\left\\{ D_{KL} (N(0, \\lambda^s_d)||N(\\tilde{\\mu}^t_d, \\tilde{\\sigma}^t_{d}{}^2)) + D_{KL} (N(\\tilde{\\mu}^t_d, \\tilde{\\sigma}^t_{d}{}^2)||N(0, \\lambda^s_d)) \\right\\} \\\\ = \u0026 \\sum_{d=1}^K a_d \\left\\{ \\frac{(\\tilde{\\mu}^t_d)^2 + \\lambda^s_d}{2\\tilde{\\sigma}^t_{d}{}^2} + \\frac{(\\tilde{\\mu}^t_d)^2 + \\tilde{\\sigma}^t_{d}{}^2}{2\\lambda^s_d} - 1 \\right\\} \\quad \\text{(7)} \\end{align} $$ 其中 $ a_d $ 是维度权重，$ \\lambda_d^s $ 是源域子空间的第 $ d $ 个特征值，$ \\tilde{\\mu}_d^t $ 和 $ \\tilde{\\sigma}_{d}{}^2 $ 是投影后的目标特征在第 $ d $ 个维度上的均值和方差 伪代码：\n输入：预训练好的源模型 $ f_\\theta $、源基向量 $ V_s $、源均值 $ \\mu^s $、源方差 $ \\Lambda^s $、目标数据集 $ T $ 输出：适应后的模型 $ f_\\phi^t $ 步骤： 计算源子空间中每个维度的权重 $ a_d $ 对于目标数据集 $ T $ 中的每个 mini batch $ \\{x\\}_i^B $： 提取目标特征 $ z = g_\\phi(x) $。 将目标特征投影到源子空间 $ \\tilde{z} $ 计算投影后目标特征的均值 $ \\tilde{\\mu}^t $ 和方差 $ \\tilde{\\sigma}^{t2} $ 更新特征提取器 $ g_\\phi $ 以最小化损失函数 $ L_{TTA}(\\phi) $ 重复直到收敛。 对角高斯分布的合理性 为什么假设特征分布为对角高斯分布是合理的：\n中心极限定理：当特征被投影到子空间后，如果原始特征维度 $ D $ 足够大，根据中心极限定理，投影后的特征分布会倾向于高斯分布。 PCA 的去相关性：由于子空间检测使用了 PCA，投影到主成分上的特征是去相关的，这意味着不同维度之间是独立的，这使得对角高斯分布的假设（即各维度独立）变得合理。 Appendix A. LIMITATION：SSA 假设是协变量偏移，即 $ p(y|x) $ 不变，未来工作将考虑 $ p(y|x) $ 变化的情况\nB. EVALUATION METRIC：R²接近 1 表示模型拟合效果好 $$ R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} \\quad \\text{(10)} $$ 其中 $ \\hat{y}_i $ 是预测值，$ y_i $ 是真实值，$ \\bar{y} $ 是真实值的平均值。\nD. ADDITIONAL EXPERIMENTAL RESULTS：\nD.1 特征对齐的度量：比较了 KL 散度、2WD 和 L1 范数作为特征对齐损失的效果，结果显示 KL 散度结合子空间检测（SSA）表现最佳。 公式 (11)：2-Wasserstein Distance for Gaussians $$ W_2^2 (N(\\mu_1, \\sigma_1^2), N(\\mu_2, \\sigma_2^2)) = (\\mu_1 - \\mu_2)^2 + (\\sigma_1 - \\sigma_2)^2 $$ 公式 (12)：L1 Norm of Statistics $$ L_1 (N(\\mu_1, \\sigma_1^2), N(\\mu_2, \\sigma_2^2)) = |\\mu_1 - \\mu_2| + |\\sigma_1 - \\sigma_2| $$ 公式 (13)：SSA Loss with 2WD $$ L_{TTA-2WD} = \\sum_{d=1}^K a_d \\left\\{ (\\tilde{\\mu}^t_d)^2 + (\\tilde{\\sigma}^t_d - \\sqrt{\\lambda^s_d})^2 \\right\\} $$ 公式 (14)：SSA Loss with L1 Norm $$ L_{TTA-L1} = \\sum_{d=1}^K a_d \\left\\{ |\\tilde{\\mu}^t_d| + |\\tilde{\\sigma}^t_d - \\sqrt{\\lambda^s_d}| \\right\\} $$ D.2 特征可视化：通过 PCA 和 UMAP 等降维技术可视化了源域和目标域特征分布（图 4-5），直观地展示了 SSA 如何成功地将目标特征分布拉近源域。 D.3 原始特征维度对子空间的影响：分析了原始特征维度对子空间的重要性。 公式 (15)：Gradient Norm $ s_d $ $$ s_d = ||\\frac{\\partial \\tilde{z}}{\\partial z_d}||_2 = ||(V_s^T)_d||_2 = ||[v_{1,d}^s, \\dots, v_{K,d}^s]||_2, $$ 其中 $ (V_s^T)_d $ 是 $ V_s^T $ 的第 $ d $ 行。 发现：回归模型的特征子空间确实受许多原始特征维度影响很小（图 6），这进一步确认了子空间检测的必要性。 D.4 附加消融实验：进一步证实了子空间检测对于 SSA 性能的重要性（表 13-14）。 D.5 Vision Transformer 实验：在 Vision Transformer 上验证了 SSA 的有效性（表 15-16），表明该方法对不同模型架构也适用。 D.6 多任务回归模型：将 SSA 应用于多任务回归，模型同时输出多个预测值（如头部姿态的俯仰、偏航、滚转角度），结果表明 SSA 同样有效（表 17）。 D.7 与分类 TTA 结合：探索了 SSA 与分类 TTA 结合的可能性（表 18-20）。 D.8 超参数敏感性：分析了学习率和批次大小等超参数对 SSA 性能的影响（表 21-26），发现 SSA 在典型参数范围内表现稳定。 D.9 额外结果：提供了 MAE 等其他指标的性能数据（表 27-28）。 D.10 在线设置：SSA 在分批在线（batched online）设置下也表现出色（表 29-31）。 ","permalink":"https://diefish1024.github.io/posts/literature-notes/ssa/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eTTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征\u003c/p\u003e\n\u003ch2 id=\"problem-setting\"\u003eProblem Setting\u003c/h2\u003e\n\u003cp\u003e考虑一个回归模型 $ f_\\theta: \\mathcal{X} \\to \\mathbb{R} $，可以进一步分解为\u003cstrong\u003e特征提取器\u003c/strong\u003e $ g_\\phi: \\mathcal{X} \\to \\mathbb{R}^D $（从输入 $ \\mathcal{X} $ 提取 $ D $ 维特征 $ z $）和\u003cstrong\u003e线性回归器\u003c/strong\u003e $ h_\\psi(z) = w^T z + b $（或者 $ h_{\\psi}(z)=Wz+b $）\u003c/p\u003e\n\u003cp\u003e$ f_\\theta $ 首先在一个有标签的\u003cstrong\u003e源数据集\u003c/strong\u003e $ S = \\{(x_i, y_i)\\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样\u003c/p\u003e\n\u003cp\u003e目标是使用一个\u003cstrong\u003e无标签的\u003c/strong\u003e目标数据集 $ T = \\{x_j\\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\\theta $ 到目标域\u003c/p\u003e\n\u003cp\u003e我们假设存在 \u003cstrong\u003ecovariate shift\u003c/strong\u003e ，这意味着：\u003c/p\u003e","title":"SSA"},{"content":"Method Problem Set EEG 数据 $ \\{ X_{s,l}^{i},y_{s,l}^{i} \\}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类\nSource Model Training 对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异\nEA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值 $$ R_{s,l} = \\dfrac{1}{n}\\sum_{i=1}^{n} X_{i}(X_{i})^{T} \\implies \\bar{X}_{i} = R_{s,l}^{-1/2}X_{i} $$ 之后再整合经过对齐的受试者数据，形成“源域”\n在整合后的数据上独立训练 $ M $ 个模型\nIncremental EA on Target Data 对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据\nTarget Label Prediction 用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $\n新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}' $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}') $\n之后结合这 $ M $ 个概率向量来获得最终的预测标签 $ \\hat{y}_{a} $\n$ a\\leq M $ 数据量较少：直接对所有模型的预测向量平均 $ a\u003eM $ 数据量较多：使用谱元学习器对各个模型进行加权平均，根据历史表现（预测的协方差矩阵）分配不同的权重 Target Model Update 在数据量足够以后（$ a\u003eB $）使用一个滑动批次的数据更新模型，在此之前模型不变\n组合损失函数： $$ L_{M} = L_{CEM}(f_{m};\\{ X'_{i} \\}_{i=a-B+1}^{a}) + L_{MDR}(f_{m};\\{ X'_{i} \\}_{i=a-B+1}^{a}) $$ 有两个部分\n1) Conditional Entropy Minimization 条件熵最小化\n使分类边界更加清晰 通过最小化每个预测的条件熵（使用温度缩放因子 $ T $ 进行校准），使模型倾向于输出接近 0 或 1 的概率 2) Adaptive Marginal Distribution Regularization 自适应边缘分布正则化\n防止出现所有数据都在单类别和对错误结果过于自信的不良结果 计算当前批次每个类别的平均预测概率 $ p_{k} $ 通过设置阈值得到伪标签，估计目标域的类别评论 $ z_{k} $ 校准平均预测概率 $ q'_{k} $ $$ q_{k} = \\dfrac{p_{k}}{c+z_{k}},\\quad q'_{k} = \\dfrac{q_{k}}{\\sum q} $$ $ L_{MDR} = \\sum_{k=1}^{K}q'_{k}\\log q'_{k} $ （采用负熵的形式） Complete T-TIME Algorithm 先预测，后台并行地更新模型\nExperiment 使用三个运动想象数据集\n每次把一个受试者的数据作为目标域，其余作为源域\nClassification Accuracies on Balanced Classes 过于复杂的算法由于数据不足，性能反而下降 基于熵的方法普遍表现良好，MCC 在离线迁移学习中表现最好 T-TIME 在所有在线迁移学习算法中表现最佳，并且其性能与表现最佳的离线迁移学习方法相当 Classification Performance Under Class-Imbalance 使用随机移除数据来创建不平衡数据集\n传统方法表现较弱 T-TIME 表现突出 ","permalink":"https://diefish1024.github.io/posts/literature-notes/t-time/","summary":"\u003ch1 id=\"method\"\u003eMethod\u003c/h1\u003e\n\u003ch3 id=\"problem-set\"\u003eProblem Set\u003c/h3\u003e\n\u003cp\u003eEEG 数据 $ \\{ X_{s,l}^{i},y_{s,l}^{i} \\}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类\u003c/p\u003e\n\u003ch3 id=\"source-model-training\"\u003eSource Model Training\u003c/h3\u003e\n\u003cp\u003e对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异\u003c/p\u003e\n\u003cp\u003eEA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值\n$$ \n\nR_{s,l} = \\dfrac{1}{n}\\sum_{i=1}^{n} X_{i}(X_{i})^{T} \\implies \\bar{X}_{i} = R_{s,l}^{-1/2}X_{i}\n\n $$\n之后再整合经过对齐的受试者数据，形成“源域”\u003c/p\u003e\n\u003cp\u003e在整合后的数据上独立训练 $ M $ 个模型\u003c/p\u003e\n\u003ch3 id=\"incremental-ea-on-target-data\"\u003eIncremental EA on Target Data\u003c/h3\u003e\n\u003cp\u003e对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据\u003c/p\u003e\n\u003ch3 id=\"target-label-prediction\"\u003eTarget Label Prediction\u003c/h3\u003e\n\u003cp\u003e用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $\u003c/p\u003e\n\u003cp\u003e新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}' $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}') $\u003c/p\u003e","title":"T-TIME"},{"content":"Setting Fully Test-Time Adaptation 是一种独特的模型适应设定。在此设定下，模型 $ f_\\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \\theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。\nFTT-Adaptation 与以下方法不同：\nFine-tuning：需要目标标签进行重新训练。 Domain Adaptation：需要源数据和目标数据进行联合训练。 Test-Time Training (TTT)：需要修改训练过程并共同优化有监督及自监督损失。 相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。\nMethod 论文的核心贡献是提出了 Tent 方法，其核心思想是通过最小化测试熵（Test Entropy Minimization）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。\nEntropy Objective Tent 的测试时目标函数是最小化模型预测 $ \\hat{y} = f_\\theta(x^t) $ 的熵 $ H(\\hat{y}) $。论文中使用的香农熵计算公式如下：\n$$ H(\\hat{y}) = - \\sum_c p(\\hat{y}_c) \\log p(\\hat{y}_c) $$ 其中， $ p(\\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。\n最小化熵促使模型输出更“尖锐”或更“确定”的预测分布。 优势：熵是一种无监督目标，仅依赖于模型预测，不需要真实标签。最小化熵与减少预测误差和数据漂移之间存在内在联系，因为更确定的预测通常意味着更正确的预测。 Modulation Parameters Tent 不直接修改原始模型的全部参数 $ \\theta $。相反，它仅更新模型内部归一化层（如Batch Normalization layers）中的线性且低维度的仿射变换参数：尺度参数 $ \\gamma $ 和偏移参数 $ \\beta $。\n这一选择的理由是：这些参数只占模型总参数的极小部分（\u0026lt;1%），优化效率高且稳定。 特征调制过程包含两个步骤： 1.Normalization (标准化)：根据当前批次测试数据的均值 $ \\mu $ 和标准差 $ \\sigma $ 来标准化特征 $ x $，即 $ \\hat{x} = (x - \\mu)/\\sigma $。这里的 $ \\mu, \\sigma $ 是在测试时从当前批次数据中估计的。 2.Transformation (仿射变换)：对标准化后的特征 $ \\hat{x} $ 应用仿射变换，即 $ x' = \\gamma \\hat{x} + \\beta $。参数 $ \\gamma $ 和 $ \\beta $ 通过最小化熵目标函数进行优化。 Algorithm Tent 算法的流程如下：\nInitialization： 加载预训练好的源模型参数 $ \\theta $。 固定所有非仿射变换的参数。 丢弃源数据中估计的归一化统计量。 优化器收集所有归一化层的通道级仿射变换参数 $ \\{\\gamma_{l,k}, \\beta_{l,k}\\} $。 Iteration：在线处理数据批次。 Forward Pass：对每个数据批次，逐层估计该批次数据的归一化统计量 ($ \\mu, \\sigma $)。 Backward Pass：计算预测熵 $ H(\\hat{y}) $ 相对于仿射变换参数 $ \\gamma, \\beta $ 的梯度 $ \\nabla H(\\hat{y}) $。 Update：使用梯度更新 $ \\gamma, \\beta $ 参数。Tent 采用高效的在线更新策略，每次更新只影响下一个批次的数据处理。 Termination：对于在线适应，适应过程只要有测试数据就持续进行。对于离线适应，模型会先进行更新，然后重复推断，适应可以持续多个Epochs。 Experiments 论文在多种计算机视觉任务和数据集上对 Tent 进行了全面评估。\nRobustness To Corruptions 在图像分类的鲁棒性基准测试中，使用受损版本的 CIFAR-10/100-C 和 ImageNet-C 数据集（15 种损坏类型，不同严重程度）。\n主要发现： Tent 在 ImageNet-C 上达到了 44.0% 的最低错误率，优于 SOTA 鲁棒性训练方法（如Adversarial Noise Training (ANT) 的 50.2%）和Test-Time Normalization (BN) 基线（49.9%）。 在 CIFAR-10/100-C 上，Tent 也显著优于其他 TTA baseline（BN, Pseudo-Labeling (PL)）以及需要联合训练源域和目标域的Domain Adaptation（RG, UDA-SS）和Test-Time Training (TTT) 方法。 这些改进仅通过一次Epoch的测试时优化实现，且未改变原始模型训练。 Source-Free Domain Adaptation 评估 Tent 在无源域适应场景下的性能，包括数字识别（从 SVHN 到 MNIST/MNIST-M/USPS）和语义分割（从 GTA 到 Cityscapes）。\n主要发现： 在数字识别任务中，Tent 大多数情况下错误率低于源模型和BN，部分情况甚至优于需要源数据的Domain Adaptation方法（RG, UDA-SS）。 语义分割任务中，Tent 将Intersection-Over-Union (IOU) 分数从源模型的 28.8% 提高到 35.8%，显著优于 BN 的 31.4%。 Analysis 论文通过多项分析实验探究了 Tent 的工作原理和特性：\nTent 降低熵和误差：实验证实，Tent 成功降低了预测的熵值和任务损失（如Softmax Cross-Entropy），印证了熵最小化与误差减少之间的正相关性。 Tent 需要特征调制：不更新归一化统计量或不优化仿射变换参数会显著降低 Tent 性能，说明这些特征调制步骤对于适应不可或缺。 Tent 泛化到不同的目标数据：适应过程对未用于更新的其他测试数据点同样有效，表明其学习到的调制是通用的。 Tent 调制与归一化不同：对比分析显示，Tent 的特征调制使特征更接近在目标标签上优化的Oracle模型（理想模型），而非仅像Batch Normalization那样接近原始参考分布。 Tent 适应其他网络架构：Tent 在基于Self-Attention 和Equilibrium Solving (MDEQ) 的模型上也能有效降低误差，展现了其普适性。 Related Work 论文回顾了与 Tent 相关的现有工作：\nTrain-Time Adaptation 方法：传统的Domain Adaptation、Test-Time Training (TTT) 等，通常需要源数据或训练阶段修改模型。 Source-Free Adaptation 方法：近期一些不依赖源数据的方法，但通常需要更复杂的设计、离线优化或修改训练过程。Tent 的优势在于其在线、高效且不改变训练过程。 Entropy Minimization：熵最小化已被广泛用于Semi-Supervised Learning和Domain Adaptation的正则化项，但 Tent 首次将其作为Fully Test-Time Adaptation中唯一的无监督损失来驱动模型适应。 Feature Modulation：归一化层和仿射变换已被用于各种任务的特征调制，但 Tent 将其作为在测试时通过无监督目标进行优化的核心机制。 Discussion Tent 通过Test Entropy Minimization实现了在数据漂移情况下的泛化误差降低。其核心在于模型的自监督自我改进，即依据自身的预测反馈进行调整。\n优势总结： 高效：仅通过在线优化少数参数（$ \\gamma, \\beta $）实现。 实用：无需源数据访问，不改变模型训练过程。 通用：适用于多种数据漂移类型和不同网络架构。 尽管 Tent 在广泛的场景中表现出色，但仍存在挑战，例如在特定困难的数据漂移（如 SVHN 到 MNIST-M/USPS）上仍有提升空间。未来研究方向可探索更全面的参数调整、更通用的Test-Time Adaptation Loss以及进一步提升效率的方法。总而言之，Tent 为Fully Test-Time Adaptation 提供了一个创新且实用的范式，使得模型能够在部署后，在面对未知且无标签的测试数据时，具备强大的自我适应能力。\n","permalink":"https://diefish1024.github.io/posts/literature-notes/tent/","summary":"\u003ch1 id=\"setting\"\u003eSetting\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eFully Test-Time Adaptation\u003c/strong\u003e 是一种独特的模型适应设定。在此设定下，模型 $ f_\\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \\theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。\u003c/p\u003e\n\u003cp\u003eFTT-Adaptation 与以下方法不同：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eFine-tuning\u003c/strong\u003e：需要目标标签进行重新训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain Adaptation\u003c/strong\u003e：需要源数据和目标数据进行联合训练。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest-Time Training (TTT)\u003c/strong\u003e：需要修改训练过程并共同优化有监督及自监督损失。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。\u003c/p\u003e\n\u003ch2 id=\"method\"\u003eMethod\u003c/h2\u003e\n\u003cp\u003e论文的核心贡献是提出了 \u003cstrong\u003eTent\u003c/strong\u003e 方法，其核心思想是通过\u003cstrong\u003e最小化测试熵\u003c/strong\u003e（\u003cstrong\u003eTest Entropy Minimization\u003c/strong\u003e）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。\u003c/p\u003e\n\u003ch3 id=\"entropy-objective\"\u003eEntropy Objective\u003c/h3\u003e\n\u003cp\u003eTent 的测试时目标函数是最小化模型预测 $ \\hat{y} = f_\\theta(x^t) $ 的\u003cstrong\u003e熵 $ H(\\hat{y}) $\u003c/strong\u003e。论文中使用的\u003cstrong\u003e香农熵\u003c/strong\u003e计算公式如下：\u003c/p\u003e\n$$ \n\nH(\\hat{y}) = - \\sum_c p(\\hat{y}_c) \\log p(\\hat{y}_c)\n\n $$\n\u003cp\u003e其中， $ p(\\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。\u003c/p\u003e","title":"Tent"}]