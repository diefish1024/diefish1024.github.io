[{"content":"推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\n1. What is KV Cache? KV Cache，全称 Key-Value Cache，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是缓存并重用在注意力机制中计算得到的 Key (K) 和 Value (V) 向量。\n2. Transformer Attention Mechanism Review 要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\n每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\nQ 向量：代表当前 token 的“查询”信息 K 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配 V 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出 自注意力机制的计算过程为以下步骤：\n计算 Query 与所有 Key 的点积，得到注意力分数 将注意力分数进行缩放，除以 $\\sqrt{d_k}$（$d_k$ 是 Key 向量的维度) 对缩放后的分数进行 Softmax，将其转换为注意力权重，表示每个 token 对当前 token 的重要性 将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出 公式为： $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$ 其中矩阵 $Q,K,V \\in \\mathbb{R}^{L \\times d}$ ，$L$ 为当前上下文长度\n（处于简洁性的考虑，忽略了 Causal Mask ，实际上 $QK^{T}$ 应该 Mask 成下三角矩阵来强制不能看到序列未来的信息）\n3. The Problem KV Cache Solves 在大型语言模型中，当模型以自回归方式生成文本时（每次生成一个新 token，并将其添加到输入序列中，然后根据整个序列生成下一个 token），会遇到一个效率问题：\n假设我们要生成“中华人民”\n输入：“中” 模型计算“中”的 $Q, K, V$ 计算 attention ，生成“华” 输入：“中华” 模型再次计算“中”和“华”的 $Q, K, V$ 计算 attention ，生成“人” 输入：“中华人” 模型再次计算“中”、“华”和“人”的 $Q, K, V$ 计算 attention ，生成“民” 可以看到，在每一步生成新 token 时，都需要重新计算之前已经处理过的所有 token 的 $K$ 和 $V$ 向量。这种重复计算在序列较长时会消耗大量的计算资源和时间，效率低下。\n4. How KV Cache Works 根据上面分析得到的问题，很容易想到 KV Cache 的核心思想：将已经计算过的 Key 和 Value 向量缓存起来，在后续的生成步骤中直接重用，而不是重新计算。\n以生成“中华人民”为例，使用 KV Cache 的流程如下：\n输入：“中” 计算“中”的 $K_1, V_1$ 将 $K_1, V_1$ 存入 KV Cache 使用 $Q_1, K_1, V_1$ 计算 attention ，生成“华” 输入：“华”（当前 token 只有“华”，但注意力要关注整个序列“中华”） 计算“华”的 $K_2, V_2$ 将 $K_2, V_2$ 添加到 KV Cache。此时 KV Cache 包含 $[K_1, K_2]$ 和 $[V_1, V_2]$ 使用当前 $Q_2$ 和缓存中的 $[K_1, K_2], [V_1, V_2]$ 计算 attention ，生成“人” 输入：“人” 计算“人”的 $K_3, V_3$ 将 $K_3, V_3$ 添加到 KV Cache。此时 KV Cache 包含 $[K_1, K_2, K_3]$ 和 $[V_1, V_2, V_3]$ 使用当前 $Q_3$ 和缓存中的 $[K_1, K_2, K_3], [V_1, V_2, V_3]$ 计算 attention ，生成“民” 通过这种方式，每一步只需要计算当前新生成 token 的 $K, V$ 向量，而无需重新计算之前所有 token 的 $K, V$。\n5. Why Not QKV Cache? 可能会好奇，既然 K 和 V 都需要缓存，为什么不也缓存 Q 呢？也就是说，为什么是 KV Cache 而不是 QKV Cache？\n原因在于 Q 向量的性质：\nQ 向量是用来“查询”当前 token 与序列中其他 token 的相关性的。在自回归生成过程中，每一步生成一个新的 token，这个新 token 对应的 Query 向量是新的，它基于当前步的隐藏状态计算得出。换句话说，每次生成新 token 时，其对应的 $Q$ 向量都是独一无二的，并且需要重新计算以反映最新的生成上下文。 K 和 V 向量则代表了序列中每个 token 的“内容”信息。对于已经处理过的 token，它们的 $K$ 和 $V$ 向量一旦计算出来，其内容信息就是固定不变的。因此，这些 $K$ 和 $V$ 向量可以直接被缓存并反复使用，而无需重新计算。 因此，不缓存 Q 是因为它在每一步都是一个新的计算结果；而缓存 K 和 V 则可以显著减少重复计算，从而提高效率。\n6. KV Cache in Attention Mechanism 在数学上，当使用 KV Cache 进行自回归解码时，注意力公式中的 $K$ 和 $V$ 矩阵会随着生成过程的进行而不断增长。\n假设我们正在生成第 $t$ 个 token。\n当前 token 的 Q 向量是 $Q_t$ ，这是一个行向量，代表当前第 $t$ 个 token 的 Query ，维度为 $1 \\times d_k$ K 矩阵 $K_{\\text{cached}}$ 将包含从第一个 token 到第 $t$ 个 token 的所有 K 向量： $K_{\\text{cached}} = [K_1^T, K_2^T, \\dots, K_t^T]^T$ ，维度为 $t \\times d_k$ V 矩阵 $V_{\\text{cached}}$ 将包含从第一个 token 到第 $t$ 个 token 的所有 V 向量： $V_{\\text{cached}} = [V_1^T, V_2^T, \\dots, V_t^T]^T$ 。其维度为 $t \\times d_v$ 那么，第 $t$ 个 token 的注意力计算变为： $$ \\text{Attention}_{t}(Q_t, K_{\\text{cached}}, V_{\\text{cached}}) = \\text{softmax}\\left(\\frac{Q_t K_{\\text{cached}}^T}{\\sqrt{d_k}}\\right)V_{\\text{cached}} $$ 其中\n$Q_t K_{\\text{cached}}^T$ 是一个 $1 \\times t$ 的行向量，代表当前 Query 与所有历史 Key 的相关性分数 $\\text{softmax}$ 操作将这个 $1 \\times t$ 的向量转化为注意力权重 这个 $1 \\times t$ 的注意力权重向量再与 $V_{\\text{cached}}$ 矩阵（维度 $t \\times d_v$）相乘，得到最终的注意力输出，维度是 $1 \\times d_v$ 每次生成新的 token $t+1$ 时，我们只需要计算新的 $Q_{t+1}$，将新计算的 $K_{t+1}$ 和 $V_{t+1}$ 拼接到 $K_{\\text{cached}}$ 和 $V_{\\text{cached}}$ 末尾，形成 $K\u0026rsquo;_{\\text{cached}} = \\text{concat}(K_{\\text{cached}}, K_{t+1})$ 和 $V\u0026rsquo;_{\\text{cached}} = \\text{concat}(V_{\\text{cached}}, V_{t+1})$\n7. Limitations and Considerations 尽管 KV Cache 带来了巨大的性能提升，但也存在一些问题：\n内存占用：KV Cache 需要存储所有已处理 token 的 Key 和 Value 向量。对于大型模型和长上下文序列，这些缓存可能非常大，导致显存（GPU Memory）成为瓶颈。 上下文长度限制：由于内存限制，KV Cache 会限制模型能够处理的最大上下文长度。一旦达到内存上限，就需要采取策略来管理缓存，例如丢弃最早的 Key/Value 对（类似于循环缓冲区），但这可能会影响模型对长距离依赖的理解。 Summary KV Cache 是 Transformer 模型在自回归推理过程中非常重要的一种优化技术。通过缓存并重用已经计算过的 Key 和 Value 向量，它极大地减少了重复计算，从而显著提升了大型语言模型的生成速度。\nReferences KV Cache 原理讲解 （Bilibili） 注意：此视频内容存在部分错误 看图学KV Cache（知乎） 为什么没有Q Cache（知乎） ","permalink":"https://diefish1024.github.io/posts/notes/ai-infra/kv-cache-%E5%85%A5%E9%97%A8/","summary":"\u003cp\u003e推理效率对于 llm 是一个至关重要的问题。当模型生成文本时，尤其是以自回归方式逐词生成时，效率瓶颈会变得非常明显。KV Cache 就是为了解决这一问题而诞生的技术。\u003c/p\u003e\n\u003ch3 id=\"1-what-is-kv-cache\"\u003e1. What is KV Cache?\u003c/h3\u003e\n\u003cp\u003eKV Cache，全称 \u003cstrong\u003eKey-Value Cache\u003c/strong\u003e，是一种优化技术，用于加速 Transformer 架构在自回归生成过程中的推理速度。它的核心思想是\u003cstrong\u003e缓存\u003c/strong\u003e并\u003cstrong\u003e重用\u003c/strong\u003e在注意力机制中计算得到的 \u003cstrong\u003eKey (K)\u003c/strong\u003e 和 \u003cstrong\u003eValue (V)\u003c/strong\u003e 向量。\u003c/p\u003e\n\u003ch3 id=\"2-transformer-attention-mechanism-review\"\u003e2. Transformer Attention Mechanism Review\u003c/h3\u003e\n\u003cp\u003e要理解 KV Cache，首先需要对 Transformer 架构中的自注意力机制有一个基本认识。自注意力机制允许模型在处理序列中的某个词时，考虑序列中所有其他词的重要性。\u003c/p\u003e\n\u003cp\u003e每个输入 token（词或子词）在进入注意力层时，都会被转换成三个不同的向量：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQ 向量：代表当前 token 的“查询”信息\u003c/li\u003e\n\u003cli\u003eK 向量：代表所有 token 的“键”信息，用于与 Query 进行匹配\u003c/li\u003e\n\u003cli\u003eV 向量：代表所有 token 的“值”信息，用于加权求和，得到最终的输出\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e自注意力机制的计算过程为以下步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e计算 Query 与所有 Key 的点积，得到\u003cstrong\u003e注意力分数\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e将注意力分数进行缩放，除以 $\\sqrt{d_k}$（$d_k$ 是 Key 向量的维度)\u003c/li\u003e\n\u003cli\u003e对缩放后的分数进行 Softmax，将其转换为\u003cstrong\u003e注意力权重\u003c/strong\u003e，表示每个 token 对当前 token 的重要性\u003c/li\u003e\n\u003cli\u003e将注意力权重与 Value 向量进行加权求和，得到当前 token 的注意力输出\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e公式为：\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$\n其中矩阵 $Q,K,V \\in \\mathbb{R}^{L \\times d}$ ，$L$ 为当前上下文长度\u003c/p\u003e","title":"KV Cache 入门"},{"content":"同步和条件变量 互斥实现了原子性，但是无法实现确定性，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\n因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的发生顺序（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\n实现同步\n实现 $A\\to B$:\n1 2 3 4 5 6 7 A; can_proceed = true; (signal) while(!can_proceed); B // B: wait until the condition is satisfied 这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\n最理想的 API 是 wait_until(cond) ，但是过去为了简化设计，变成了\n条件不满足时等待：wait - 直接睡眠等待 条件满足时继续：signal/broadcast - 唤醒所有线程 （小时候的 scratch 编程其实已经有了这样的思想😂）\n在 c++ 代码中我们可以把条件放到 $\\lambda$ 表达式中：\n1 2 3 4 5 6 7 8 9 10 11 12 std::mutex mtx; std::condition_variable cv; void T_player() { std::unique_lock lk(mtx); cv.wait(lk, []{ return can_proceed; } ); cv.notify_all(); lk.unlock(); } 注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\n使用条件变量解决同步问题 大部分的同步问题都可以用经典的生产者 - 消费者问题归纳：\nProducer 和 Consumer 共享一个缓冲区，其中\nProducer 看到缓冲区有空位就会放入，否则等待 Consumer 看到缓冲区有数据就回去走，否则等待 显然一个对象的生产和消费必须满足 \u0026ldquo;happens-before\u0026rdquo; 的关系\n可以等价成打印匹配的括号，并且嵌套深度有上限（缓冲区的深度）\n处理这样的问题首先要想清楚程序继续执行的条件，比如生产的条件是 $d\u0026lt;n$ ，而消费的条件是 $d\u0026gt;0$ ，然后套入固定的模板代码即可：\n1 2 3 4 5 6 mutex_lock(lk); while (!cond) { // cond can be any calculate cond_wait(\u0026amp;cv, lk); } assert(cond); mutex_lock(lk); 1 2 3 4 mutex_lock(lk); cond = true cond_broadcast(\u0026amp;cv); //⚠️ mutex_unlock(lk); 注意：全局广播 cond_broadcast 不能被替换成单独唤醒一个线程 cond_signal ，在这里显然可能会导致所有进程都被锁住无法触发新的同步变量；并发编程很多看起来正确的地方都需要仔细思考\n遇到任何同步问题的核心都是同步条件是什么，比如括号打印可以拓展成打印 \u0026lt;\u0026gt;\u0026lt; 或者 \u0026gt;\u0026lt;\u0026gt; 两种形状，核心也是画出状态机，找到同步条件，再套入模板就解决了问题\n计算图与并发控制 并行计算的模型可以用一个 DAG 计算图去理解，任务之间存在依赖关系，通过拓扑排序的顺序去解决问题，相互不存在 \u0026ldquo;happens-before\u0026rdquo; 依赖关系的任务都可以并发解决\n为了优化效率，我们对计算任务的分配需要保证每个节点计算的消耗是远大于同步和锁的开销的，因此实际上可能是把很多个小的任务聚合成一个大的并发计算节点，交给一个线程去执行\n实现计算图有两种思路，第一种是朴素的为每个节点设置一个线程和条件变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // The dependency edge is u-\u0026gt;v void T_u() { // calculate u mutex_lock(v-\u0026gt;lock); v-\u0026gt;num_done++; cond_signal(v-\u0026gt;cv); // it\u0026#39;s okay mutex_unlock(v-\u0026gt;lock); } void T_v() { mutex_lock(v-\u0026gt;lock); while (!(v-\u0026gt;num_done == v-\u0026gt;num_predecessors)){ cond_wait(v-\u0026gt;cv, v-\u0026gt;lock); } mutex_unlock(v-\u0026gt;lock); // calculate v } 但是这样实际会产生过多的线程，造成不必要的性能开销（比如产生了多余 CPU 的 core 数量的线程），实际上更优的办法是创建一个任务调度器线程 $T_{\\text{scheduler}}$ 来专门控制产生 $T_{\\text{worker}}$ ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 mutex_lock(lk); while (!(all_done || has_job(tid))) { cond_wait(\u0026amp;worker_cv[tid], lk); } mutex_unlock(lk); if (all_done) { break; } else { process_job(tid); } signal(\u0026amp;sched_cv); ","permalink":"https://diefish1024.github.io/posts/notes/nju-os-2025/15.-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E5%90%8C%E6%AD%A5-1/","summary":"\u003ch2 id=\"同步和条件变量\"\u003e同步和条件变量\u003c/h2\u003e\n\u003cp\u003e互斥实现了\u003cstrong\u003e原子性\u003c/strong\u003e，但是无法实现\u003cstrong\u003e确定性\u003c/strong\u003e，也就是无法正确实现 \u0026ldquo;happens-before\u0026rdquo; 的关系\u003c/p\u003e\n\u003cp\u003e因此需要引入条件变量来实现线程的同步，形成受控制的并发事件的\u003cstrong\u003e发生顺序\u003c/strong\u003e（可以用乐团指挥来类比），把一系列不确定的状态在某一个时间点同步到了一个确定的状态，将发散的并发程序状态 “收束”\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e实现同步\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e实现 $A\\to B$:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-gdscript3\" data-lang=\"gdscript3\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eA\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003esignal\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003ewhile\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eB\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e//\u003c/span\u003e \u003cspan class=\"n\"\u003eB\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ewait\u003c/span\u003e \u003cspan class=\"n\"\u003euntil\u003c/span\u003e \u003cspan class=\"n\"\u003ethe\u003c/span\u003e \u003cspan class=\"n\"\u003econdition\u003c/span\u003e \u003cspan class=\"n\"\u003eis\u003c/span\u003e \u003cspan class=\"n\"\u003esatisfied\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e这样的思路大致正确，但是自选的循环有很大的性能问题，因此需要一个更加底层的机制来帮助实现这一点\u003c/p\u003e\n\u003cp\u003e最理想的 API 是 \u003ccode\u003ewait_until(cond)\u003c/code\u003e ，但是过去为了简化设计，变成了\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e条件不满足时等待：\u003ccode\u003ewait\u003c/code\u003e - 直接睡眠等待\u003c/li\u003e\n\u003cli\u003e条件满足时继续：\u003ccode\u003esignal/broadcast\u003c/code\u003e - 唤醒所有线程\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（小时候的 scratch 编程其实已经有了这样的思想😂）\u003c/p\u003e\n\u003cp\u003e在 c++ 代码中我们可以把条件放到 $\\lambda$ 表达式中：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c++\" data-lang=\"c++\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003emutex\u003c/span\u003e \u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003econdition_variable\u003c/span\u003e \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eT_player\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"n\"\u003eunique_lock\u003c/span\u003e \u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emtx\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewait\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\t\u003cspan class=\"p\"\u003e[]{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ecan_proceed\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enotify_all\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t\u003cspan class=\"n\"\u003elk\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eunlock\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e注意条件变量在等待时需要带着一把锁（需要确保检查和等待是原子操作）\u003c/p\u003e","title":"并发控制：同步 (1)"}]