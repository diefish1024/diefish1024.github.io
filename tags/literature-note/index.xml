<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Literature-Note on diefish&#39;s blog</title>
    <link>https://diefish1024.github.io/tags/literature-note/</link>
    <description>Recent content in Literature-Note on diefish&#39;s blog</description>
    <image>
      <title>diefish&#39;s blog</title>
      <url>https://diefish1024.github.io/images/avatar.jpg</url>
      <link>https://diefish1024.github.io/images/avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.150.0</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 28 Aug 2025 10:10:00 +0800</lastBuildDate>
    <atom:link href="https://diefish1024.github.io/tags/literature-note/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Matcha</title>
      <link>https://diefish1024.github.io/posts/literature-notes/matcha/</link>
      <pubDate>Thu, 28 Aug 2025 10:10:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/matcha/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;现有 TTA 方法在处理图数据时，对&lt;strong&gt;节点属性偏移&lt;/strong&gt;有效，但是对&lt;strong&gt;图结构偏移&lt;/strong&gt;（同质性、节点度的变化）效果很差。原因是&lt;strong&gt;结构偏移会严重破坏节点表示的质量&lt;/strong&gt;，使不同类别的节点在特征空间中混在一起。为此论文提出了 Matcha 框架，通过在测试的时候&lt;strong&gt;自适应地调整 GNN 的“跳数聚合参数 (hop-aggregation parameters)”&lt;/strong&gt;，并且引入了新的&lt;strong&gt;预测感知的聚类损失函数&lt;/strong&gt;来表示恢复节点表示的质量，从而有效应对结构偏移，并能和现有 TTA 方法相结合，进一步提高性能。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GNN 的脆弱性&lt;/strong&gt;：GNNs 在各类图任务上的表现依赖于训练数据和测试数据分布相同的假设，然而在现实世界中，图的分布常常会发生变化（分布偏移），分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;属性偏移 (Attribute Shift)&lt;/strong&gt;：节点的特征发生变化。例如不同社交平台，即使用户一样，其账号的内容也会因为平台差异而不同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结构偏移 (Structure Shift)&lt;/strong&gt;：节点的连接方式发生变化。比如工作平台用户倾向于连接同事，生活平台用户倾向于连接家人朋友。这种连接模式的变化就是结构偏移，具体表现为&lt;strong&gt;同质性 (Homophily)&lt;/strong&gt; 和 &lt;strong&gt;节点度 (Degree)&lt;/strong&gt; 的变化。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TTA 的局限性&lt;/strong&gt;：TTA 允许一个预训练好的模型在不访问原始训练数据的情况下，利用无标签的测试数据进行自适应调整 。目前 TTA 在计算机视觉领域处理图像损坏、风格变化等属性偏移问题上很成功 。然而为图像处理设计的 TTA 方法直接应用到图上时，其在处理图结构偏移时的性能提升非常有限，几乎失效。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;400&#34; loading=&#34;lazy&#34; src=&#34;https://diefish1024.github.io/images/matcha/pasted-image-20250828111414.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;p&gt;两种偏移方式对 GNN 的影响存在本质不同。&lt;/p&gt;
&lt;h3 id=&#34;perliminaries&#34;&gt;Perliminaries&lt;/h3&gt;
&lt;p&gt;论文聚焦于 GTTA 任务。一个 GNN 模型可以被看成两个部分的组合，一个&lt;strong&gt;特征提取器 $ f_{S} $&lt;/strong&gt; ，一个&lt;strong&gt;分类器 $ g_{S} $&lt;/strong&gt; ，通常是一个线性层。&lt;/p&gt;
&lt;p&gt;两种偏移的正式定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;属性偏移&lt;/strong&gt;：源图和目标图中，节点的条件概率分布不同 $ \mathbb{P}^{S}_{x | y} \neq \mathbb{P}^{T}_{x | y} $ 。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结构偏移&lt;/strong&gt;：图的邻接矩阵和标签的联合分布不同，即 $ \mathbb{P}^{S}_{A \times Y} \neq \mathbb{P}^{T}_{A \times Y} $ 。论文主要关注两种具体的结构偏移：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;度偏移&lt;/strong&gt;：源图和目标图的平均节点度数不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;同质性偏移&lt;/strong&gt;：源图和目标图的同质性水平不同。其中图的所有节点同质性的平均值 $ h(\mathcal{G}) = \dfrac{1}{N}\sum_{i}h_{i} $ ，单个节点 $ v_{i} $ 的同质性计算公式为： $$ 
 h_{i} = \dfrac{\left| \{ v_{j} \in \mathbb{N}(v_{i}): y_{j} = y_{i} \} \right|}{d_{i}}  
 $$ 其中 $ y $ 表示节点标签，$ d $ 表示节点度数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;impact-of-distribution-shifts&#34;&gt;Impact of Distribution Shifts&lt;/h3&gt;
&lt;p&gt;通过数学建模来显示两种偏移的不同影响机制。&lt;/p&gt;</description>
    </item>
    <item>
      <title>EmT</title>
      <link>https://diefish1024.github.io/posts/literature-notes/emt/</link>
      <pubDate>Wed, 30 Jul 2025 12:55:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/emt/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方案&lt;/strong&gt;：提出 &lt;strong&gt;Emotion Transformer (EmT)&lt;/strong&gt; ，为 Graph-Transformer 混和架构&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心模块&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;TGC&lt;/strong&gt;：将 EEG 信号转换为时序图序列&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RMPG&lt;/strong&gt;：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TCT&lt;/strong&gt;：使用任务自适应的 Transformer，学习 token 序列上下文（核心）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TSO&lt;/strong&gt;：输出分类/回归结果&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成果&lt;/strong&gt;：在多个公开数据集的广义跨被试任务上面超过了 baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction--related-work&#34;&gt;Introduction &amp;amp; Related Work&lt;/h2&gt;
&lt;p&gt;为什么 EEG 难以使用跨被试 (cross-subject) 的场景？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;个体差异&lt;/strong&gt;：不同被试生理结构和认知策略差异，导致 EEG 模式不同&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低信噪比&lt;/strong&gt;：EEG 信号容易受到外源噪声干扰（肌电、眼电……）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目标是学习一种&lt;strong&gt;跨被试共享&lt;/strong&gt;、具有&lt;strong&gt;泛化能力&lt;/strong&gt;的情绪表征&lt;/p&gt;
&lt;h3 id=&#34;gpaph-neural-networks&#34;&gt;Gpaph Neural Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：EEG 数据具有非欧图结构，适合使用 GNN 来处理&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代表工作&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ChebyNet&lt;/strong&gt;：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GCN&lt;/strong&gt;：通过局部一阶聚合近似光谱滤波&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DGCNN / RGNN&lt;/strong&gt;：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有&lt;strong&gt;局限性&lt;/strong&gt;；而 EmT 通过&lt;strong&gt;多视图可学习邻接矩阵&lt;/strong&gt;和&lt;strong&gt;时序图&lt;/strong&gt;来弥补&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;temporal-context-learning&#34;&gt;Temporal Context Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心理念&lt;/strong&gt;：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代表工作&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;LSTM / TCN / TESANet / Conformer / AMDET&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;局限性&lt;/strong&gt;：这些方法通常从扁平化的 EEG 特征向量学习，可能&lt;strong&gt;未能有效学习空间关系&lt;/strong&gt;；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;eeg-emotion-recognition&#34;&gt;EEG Emotion Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;核心理念&lt;/strong&gt;：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代表工作&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;GCB-Net / TSception&lt;/li&gt;
&lt;li&gt;局限性：没有关注长时序上下文信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;EmT 是一个端到端的框架，包含四大模块：&lt;/p&gt;</description>
    </item>
    <item>
      <title>SSA</title>
      <link>https://diefish1024.github.io/posts/literature-notes/ssa/</link>
      <pubDate>Wed, 30 Jul 2025 11:00:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/ssa/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征&lt;/p&gt;
&lt;h2 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h2&gt;
&lt;p&gt;考虑一个回归模型 $ f_\theta: \mathcal{X} \to \mathbb{R} $，可以进一步分解为&lt;strong&gt;特征提取器&lt;/strong&gt; $ g_\phi: \mathcal{X} \to \mathbb{R}^D $（从输入 $ \mathcal{X} $ 提取 $ D $ 维特征 $ z $）和&lt;strong&gt;线性回归器&lt;/strong&gt; $ h_\psi(z) = w^T z + b $（或者 $ h_{\psi}(z)=Wz+b $）&lt;/p&gt;
&lt;p&gt;$ f_\theta $ 首先在一个有标签的&lt;strong&gt;源数据集&lt;/strong&gt; $ S = \{(x_i, y_i)\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样&lt;/p&gt;
&lt;p&gt;目标是使用一个&lt;strong&gt;无标签的&lt;/strong&gt;目标数据集 $ T = \{x_j\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\theta $ 到目标域&lt;/p&gt;
&lt;p&gt;我们假设存在 &lt;strong&gt;covariate shift&lt;/strong&gt; ，这意味着：&lt;/p&gt;</description>
    </item>
    <item>
      <title>T-TIME</title>
      <link>https://diefish1024.github.io/posts/literature-notes/t-time/</link>
      <pubDate>Wed, 30 Jul 2025 11:00:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/t-time/</guid>
      <description>&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;h3 id=&#34;problem-set&#34;&gt;Problem Set&lt;/h3&gt;
&lt;p&gt;EEG 数据 $ \{ X_{s,l}^{i},y_{s,l}^{i} \}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类&lt;/p&gt;
&lt;h3 id=&#34;source-model-training&#34;&gt;Source Model Training&lt;/h3&gt;
&lt;p&gt;对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异&lt;/p&gt;
&lt;p&gt;EA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值
$$ 

R_{s,l} = \dfrac{1}{n}\sum_{i=1}^{n} X_{i}(X_{i})^{T} \implies \bar{X}_{i} = R_{s,l}^{-1/2}X_{i}

 $$
之后再整合经过对齐的受试者数据，形成“源域”&lt;/p&gt;
&lt;p&gt;在整合后的数据上独立训练 $ M $ 个模型&lt;/p&gt;
&lt;h3 id=&#34;incremental-ea-on-target-data&#34;&gt;Incremental EA on Target Data&lt;/h3&gt;
&lt;p&gt;对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据&lt;/p&gt;
&lt;h3 id=&#34;target-label-prediction&#34;&gt;Target Label Prediction&lt;/h3&gt;
&lt;p&gt;用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $&lt;/p&gt;
&lt;p&gt;新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}&#39; $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}&#39;) $&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tent</title>
      <link>https://diefish1024.github.io/posts/literature-notes/tent/</link>
      <pubDate>Wed, 30 Jul 2025 11:00:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/tent/</guid>
      <description>&lt;h1 id=&#34;setting&#34;&gt;Setting&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Fully Test-Time Adaptation&lt;/strong&gt; 是一种独特的模型适应设定。在此设定下，模型 $ f_\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。&lt;/p&gt;
&lt;p&gt;FTT-Adaptation 与以下方法不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;：需要目标标签进行重新训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Domain Adaptation&lt;/strong&gt;：需要源数据和目标数据进行联合训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;：需要修改训练过程并共同优化有监督及自监督损失。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;论文的核心贡献是提出了 &lt;strong&gt;Tent&lt;/strong&gt; 方法，其核心思想是通过&lt;strong&gt;最小化测试熵&lt;/strong&gt;（&lt;strong&gt;Test Entropy Minimization&lt;/strong&gt;）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。&lt;/p&gt;
&lt;h3 id=&#34;entropy-objective&#34;&gt;Entropy Objective&lt;/h3&gt;
&lt;p&gt;Tent 的测试时目标函数是最小化模型预测 $ \hat{y} = f_\theta(x^t) $ 的&lt;strong&gt;熵 $ H(\hat{y}) $&lt;/strong&gt;。论文中使用的&lt;strong&gt;香农熵&lt;/strong&gt;计算公式如下：&lt;/p&gt;
$$ 

H(\hat{y}) = - \sum_c p(\hat{y}_c) \log p(\hat{y}_c)

 $$
&lt;p&gt;其中， $ p(\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。&lt;/p&gt;</description>
    </item>
    <item>
      <title>CoTTA</title>
      <link>https://diefish1024.github.io/posts/literature-notes/cotta/</link>
      <pubDate>Wed, 30 Jul 2025 10:59:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/cotta/</guid>
      <description>&lt;h1 id=&#34;setting&#34;&gt;Setting&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Continual Test-Time Domain Adaptation&lt;/strong&gt; 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个&lt;strong&gt;非平稳&lt;/strong&gt;且&lt;strong&gt;持续变化&lt;/strong&gt;的目标环境 。&lt;/p&gt;
&lt;p&gt;CoTTA 与以下方法不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standard Domain Adaptation&lt;/strong&gt;：需要同时访问源数据和（静态的）目标数据进行训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standard Test-Time Adaptation / Fully Test-Time Adaptation&lt;/strong&gt;：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比之下，CoTTA 专门解决在&lt;strong&gt;无源数据&lt;/strong&gt;的条件下，模型如何在线适应一个&lt;strong&gt;持续变化的&lt;/strong&gt;数据流，同时克服现有方法中常见的&lt;strong&gt;错误累积&lt;/strong&gt;和&lt;strong&gt;灾难性遗忘&lt;/strong&gt;问题。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;论文的核心贡献是提出了&lt;strong&gt;CoTTA (Continual Test-Time Adaptation)&lt;/strong&gt; 方法，旨在通过&lt;strong&gt;减少错误累积&lt;/strong&gt;和&lt;strong&gt;避免灾难性遗忘&lt;/strong&gt;，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。&lt;/p&gt;
&lt;h3 id=&#34;1-减少错误累积-reducing-error-accumulation&#34;&gt;1. 减少错误累积 (Reducing Error Accumulation)&lt;/h3&gt;
&lt;p&gt;为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;权重平均伪标签 (Weight-Averaged Pseudo-Labels)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;该方法采用一个&lt;strong&gt;教师 - 学生 (teacher-student)&lt;/strong&gt; 框架。学生模型 (student model) 在线进行学习和更新。&lt;/li&gt;
&lt;li&gt;教师模型 (teacher model) 的权重是学生模型权重的&lt;strong&gt;指数移动平均 (Exponential Moving Average, EMA)&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的&lt;strong&gt;一致性损失&lt;/strong&gt; (consistency loss) 来进行更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。&lt;/li&gt;
&lt;li&gt;它首先使用&lt;strong&gt;原始预训练模型&lt;/strong&gt;评估当前测试数据的&lt;strong&gt;预测置信度&lt;/strong&gt;，以此来近似域差异的大小。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;条件性应用&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;如果置信度&lt;strong&gt;高&lt;/strong&gt;（域差异小），则直接使用教师模型的预测作为伪标签。&lt;/li&gt;
&lt;li&gt;如果置信度&lt;strong&gt;低&lt;/strong&gt;（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签。这可以进一步提高伪标签的鲁棒性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-避免灾难性遗忘-avoiding-catastrophic-forgetting&#34;&gt;2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting)&lt;/h3&gt;
&lt;p&gt;为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了&lt;strong&gt;随机恢复 (Stochastic Restoration)&lt;/strong&gt; 机制。&lt;/p&gt;</description>
    </item>
    <item>
      <title>DANN</title>
      <link>https://diefish1024.github.io/posts/literature-notes/dann/</link>
      <pubDate>Wed, 30 Jul 2025 10:59:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/dann/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;类似 GAN 的对抗训练思想&lt;/p&gt;
&lt;h2 id=&#34;domain-adaptation&#34;&gt;Domain Adaptation&lt;/h2&gt;
&lt;p&gt;给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \eta: X\to Y $ 使其在目标域上的目标风险
$$ 

R_{D_{T}}(\eta) = \underset{(\mathbf{x},y)\sim D_{T}}{\mathrm{Pr}}(\eta(\mathbf{x}) \neq y)

 $$
最小&lt;/p&gt;
&lt;h4 id=&#34;domain-divergence&#34;&gt;Domain Divergence&lt;/h4&gt;
&lt;p&gt;需要量化两个领域的“相似度”，从而引出了 &lt;strong&gt;H- 散度&lt;/strong&gt; 的概念：
$$ 

d_{\mathcal{H}}(D_S, D_T) = 2 \sup_{\eta \in \mathcal{H}} \left| \Pr_{x \sim D_S}[\eta(x) = 1] - \Pr_{x \sim D_T}[\eta(x) = 1] \right|

 $$
含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果&lt;/p&gt;
&lt;p&gt;由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度
$$ 

\hat{d}_{\mathcal{H}}(S, T) = 2 \left(1 - \min_{\eta \in \mathcal{H}} \left[ \dfrac{1}{n}\sum_{i=1}^n \mathcal{I}[\eta(x_i) = 0] + \dfrac{1}{n&#39;}\sum_{i=n+1}^N \mathcal{I}[\eta(x_i) = 1] \right] \right)

 $$
其中 $ \mathcal{I}[\cdot] $ 表示条件为真时为 1，否则为 0&lt;/p&gt;</description>
    </item>
    <item>
      <title>Benchmarking TTA</title>
      <link>https://diefish1024.github.io/posts/literature-notes/benchmarking-tta/</link>
      <pubDate>Wed, 30 Jul 2025 10:58:00 +0800</pubDate>
      <guid>https://diefish1024.github.io/posts/literature-notes/benchmarking-tta/</guid>
      <description>&lt;h3 id=&#34;a-general-paradigm-of-test-time-adaptation&#34;&gt;A General Paradigm of Test-Time Adaptation&lt;/h3&gt;
&lt;p&gt;根据测试数据接收方式和适应过程，TTA 分为三种主要范式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Test-Time Batch Adaptation (TTBA) 测试时间批次适应：&lt;/strong&gt; 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Online Test-Time Adaptation (OTTA) 在线测试时间适应：&lt;/strong&gt; 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test-Time Domain Adaptation (TTDA) 测试时间域适应：&lt;/strong&gt; 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datasets-for-evaluation&#34;&gt;Datasets for Evaluation&lt;/h3&gt;
&lt;p&gt;论文使用了两种不同类型的分布偏移数据集进行评估：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Corruption Datasets 损坏数据集：&lt;/strong&gt; 原始数据集（CIFAR-10，ImageNet）经过&lt;strong&gt;人为损坏处理&lt;/strong&gt;后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural-shift Datasets 自然偏移数据集：&lt;/strong&gt; 这些数据集代表数据分布中&lt;strong&gt;自然发生的变化&lt;/strong&gt;，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results-on-natural-shift-datasets&#34;&gt;Results on Natural Shift Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。&lt;/li&gt;
&lt;li&gt;PredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;T3A&lt;/strong&gt; 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。&lt;/li&gt;
&lt;li&gt;对于自然偏移数据集，&lt;strong&gt;TTDA 算法&lt;/strong&gt; 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
