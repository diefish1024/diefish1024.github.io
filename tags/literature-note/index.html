<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Literature-Note | diefish's blog</title><meta name=keywords content><meta name=description content="A freshman at SJTU John class."><meta name=author content="diefish"><link rel=canonical href=https://diefish1024.github.io/tags/literature-note/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=16x16 href=https://diefish1024.github.io/images/avatar.jpg><link rel=icon type=image/png sizes=32x32 href=https://diefish1024.github.io/images/avatar.jpg><link rel=apple-touch-icon href=https://diefish1024.github.io/images/avatar.jpg><link rel=mask-icon href=https://diefish1024.github.io/images/avatar.jpg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://diefish1024.github.io/tags/literature-note/index.xml><link rel=alternate hreflang=en href=https://diefish1024.github.io/tags/literature-note/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://diefish1024.github.io/tags/literature-note/"><meta property="og:site_name" content="diefish's blog"><meta property="og:title" content="Literature-Note"><meta property="og:description" content="A freshman at SJTU John class."><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta property="og:image" content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://diefish1024.github.io/images/avatar.jpg"><meta name=twitter:title content="Literature-Note"><meta name=twitter:description content="A freshman at SJTU John class."></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://diefish1024.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://diefish1024.github.io/images/avatar.jpg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://diefish1024.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://diefish1024.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://diefish1024.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://diefish1024.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://diefish1024.github.io/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://diefish1024.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://diefish1024.github.io/tags/>Tags</a></div><h1>Literature-Note</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Matcha</h2></header><div class=entry-content><p>Abstract 现有 TTA 方法在处理图数据时，对节点属性偏移有效，但是对图结构偏移（同质性、节点度的变化）效果很差。原因是结构偏移会严重破坏节点表示的质量，使不同类别的节点在特征空间中混在一起。为此论文提出了 Matcha 框架，通过在测试的时候自适应地调整 GNN 的“跳数聚合参数 (hop-aggregation parameters)”，并且引入了新的预测感知的聚类损失函数来表示恢复节点表示的质量，从而有效应对结构偏移，并能和现有 TTA 方法相结合，进一步提高性能。
Introduction GNN 的脆弱性：GNNs 在各类图任务上的表现依赖于训练数据和测试数据分布相同的假设，然而在现实世界中，图的分布常常会发生变化（分布偏移），分为：
属性偏移 (Attribute Shift)：节点的特征发生变化。例如不同社交平台，即使用户一样，其账号的内容也会因为平台差异而不同。
结构偏移 (Structure Shift)：节点的连接方式发生变化。比如工作平台用户倾向于连接同事，生活平台用户倾向于连接家人朋友。这种连接模式的变化就是结构偏移，具体表现为 同质性 (Homophily) 和 节点度 (Degree) 的变化。
TTA 的局限性：TTA 允许一个预训练好的模型在不访问原始训练数据的情况下，利用无标签的测试数据进行自适应调整 。目前 TTA 在计算机视觉领域处理图像损坏、风格变化等属性偏移问题上很成功 。然而为图像处理设计的 TTA 方法直接应用到图上时，其在处理图结构偏移时的性能提升非常有限，几乎失效。
Analysis 两种偏移方式对 GNN 的影响存在本质不同。
Perliminaries 论文聚焦于 GTTA 任务。一个 GNN 模型可以被看成两个部分的组合，一个特征提取器 $ f_{S} $ ，一个 分类器 $ g_{S} $ ，通常是一个线性层。
两种偏移的正式定义：
属性偏移：源图和目标图中，节点的条件概率分布不同 $ \mathbb{P}^{S}_{x | y} \neq \mathbb{P}^{T}_{x | y} $ 。 结构偏移：图的邻接矩阵和标签的联合分布不同，即 $ \mathbb{P}^{S}_{A \times Y} \neq \mathbb{P}^{T}_{A \times Y} $ 。论文主要关注两种具体的结构偏移： 度偏移：源图和目标图的平均节点度数不同。 同质性偏移：源图和目标图的同质性水平不同。其中图的所有节点同质性的平均值 $ h(\mathcal{G}) = \dfrac{1}{N}\sum_{i}h_{i} $ ，单个节点 $ v_{i} $ 的同质性计算公式为： $$ h_{i} = \dfrac{\left| \{ v_{j} \in \mathbb{N}(v_{i}): y_{j} = y_{i} \} \right|}{d_{i}} $$ 其中 $ y $ 表示节点标签，$ d $ 表示节点度数。 Impact of Distribution Shifts 通过数学建模来显示两种偏移的不同影响机制。
...</p></div><footer class=entry-footer><span title='2025-08-28 10:10:00 +0800 +0800'>August 28, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to Matcha" href=https://diefish1024.github.io/posts/literature-notes/matcha/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>EmT</h2></header><div class=entry-content><p>Abstract 问题：现有 EEG 情绪识别方法对长期上下文信息关注不足，导致跨被试泛化能力减弱 方案：提出 Emotion Transformer (EmT) ，为 Graph-Transformer 混和架构 核心模块： TGC：将 EEG 信号转换为时序图序列 RMPG：使用残差多视图金字塔 GCN，学习动态、多尺度的空间连接模式，生成 token（核心） TCT：使用任务自适应的 Transformer，学习 token 序列上下文（核心） TSO：输出分类/回归结果 成果：在多个公开数据集的广义跨被试任务上面超过了 baseline Introduction & Related Work 为什么 EEG 难以使用跨被试 (cross-subject) 的场景？
个体差异：不同被试生理结构和认知策略差异，导致 EEG 模式不同 低信噪比：EEG 信号容易受到外源噪声干扰（肌电、眼电……） 目标是学习一种跨被试共享、具有泛化能力的情绪表征
Gpaph Neural Networks 核心思想：EEG 数据具有非欧图结构，适合使用 GNN 来处理 代表工作： ChebyNet：使用切比雪夫多项式近似光谱滤波，EmT 模型中采用其作为 GCN 层 GCN：通过局部一阶聚合近似光谱滤波 DGCNN / RGNN：使用 GNNs 提取 EEG 空间信息；依赖单一的邻接矩阵，忽略时序上下文，具有局限性；而 EmT 通过多视图可学习邻接矩阵和时序图来弥补 Temporal Context Learning 核心理念：情绪是连续认知过程，EEG 信号中嵌入时序上下文信息 代表工作： LSTM / TCN / TESANet / Conformer / AMDET 局限性：这些方法通常从扁平化的 EEG 特征向量学习，可能未能有效学习空间关系；EmT 则通过并行 GCN 和 STA 层更有效地捕捉时空信息 EEG Emotion Recognition 核心理念：EEG 情绪识别面临个体差异大、信噪比低等挑战，需提取光谱、空间、时序特征 代表工作： GCB-Net / TSception 局限性：没有关注长时序上下文信息 Method EmT 是一个端到端的框架，包含四大模块：
...</p></div><footer class=entry-footer><span title='2025-07-30 12:55:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to EmT" href=https://diefish1024.github.io/posts/literature-notes/emt/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>SSA</h2></header><div class=entry-content><p>Introduction TTA 在回归任务上的局限：为分类任务设计，一般基于熵最小化和特征对齐；熵最小化不适用，回归模型产生单一值，不产生概率分布；简单特征对齐对回归模型效果不佳，可能反而会稀释需要学习的特征
Problem Setting 考虑一个回归模型 $ f_\theta: \mathcal{X} \to \mathbb{R} $，可以进一步分解为特征提取器 $ g_\phi: \mathcal{X} \to \mathbb{R}^D $（从输入 $ \mathcal{X} $ 提取 $ D $ 维特征 $ z $）和线性回归器 $ h_\psi(z) = w^T z + b $（或者 $ h_{\psi}(z)=Wz+b $）
$ f_\theta $ 首先在一个有标签的源数据集 $ S = \{(x_i, y_i)\}_{i=1}^{N_s} $ 上进行预训练，数据从源域分布 $ p_s $ 中采样
目标是使用一个无标签的目标数据集 $ T = \{x_j\}_{j=1}^{N_t} $ 来适应预训练好的模型 $ f_\theta $ 到目标域
我们假设存在 covariate shift ，这意味着：
...</p></div><footer class=entry-footer><span title='2025-07-30 11:00:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to SSA" href=https://diefish1024.github.io/posts/literature-notes/ssa/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>T-TIME</h2></header><div class=entry-content><p>Method Problem Set EEG 数据 $ \{ X_{s,l}^{i},y_{s,l}^{i} \}_{i=1}^{n_{s,l}} $ ，进行无监督在线 K 分类
Source Model Training 对源数据做 Euclidean alignment (EA) 数据对齐，减少不同个体 EEG 信号差异
EA 计算每个个体所有 EEG 试次协方差矩阵的算术平均值 $$ R_{s,l} = \dfrac{1}{n}\sum_{i=1}^{n} X_{i}(X_{i})^{T} \implies \bar{X}_{i} = R_{s,l}^{-1/2}X_{i} $$ 之后再整合经过对齐的受试者数据，形成“源域”
在整合后的数据上独立训练 $ M $ 个模型
Incremental EA on Target Data 对新数据增量式更新协方差矩阵，再用新的矩阵更新所有测试数据
Target Label Prediction 用训练好的 $ M $ 模型初始化用于适应目标域的 $ M $ 个 TTA 模型 $ f_{m} $
新的 $ X_{a} $ 经过 IEA 被变换为 $ X_{a}' $ 后被输入到每个模型 $ f_{m} $ 中进行分类，输出概率向量 $ f_{m}(X_{a}') $
...</p></div><footer class=entry-footer><span title='2025-07-30 11:00:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to T-TIME" href=https://diefish1024.github.io/posts/literature-notes/t-time/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Tent</h2></header><div class=entry-content><p>Setting Fully Test-Time Adaptation 是一种独特的模型适应设定。在此设定下，模型 $ f_\theta(x) $ 在训练阶段已通过源数据 $ x^s $ 和标签 $ y^s $ 完成训练，获得参数 $ \theta $。但在测试阶段，模型将遇到与源数据分布不同的无标签目标数据 $ x^t $。
FTT-Adaptation 与以下方法不同：
Fine-tuning：需要目标标签进行重新训练。 Domain Adaptation：需要源数据和目标数据进行联合训练。 Test-Time Training (TTT)：需要修改训练过程并共同优化有监督及自监督损失。 相比之下，FTT-Adaptation 仅能利用预训练模型 $ f_\theta $ 和无标签目标数据 $ x^t $ 进行适应，不依赖源数据或额外的监督信息。
Method 论文的核心贡献是提出了 Tent 方法，其核心思想是通过最小化测试熵（Test Entropy Minimization）来适应模型预测，旨在使模型对测试数据的预测结果更“有信心”。
Entropy Objective Tent 的测试时目标函数是最小化模型预测 $ \hat{y} = f_\theta(x^t) $ 的熵 $ H(\hat{y}) $。论文中使用的香农熵计算公式如下：
$$ H(\hat{y}) = - \sum_c p(\hat{y}_c) \log p(\hat{y}_c) $$ 其中， $ p(\hat{y}_c) $ 表示模型预测目标数据 $ x^t $ 属于类别 $ c $ 的概率。
...</p></div><footer class=entry-footer><span title='2025-07-30 11:00:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to Tent" href=https://diefish1024.github.io/posts/literature-notes/tent/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>CoTTA</h2></header><div class=entry-content><p>Setting Continual Test-Time Domain Adaptation 是一种更具挑战性的模型适应设定。在此设定下，一个在源数据上预训练好的模型，在测试时会遇到一个非平稳且持续变化的目标环境 。
CoTTA 与以下方法不同：
Standard Domain Adaptation：需要同时访问源数据和（静态的）目标数据进行训练。 Standard Test-Time Adaptation / Fully Test-Time Adaptation：通常假设目标域是固定的或静态的，而 CoTTA 关注的是持续变化的目标域。 Test-Time Training (TTT)：需要修改源模型的训练过程以加入辅助任务，因此无法使用任意的“开箱即用”预训练模型。 相比之下，CoTTA 专门解决在无源数据的条件下，模型如何在线适应一个持续变化的数据流，同时克服现有方法中常见的错误累积和灾难性遗忘问题。
Method 论文的核心贡献是提出了CoTTA (Continual Test-Time Adaptation) 方法，旨在通过减少错误累积和避免灾难性遗忘，实现模型在非平稳环境下的长期稳定适应，主要有两个关键部分。
1. 减少错误累积 (Reducing Error Accumulation) 为了生成更可靠的自训练信号，CoTTA 采用了平均化的伪标签策略，该策略结合了权重平均和数据增强平均。
权重平均伪标签 (Weight-Averaged Pseudo-Labels) 该方法采用一个教师 - 学生 (teacher-student) 框架。学生模型 (student model) 在线进行学习和更新。 教师模型 (teacher model) 的权重是学生模型权重的指数移动平均 (Exponential Moving Average, EMA)。 由于教师模型的更新更平滑，其预测结果通常比学生模型更准确，因此用它来生成伪标签，可以有效减少错误累积。学生模型通过最小化与教师伪标签的一致性损失 (consistency loss) 来进行更新。 数据增强平均伪标签 (Augmentation-Averaged Pseudo-Labels) 为了进一步提升伪标签在遇到较大域偏移时的质量，CoTTA 会有条件地使用数据增强。 它首先使用原始预训练模型评估当前测试数据的预测置信度，以此来近似域差异的大小。 条件性应用： 如果置信度高（域差异小），则直接使用教师模型的预测作为伪标签。 如果置信度低（域差异大），则对输入数据进行 N 次随机增强，并将教师模型对这些增强样本的平均预测结果作为伪标签。这可以进一步提高伪标签的鲁棒性。 2. 避免灾难性遗忘 (Avoiding Catastrophic Forgetting) 为了在长期适应过程中保留从源域学到的知识，CoTTA 引入了随机恢复 (Stochastic Restoration) 机制。
...</p></div><footer class=entry-footer><span title='2025-07-30 10:59:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to CoTTA" href=https://diefish1024.github.io/posts/literature-notes/cotta/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>DANN</h2></header><div class=entry-content><p>Introduction 类似 GAN 的对抗训练思想
Domain Adaptation 给定源域 $ D_{S} $ （有标签）和目标域 $ D_{T} $ （无标签），目标是训练一个分类器 $ \eta: X\to Y $ 使其在目标域上的目标风险 $$ R_{D_{T}}(\eta) = \underset{(\mathbf{x},y)\sim D_{T}}{\mathrm{Pr}}(\eta(\mathbf{x}) \neq y) $$ 最小
Domain Divergence 需要量化两个领域的“相似度”，从而引出了 H- 散度 的概念： $$ d_{\mathcal{H}}(D_S, D_T) = 2 \sup_{\eta \in \mathcal{H}} \left| \Pr_{x \sim D_S}[\eta(x) = 1] - \Pr_{x \sim D_T}[\eta(x) = 1] \right| $$ 含义是最优的分类器将目标域和源域判定为 1 的可能性之差，当 H- 散度非常小时，说明两个领域很难被区分，也就说明学习的特征实现了领域不变性的效果
由于理论 H 散度是理想数据分布上的定义，实际中只有有限的样本集 $ S $ 和 $ T $ ，因此需要一定的近似，于是需要经验 H- 散度 $$ \hat{d}_{\mathcal{H}}(S, T) = 2 \left(1 - \min_{\eta \in \mathcal{H}} \left[ \dfrac{1}{n}\sum_{i=1}^n \mathcal{I}[\eta(x_i) = 0] + \dfrac{1}{n'}\sum_{i=n+1}^N \mathcal{I}[\eta(x_i) = 1] \right] \right) $$ 其中 $ \mathcal{I}[\cdot] $ 表示条件为真时为 1，否则为 0
...</p></div><footer class=entry-footer><span title='2025-07-30 10:59:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to DANN" href=https://diefish1024.github.io/posts/literature-notes/dann/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Benchmarking TTA</h2></header><div class=entry-content><p>A General Paradigm of Test-Time Adaptation 根据测试数据接收方式和适应过程，TTA 分为三种主要范式：
Test-Time Batch Adaptation (TTBA) 测试时间批次适应： 数据以小批次形式到达。模型会针对每个到来的小批次进行适应，并立即提供预测。 Online Test-Time Adaptation (OTTA) 在线测试时间适应： 数据以序列化的方式（小批次）到达。模型进行增量更新，并且过去的适应经验会影响未来的预测。 Test-Time Domain Adaptation (TTDA) 测试时间域适应： 整个目标域的数据（所有测试数据）可在预测前一次性用于适应。 Datasets for Evaluation 论文使用了两种不同类型的分布偏移数据集进行评估：
Corruption Datasets 损坏数据集： 原始数据集（CIFAR-10，ImageNet）经过人为损坏处理后得到的，通过添加不同类型的噪声、模糊等，模拟不同严重程度的分布偏移。 Natural-shift Datasets 自然偏移数据集： 这些数据集代表数据分布中自然发生的变化，收集自不同的真实世界来源或条件（Office-Home，DomainNet，其中图像可能是不同风格的艺术作品、剪贴画、真实世界照片或草图）。 Results on Natural Shift Datasets TTA 方法在自然偏移数据集上的表现与在损坏数据集上的表现有所不同。 PredBN 在损坏数据集上有效，但在自然偏移数据集上表现不佳，有时甚至比源模型更差。这可能是因为自然偏移对数据分布的影响与人工损坏不同。 T3A 在 OTTA 范式下的自然偏移数据集上表现优于其他 OTTA 算法。这归因于其特征生成方式及其分类器优化能力。 对于自然偏移数据集，TTDA 算法 持续取得了最高的性能。一些 OTTA 方法的多轮次也能达到可比的成果。</p></div><footer class=entry-footer><span title='2025-07-30 10:58:00 +0800 +0800'>July 30, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;diefish</footer><a class=entry-link aria-label="post link to Benchmarking TTA" href=https://diefish1024.github.io/posts/literature-notes/benchmarking-tta/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://diefish1024.github.io/>diefish's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>